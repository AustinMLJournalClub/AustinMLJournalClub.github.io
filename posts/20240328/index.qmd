---
title: "Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations"
author: "Hongsup Shin"
date: "2024-03-28"
image: https://www.science.org/cms/10.1126/science.aax2342/asset/1353fece-9d19-4293-be71-0d7e8a834b28/assets/graphic/366_447_f1.jpeg
description: Amidst the LLM hype in ML, algorithmic bias is continued being overlooked although they still have major impact on critical domains such as healthcare, finance, and criminal justice. This seminal paper from 2019 found racial bias in a widely used healthcare algorithm, which used a problematic proxy (healthcare cost) as a target instead of what really matters, patient's sickness. The paper is a few years old but the message is still relevant, and we also get to discuss what's happened since then.
categories: [paper]
---

:::{.callout-note}
- Paper: [Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations](https://www.science.org/doi/10.1126/science.aax2342)
- Presenter: [Hongsup](https://www.linkedin.com/in/hongsupshin/)
- Attendees: 
:::

## Why this paper?

Fresh out of academia and at my first job, I remember being surprised by the great power I was able to wield as the main data scientist in the team. A few lines of my code could easily have cascading impact on business decisions. And (as is often the case) since management didn't care much about technical details, this power gave me a sense of grave responsibility, which was honestly often terrifying. To this day, this sense is something I try to remind myself of, especially because ML systems are getting more complex and we still have very little accountability for ML. So in a way, every ML practitioner is the first line of defense. And this resonsibility is more critical if one is working in an high-impact domain such as healthcare.

These days it feels like ML is all about LLMs and AI assistants. But algorthmic bias is still widespread, and unfortunately it's even more overshadowed by this massive hype. This seminal paper from 2019 identified racial bias in a healthcare algorithm and discussed the problem of label choice bias. I find this paper still relevent because this bias can easily sneak when building datasets and algorithms. Since the paper is a few years old, it will be interesting to discuss what's happened since then.

## Paper summary
- this is a famous paper in Fair ML field
- cited frequently
- "algorithm audit" paper
- inspirational: uses many different (and creative) approaches to prove the main point

## author: ziad obermeyer
- uc berkeley; school of public health; harvard med schoool, history/science, philosophy masters, physician
- physician specliazed in emergency medicine
- nightingale foundation; open sourcing medical imaging
- **he was working at a hospital that gave access to the algorithm**
- also **Sendhil Mullainathan**: U chicaco economist

### Abstract
- At a given score (algorithm estimate), Black patients are much sicker than White
- The algorithm uses healthcare cost as target (proxy), not the sickness itself
- Label choice bias: the choice of convenient, seemingly effective proxies for ground truth can cause algorithmic bias

### **Introduction**

#### Many examples
- these examples still exist

#### Difficulty of empricial investigation
- proprietary algos -> must work "from the outside" (creative, audit)
  - still limiting: difficult to understand how and why
  - training data, objective function, prediction method

#### this study: high-risk care management system
- rich data, live, scaled
- risk prediction tool
- used for 200M in the US 
- high-risk care management system
  - provide care for complex medical needs (expensive)
  - effective in saving cost and improving outcome
  - resource is limited: use alorithm to prioritize **who will benefit the most**
  - **healthy care systems rely on algirhtms **exensively** for this

#### Supp: **algorithm in context**
- obamacare -> pressure on hospitals to reduce health care cost
- causes development of "high risk care management" programs
  - early identification of high risk patient -> reduce cost (like ER visit)

#### figure out "who will benefit the most"
- difficult **causal inference** problem; we need to estimate individual treatment effeects
- assumption: "those with the greatest needs will benefit the most"
- target: "future health care needs"

#### rare opportunity
- access to everything: data, objective function, prediction
- not a unique algorithm: widely used

#### **implications of the study**
- many analogies (risk assessment tools)
- importance and opportunity of studying bias in healthcare algo ("in its own right")

### Data and analytic strategy

#### Data
- large academic hospital
- 2013-2015
- primary: white vs. black (self-reporting)
  - disadvantage: lack of intersectional racial/ethnic identities
- sample size
  - black: 6k (12k patient-years)
  - white: 40k (90k patient-years)
  - "patient-years": 1 patient-year = data collected for a patient in a calendar year

#### algorithm scores
- generated for each patient during the enrollment period
- `> 97`%: automatically "identified" as enrolled (not guaranteed for enrollment)
- `> 55%`: referred to their PCP for their opinion
- **program run 3x a year: patients whose scores > threshold (97%)**

#### main methodology
- fairness metrics
  - calibration
  - statistical parity (demographic parity): college admission rate between white vs. black
  - not possible to optimize all metrics simultaneously
  - **UT Austin presence: Maria De-Arteaga**
- this study: calibration
  - E[Y|R, W] = E[Y|R, B]
    - Y: interest
    - R: risk score
- what we do: **EXAMINE CALIBRATION** R vs. H, R vs. C
  - **WE DO NOT KNOW HOW $R$ IS CALCULATED!**
  - $R_{i, t}$: risk score given patient i in year t given $X_{i, t-1}$ (commercial algo)
  - $H_{i, t}$: realized health
  - $C_{i, t}$: actual cost 
  - **how well R is calibrated across race for H (or C)**

#### measuring H and C
- H: health record data, diagnoses, lab results, vital signs, etc.
- C: insurance claims on utilization, outpatient/ER visit, hospitalization, health care costs

#### **supp**: algo implementation
- label: C (total medical expenditures)
- features: insurance claim data
  - demographics
  - insurance type
  - ICD-9 (international classification of diseases code) diagnosis and procedure codes
  - prescribed medications
  - encounters, categorized by type of service (surgical, radiology, etc.)
  - billed amounts, categorized by type (outpatient, dialysis, etc.)
- **algo is run 3 times a year** 

### Health disparities conditional on risk score

#### overall measure of health: comorbidity score
- total number of chronic illnesses over year t = **a measure of medical complexity; correlates w/ treatment effect of care management programs**
- **more biological measure (from eletronic health records; EHR) are also used: measure for severity (not just presence)**
  - **these data need to be cleaned extensively; usually not available for model building; hard to validate**
  - **in the analysis, the authors analzed the difference in these values between black and white**

#### Fig 1A: mean comorbidity vs. risk score
- at the same risk score, Blacks have significantly more illness than Whites
- 97%: black 4.8, white 3.8

#### Meaning of this score discrepancy
- less healthy blacks = more healthy whites -> same score -> substantial disparities in program screening
- use counterfactual to measure this
  - counterfactual: no racial bias

#### **how the counterfactual works**!
1. at a given percentile (e.g., 97th percentile) based on the risk score (cost algo)
2. identify whites who are above, identify blacks who are above and below cutoff
3. if the healthiest white-upper's comorbidity < sickest black-lower's comorbidity 
   (= if the healthist white-upper is healthier than the sickest black-lower)
4. remove the healthiest white (white-upper) 
5. and give this spot to the sickest black from black-below (move them to the black-upper)
6. check all blacks in black-below

#### Fig 2: blacks are substantially less healthy than whites
- across many biomarkers
- at any levels of algorithm predictions

##### Additional robustness checks related to program effect
Does the program itself have an effect?
1. Predict H_{t-1} instead of H_{t}: no racial difference 
2. Compare enrolled vs. unenrolled: no difference (**is it though?**)

### Mechanisms of bias

- algorithm excludes race
- target: cost
- **Fig 3A**: in terms of cost, it's well calibrated between white and black (no bias)
- why then?
  - **Fig 3B**: at a given level of health, Black generate lower costs than Whites
  - Blacks also generate very different kinds of costs (**Tab S2**)
    - more costs related to ER and dialysis
    - fewer inpatient surgical and outpatient specialist costs

#### Why this happens

1. poor patients face substantial barriers to accessing health care, even the insured ones
   1. geography, transportation
   2. demands from job, child care
   3. knowledge of reasons to seek care
2. cost directly affected by race
   1. **taste-based discrmination**: employers' prejudice or dislikes in an organisational culture rooted in prohibited grounds can have negative results in hiring minority workers
      1. One of the leading causes of labor market discrimination (the other: statistical discrimination)
      2. employers' preference for employees of certain groups is unrelated to their preference for more productive employees
   2. changes to the doctor-patient relationship
      1. black patients would take preventive care suggested by black provider more
      2. blacks also have lower trust in health care system (Tuskegee study; 40 years US gov, conducting study on 400 black men; the men were not informed; most infamous biomedical research study)
      3. doctors' different perceptions of black patients: intelligence, affiliation, pain tolerance

This results in **lowering health spending substantially for black patients conditional on needs**; known fact for 20 years

### Problem formulation
 
#### cost as target
- widely used approach (industry wide)
  - 10 most widely used algo: supplementary
- not limited to industry

#### there can be alternatige targets
- avoidable future cost (narrower definition): ER cost
- measure of health directly

#### problem formulation challenge; dilemma of which label to choose
- problem formulation: "turning an often amorphous concept we wish to predict into a concrete variable"
- health is challenging; the idea is holistic and multi-dimentionsal
- label choice is important; but each choice has tradeoffs

### Experiments on label choice

#### 3 labels
1. total cost
2. avoidable cost
3. health (comorbidity)

(race is excluded in all cases)

#### results (Tab 2)
- they performed similarly
- **how they built table 2**
  1. select the top k(3%) based on y_pred (row)
  2. sum the y_test (column) within the top k
  3. divide this number by the total sum
- concentration in highest-risk patients table
  - the columns are essentially concentraion by their target metric
  - `top_k_df[outcome].sum() / holdout_pred_df[outcome].sum()`
    - top_k_df[outcome].sum(): sum of top k (sorted by that outcome predictor) predicted values of that column label
    - holdout_pred_df[outcome].sum(): sum of all predicted values of that column label
- notice on the black fraction
  - active chronic condition has highest proportion

### Relation to human judgment

#### human-algo interaction (HAI)
- algo: screening tool -> doctor make the decision
- `>55%`: PCPs are informed and make enrollment decision
- doctor's realized enrollment decision = **how doctors respond to algorithmic predictions**
- ref.: UT Austin presence; Min Kyung Lee (HAI)

#### Tab 3: doctors' decision vs. algo predictions
- 19.2% black in enrollment data (what doctors did)
- 4 scenarios
  1. random sampling from cost model
  2. sampling sicker patients from cost model
  3. just using the cost algo
  4. just using the health algo
- 1 and 3 -> black patient prop. < 19.2% (still very small delta)
- 2 and 4 -> more black patients enrolled
- doctors can "redress" (=correct) the algo but not so much

### Discussion

#### label choice bias
- other examples in health: using mortality and readmission rate to measure hospital performance -> penalize those serving poor and non-white pops
- credit-scoring
- policing
- hiring
- retail
- **can be deadly because it comes from "reasonable" choice**

#### working on solutions together
- authors contacted the algo manufacturer -> they could replicate the results
- using the same model infrastucture
- but with new target: index that combines health and cost
- 84% reduction in bias
- working on table 3 into scalable predictor of multi-dim health issues
- the manufacturer is an industry leader -> hopeful note

#### conclusions
- algo baises are fixable
- algo change wasn't required
- it's more of a data problem; label!
- deep knowledge of domain required
- worth investing research on this

## **impact**
- [2021: FTC: Aiming for truth, fairness, and equity in your company’s use of AI](https://www.ftc.gov/business-guidance/blog/2021/04/aiming-truth-fairness-equity-your-companys-use-ai)
- [2019: New York Regulator Probes UnitedHealth Algorithm for Racial Bias](https://www.wsj.com/articles/new-york-regulator-probes-unitedhealth-algorithm-for-racial-bias-11572087601)
- [2024: Wyden Statement at Finance Committee Hearing on AI in Health Care](https://www.finance.senate.gov/chairmans-news/wyden-statement-at-finance-committee-hearing-on-ai-in-health-care)
- [CA, 2022: Attorney General Bonta Launches Inquiry into Racial and Ethnic Bias in Healthcare Algorithms](https://oag.ca.gov/news/press-releases/attorney-general-bonta-launches-inquiry-racial-and-ethnic-bias-healthcare)

#### Other works
- [An algorithmic approach to reducing unexplained pain disparities in underserved populations](https://www.nature.com/articles/s41591-020-01192-7)
- [Clinical Notes Reveal Physician Fatigue](https://arxiv.org/abs/2312.03077)

#### Recent work of his (FAccT 2023 talk)
- **lack of data access** is a huge problem: health data can be considered a public good
  - new clinical tools: predicting sudden cardiac arrest
  - urgent need to audit many flawed algorithms
    - even hard for FDA; difficult to do rigorous external validation (they can't get the data)
    - unsafe regulatory environment (difficulty of data access)
- data platform for clinical AI
  - company: **dandelion** 
- **closing thoughts: engagement with real problems**
  - agnozing tradeoffs; better proxy? how to persuade hospitals, how to stop bias, etc.
  - generates fascinating, unexpected new technical problems to solve -> constraints -> creative solutions
  - deep satisfaction
    - being in the room where decisions are made
    - bending the process in the right direction
