[
  {
    "objectID": "posts/20230126/index.html",
    "href": "posts/20230126/index.html",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Leakage and the Reproducibility Crisis in ML-based Science\nPresenter: Kate\nAttendees: Athula, Hongsup, Joel, Kate, and Saina"
  },
  {
    "objectID": "posts/20230126/index.html#why-this-paper",
    "href": "posts/20230126/index.html#why-this-paper",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Why this paper?",
    "text": "Why this paper?\nData leakage is a problem that we have all encountered as data scientists and scientific researchers. We just never knew how big! I was drawn to this paper because it attempts to quantify the impact of data leakage. It presents the unique view of considering the impact on ML-based science and shows how widespread and problematic data leakage is and leads to exaggerated claims of predictive performance. However, the frequency of data leakage and the impact of making these types of mistakes could be just as high or higher for ML-based industry applications. As a mixed group of ex ML-based science practitioners and current ML-based industry practitioners, I thought it would be insightful to discuss the eight types of data leakage identified, the proposed mitigation strategy of filling out model info sheets for ML-based science, and if the presented solution is also reasonable for a variety of ML-based industry applications."
  },
  {
    "objectID": "posts/20230126/index.html#defining-the-leakage-and-the-reproducibility-crisis-in-ml-based-science",
    "href": "posts/20230126/index.html#defining-the-leakage-and-the-reproducibility-crisis-in-ml-based-science",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Defining the leakage and the reproducibility crisis in ML-based science",
    "text": "Defining the leakage and the reproducibility crisis in ML-based science\nThis first thing this paper does is narrow the scope of relevant literature to only papers that are used for ML-based science. The authors define ML-based science as only papers “making a scientific claim using the performance of the ML model as evidence.” Furthermore, research findings are termed reproducible “if the code used to obtain the findings are available and the data is correctly analyzed.” Lastly, data leakage is a “spurious relationship between the independent variables and the target variables that arises as an artifaction of data collection, sampling, or pre-processing strategy.” Based on these three definitions, 20 papers with data leakage in 17 fields were found to impact a total of 329 ML-based research papers. These authors make three explicit contributions: 1) present a unified taxonomy of eight types of data leakage that lead to reproducibility issues, 2) propose the use of model info sheets to identify and prevent leakage, 3) quantify the impact of data leakage on model performance of civil war predictions.  \nThe first thing we noticed when looking at Table 1 was that several of the fields are areas with potential high-stakes such as medicine, clinical epidemiology, neuropsychiatry, medicine, and radiology. Engineering applications were excluded from this analysis but several fields sounded very close to this industry such as software engineering and computer security. We didn’t feel that ML-based science should be differentiated from industry-based ML applications. If anything, we felt that data leakage in industry applications may be  harder to detect because they lack a formal peer review process and could have a bigger impact because these models are deployed in the real world."
  },
  {
    "objectID": "posts/20230126/index.html#data-leakage-taxonomy",
    "href": "posts/20230126/index.html#data-leakage-taxonomy",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Data leakage taxonomy",
    "text": "Data leakage taxonomy\n\nL1.1 No test set: This was shocking to us. How does the peer review process allow these papers to be published? Hopefully, in the future reviewers will be more demanding of ML practitioners. This is also the most common type of data leakage identified in Table 1.\nL1.2 Pre-processing on training and test set:  We felt that this was a loaded category. There are many different ways for this type of data leakage to come about. For example, during imputation, under/oversampling, and encoding. For clarity, we thought this category could have been subdivided.\nL1.3 Feature selection on training and test set: This is the second most common type of data leakage found in Table 1.\nL1.4 Duplicates in dataset: This seems more like an error than a type of data leakage.\nL2 Model features that aren’t legitimate: We spent the most time discussing this category. We found it hard to think about proxy variables because they require a lot of domain knowledge. Instead, we like the definition of a feature the model would not have access to when making new predictions.\nL3.1 Temporal leakage: This is when future data is included in model training.\nL3.2 Non-independence between training and test samples: This can be caused by resampling the same patient or geographic location. This type of leakage can also be due to the interpretation of the results. This occurs when there is a mismatch between the test distribution and the scientific claim.\nL3.3 Sampling bias in test distribution: This is choosing a non-representative subset for analysis.\n\nIn general, categories L1.1-4 are clear cut and should be identifiable and avoidable. L2 is difficult because it depends on domain knowledge. Due to the nature of collaboration the researcher with this expert domain knowledge may not be the individual performing the ML analysis. Similarly, L3.1-3.3 are also difficult to detect and avoid if the researcher is using previously collected data and not designing an experiment and collecting data from scratch. In industry, data is often given to ML practitioners and there is no option to immediately collect more data or change the collection methodology. If this is the case, data leakage could be undetectable in many industry applications as ML practitioners are working with the best data at hand. In addition, there are limited strategies for identifying and mitigating these three types of data leakage."
  },
  {
    "objectID": "posts/20230126/index.html#model-info-sheets-to-mitigate-data-leakage",
    "href": "posts/20230126/index.html#model-info-sheets-to-mitigate-data-leakage",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Model info sheets to mitigate data leakage",
    "text": "Model info sheets to mitigate data leakage\nWe appreciated that the authors suggested a mitigation strategy. We found the questions in the model info sheets to be a useful sanity check for any ML practitioner. However, the level of detail requested from the info sheets would likely only be filled out if peer reviewed journals required it.  Even if journals require model info sheets as part of the peer review process, we agree with the authors that there are still limitations. Most notably, the claims in the model info sheets can’t be verified without the code, computing environment, and data. There are some scientific research fields where authors might be unwilling or unable to disclose all information due to privacy concerns. In addition, there is nothing to prevent authors from lying or not having the ML expertise to fill out the model info sheet correctly. Since ML has become more accessible to non-ML experts, subtle cases might not easily be noticed by the author or reviewer.\nIn industry, data privacy is also a concern but ML practitioners within a company could use these model info sheets to identify data leakage in industry based applications. However, there is no way to enforce this across companies or for a user of an industry based ML application to determine if the results are sound. Industry based ML is growing and there are often many groups within a company experimenting with ML. From what we have seen, there are no established within-company standards let alone between-company standards that would require the use of model info sheets to prevent data leakage.\n\n\n\nFigure 1. A comparison of reported and corrected results in civil war prediction papers published in top political science journals."
  },
  {
    "objectID": "posts/20230126/index.html#civil-war-predictions",
    "href": "posts/20230126/index.html#civil-war-predictions",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Civil war predictions",
    "text": "Civil war predictions\nTo understand the impact of data leakage the authors present a case study on civil war predictions. They identified 12 papers on the topic that provided the code and data. Of the 12 papers identified, four claimed superior performance of complex ML models compared to logistic regression. When all types of data leakage were identified and fixed, the complex models did not outperform logistic regression (Figure 1). Figure 1 shows how the performance metric is reduced after data leakage is corrected. In particular, we noticed that model performance of the simple logistic regression model was not impacted as much by data leakage compared to the complex (tree-based) ML methods. We wish the authors had speculated on why."
  },
  {
    "objectID": "posts/20230126/index.html#final-thoughts",
    "href": "posts/20230126/index.html#final-thoughts",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Final thoughts",
    "text": "Final thoughts\nData leakage is a big issue that is not highlighted enough. The authors focused their efforts on identifying data leakage in ML-based science. While we appreciate them proposing a solution to identify and prevent data leakage, we think filling out model info sheets is a big ask and unlikely to be a common practice unless required by the peer review process. In addition, there are efficacy issues with model info sheets and it will be impossible to substantiate each claim. Industry ML applications are likely facing similar challenges with data leakage as ML-based science but these effects will be even more challenging to detect and quantify."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Why ML Journal Club",
    "section": "",
    "text": "In machine learning (ML) community, numerous research papers and new tools come out everyday. For an individual ML practitioner like myself, it is impossible to check every paper and tool, and it is also difficult to know what works and what doesn’t. There are several weekly ML newsletters that provide a summary but I still need to carve out time after work to digest it. Besides, I prefer reading the papers in depth to examine how they exactly work and whether there are any flaws. But of course reading and critiquing papers on a regular basis requires discipline and dedication.\nThis is where a journal club can be useful. A group of people reading and dissecting papers together and having a discussion about them is certainly more fun and educational than doing everything alone. In a journal club, participants usually take turns to present a paper, which makes things easy. We can also hold each other accountable so that we as a group read papers on a consistent pace. Finally, the gathering itself becomes a good networking and information-sharing opportunity.\nI have gather a group of my fellow ML practitioner friends in Austin that I have known personally for several years. We are a diverse group of engineers and researchers with different interests and backgrounds. I am really excited to participate in this journal club with these brilliant friends to critique papers together, share our own practices at work, and socialize."
  },
  {
    "objectID": "posts/20230223/index.html",
    "href": "posts/20230223/index.html",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: “Machine Learning Approaches to Real Estate Market Prediction Problem: A Case Study\nPresenter: Athula\nAttendees: Hannah, Hongsup,Kate"
  },
  {
    "objectID": "posts/20230223/index.html#why-this-paper",
    "href": "posts/20230223/index.html#why-this-paper",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "Why this paper",
    "text": "Why this paper\nIn my work, I deal with real estate data. The data consists of various attributes of real estate properites, some of which follow the ebbs and flows of the mutable economy. I came across this paper while perusing arxiv, and I was compelled by its abstract: using real estate data that spanned 10 years from a single county in Florida, the research compares four ML models that make predictions about whether or not a property’s appraised price matches its eventually listed price. Since the data spans such a long time, the researchers accounted for fluctuations in socio-economic factors over time in the model; this facet was of particular interest to me to conceptualize my own work and I was interested to learn about the authors incorporated these factors in machine learning algorithms to make predictions about the real estate market.\nWhile this abstract seemed reasonably promising to understand the application of ML in real estate evaluation, when I read the paper more closely, I found myself questioning many of the approaches the authors took to answer their central question. Therefore, this blog is structured such that it summarizes the authors’ work as outlined in this paper and offers alternative considerations from our group’s discussion."
  },
  {
    "objectID": "posts/20230223/index.html#the-data",
    "href": "posts/20230223/index.html#the-data",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "The data",
    "text": "The data\nThe authors wanted to build and compare models for predicting home prices. They posit that previous literature addressed home pricing with hedonic regression models, which are linear regression models with specific focus on price of goods, i.e. property price, but that this work did not necessarily account for broader socio-economic factors in their models. With the modeling conducted in this paper, the authors attempted to model home pricing accounting for such factors. However, instead of predicting home pricing as a continous variable as with regression, the authors chose to make home prediction a binary classification problem. The data used in this research consist of ~94,000 rows of publicly available real estate sale data from a single county in Florida, Volusia County.\n\nOutcome variable\nThe way the authors defined the binary outcome variable for this research was interesting: they took the final sale price of the home and compared it to the appraised government price of the home, assigning the variable 1 if the property’s selling price is above the appraised price (which they call high price), and 0 if not (called low price); reasoning on why this designation was useful to classify was left unsaid. More importantly there was no information on how or when the properties were initially appraised by the government, which makes the comparison to final sale price unclear. Government appraisals are technically known as a home’s assessed values and differ from appraised values. Assessed valuations are made to determine yearly property tax rates for a home in a given area; this is generally determined by the broad characteristics of a home and the taxed values of homes in the region. An appraised valuation of a home, on the other hand, is done by an appraiser who does a more thorough check of the features of a home, like its style and appliances, and determines a price for the current market price of your home. Thus, it is possible the authors of the paper were using home assessment prices instead of appraisal prices. The authors don’t provide a tally on how many of the properties are classified as high price, though the authors do show a plot (see paper Fig. 2) that suggests that the total count of high price properties have steadily increased in the 10 years of this data. But despite this fact, appraisal year or sale year are not predictor variables the authors considered.\nAnother way the author could have approached the outcome variables was to have compared the initial listing price to the final sale price. In fact, in the abstract the authors tout that the listing price is baseline against which the final sale price would be compared, but the final models differ from this initial assertion. Regardless, this binary outcome variable falls short because the magnitude of difference between any two price comparisons would be lost; a high price differential of $100k would be treated the same as a lower price differential of $5k.\n\n\nPredictor variables\nFor the predictor variables, the authors chose 21 features or columns from the data for consideration for the models. However, some of the variables were not considered in the right format: parid (property identifier), nbhd (neighborhood code), yrblt (year built), zip21 (ZIP code of area), sale_date (sale date), and luc (property class) were all variables that the authors encoded as continuous variables but should have been treated as categorical variables (see Table 1). Furthermore, the variable sale_date (sale date of the property) was not separated into its component month or year values and instead was used as a single datetime value. From our group’s discussion about this paper, it was also suggested that the authors could have considered time variables in terms of seasons or quarters, as home sales typically vary by such cadences.\nBecause, this data spanned approximately 10 years, the authors attempted to account for the market fluctuations in those years by incorporating economic factors of gross domestic product (GDP), consumer price index (CPI), producer price index (PPI), housing price index (HPI), and effective federal funds rate (EFFR). But, it was unclear where the authors collected these values from and how exactly they matched this collected information to the real estate data. There was little discussion about why these variables were chosen out of the many other indices available, and little information about how these indices were good markers for the central question. Furthermore, it was unclear whether all of these ecoonimic factors were actually available at the time of prediction when the model is used for prediction."
  },
  {
    "objectID": "posts/20230223/index.html#data-culling",
    "href": "posts/20230223/index.html#data-culling",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "Data culling",
    "text": "Data culling\nThe authors took their initial dataset and used a myriad of approaches to reduce and refactor the data. First, the authors correlated the predictor variables to the outcome variables and excluded variables which were not correlated; however, they did not account for variables that are highly correlated with one another (see paper Fig. 5) which is cause for multicollinearity in how the authors interpret the importance of features in the models subsequently. Furthermore, later on when the authors introduce the voting classifier, they deem that “the method performs best when the predictors are as independent from each another as possible.”\nNext, the authors perform a set of preliminary modeling, using the dataset in random forest and XGBoost models and examining the resulting feature importance of those model predictions to make decisions about which of the variables are used in the final models. Every step of this interim modeling process was unclear, thus making it hard to give any credit to the authors’ interpretations of the variable importance charts (Fig. 6-8). Regardless, the authors used these outcomes to further enmesh the predictor variables, using a technique called ‘mean encoding’ to merge together highly ranked variables, creating two new variables which they term F1 and F2. Mean encoding seems to be a technique for encoding variable identities into a model that accounts for how those variables interact with the outcome variables. This type of encoding of variables, in addition to the method of feature selection of predictor variables through interim modeling of the data before processing those select features to a bigger model, is an approach that is teeming with data leakage."
  },
  {
    "objectID": "posts/20230223/index.html#algorithm-predictions-and-results",
    "href": "posts/20230223/index.html#algorithm-predictions-and-results",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "Algorithm predictions and results",
    "text": "Algorithm predictions and results\nFinally, the authors get to their model comparison stage. At this point of the paper it was unclear why exactly these models were being compared against one another. The ultimate question the authors laid out was out of focus and it was unclear what exactly model comparison of the data-leaked dataset would bring.\nThe authors went on to compare the dataset with random forest, XGBoost, voting classifier, and logistic regression classifiers. The authors said they used a 10-fold cross validation method (separate to the 5-fold cross validation method they did for the preliminary modeling for the feature selection step). While hyperparameter tuning is briefly discussed, the authors don’t provide any details on what models were tuned or the parameters. The authors evaluate differences between these classifiers namely by using accuracy, precision, and recall. However, why the authors chose these metrics to evaluate their high-low classifier is unclear, and how it served their goals. Is it more important for the classifier to catch correctly labeled lower labels, or higher labels? And if it’s better at one rather than the other, what impact would it have for the market?\nBy this point, the research question was not sufficiently motivated, and the data was ‘double-dipped’ and overly engineered. In fact, in presenting the comparisons between algorithm performance, the authors showcase how the engineered data performs better at classification of the task, which is of no surprise because the data features were initially chosen because they were better at classifying the task (see paper Fig. 20, 21).\nThe seeming conclusion the authors draw from this section that XGBoost was a superior classifier, but considering all of these factors above, this conclusion did not follow logic. Perhaps a more logical conclusion could have been reached had the authors employed a linear regression model instead; in this way they could have predicted the final sale price as a continous variable instead of a binary variable one as they attempt to do here."
  },
  {
    "objectID": "posts/20230223/index.html#final-thoughts",
    "href": "posts/20230223/index.html#final-thoughts",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "Final thoughts",
    "text": "Final thoughts\nThis paper’s central question is unclear, and deviated from the beginning to end. The paper switched from comparing listing home prices to sold home prices, to appraisal home prices (which were likely assessed home prices) to sold home prices, to figuring out which model is best. If the authors had a more structured story on the questions they wanted to answer, the baseline with which they wanted to compare outcomes, and shored up methods on those comparisons, this would have been a more compelling read. As it stands, however, it is not possible to draw any conclusions from this paper."
  },
  {
    "objectID": "posts/20230622/index.html",
    "href": "posts/20230622/index.html",
    "title": "Let’s Verify Step by Step",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Let’s Verify Step by Step\nPresenter: Akshata\nAttendees: Hongsup, Meghann, Saina"
  },
  {
    "objectID": "posts/20230622/index.html#why-this-paper",
    "href": "posts/20230622/index.html#why-this-paper",
    "title": "Let’s Verify Step by Step",
    "section": "Why this paper?",
    "text": "Why this paper?\nThis is an interesting new paper by OpenAI that discusses how we can apply the same principles we use to solve math problems to AI. It’s interesting to see how a process based supervision approach is superior to outcome based supervision approach."
  },
  {
    "objectID": "posts/20230622/index.html#paper-summary",
    "href": "posts/20230622/index.html#paper-summary",
    "title": "Let’s Verify Step by Step",
    "section": "Paper summary",
    "text": "Paper summary\nThe paper evaluates different approaches to solving a dataset comprising of math problems. With this approach, they trained the model to get the right answer and but also “think” through the problem to arrive at the right answer. The authors mention that there are numerous benefits to this approach:\n\nModel reasoning is explainable as each step of the process is provided a probability score (positive, negative and neutral)\nWrong steps that would lead to incorrect outcomes are identified early on in the training process\nWrong steps that would lead to the correct solution are also identified and discouraged\nModel could be trained to reason and model its behaviors that matches human values which leads to reduction in hallucinations, false information and potentially dangerous outcomes.\n\nThe authors then attempted to showcase superior model performance with a few datasets which we then discussed."
  },
  {
    "objectID": "posts/20230622/index.html#discussion",
    "href": "posts/20230622/index.html#discussion",
    "title": "Let’s Verify Step by Step",
    "section": "Discussion",
    "text": "Discussion\n\nWhy apply language modeling for math problems? Some of us thought a logic-/rule-based approach for creating math solutions might be smarter and more efficient than using a language model because it will requires a massive amount of training data.\nWe also thought multi-step math solutions pose an interesting question because it makes it very clear (hence easy) to identify the correctness of individual steps in inference and deduction, compared to other language problems.\nSome of us questioned why the authors didn’t have enough mention of reinforcement learning although we eventually agreed that the paper’s main focus is about how to define the reward function properly.\nMany of us thought the idea of the process model is very similar to explainable AI (XAI) or interpretable AI. This is of course related to how we need to conduct value assessment when a multi-step solution reaches a correct answer with incorrect steps. This would be a big problem for XAI.\nSpeaking of value assessment, we had a question about the audience of these LLM-generated math solutions because they were extremely detailed, which we assumed are for students. Whether LLMs can tailor solutions based on the level of audience knowledge would be an interesting question to look into.\nWe had a length discussion about the “neutral” label because this would be where human labelers subjectivity will matter a lot. It would’ve been nice if the authors mentioned this more. Also, we were disappointed that the paper didn’t have any mention of who the human labelers were because they were the ones who contributed to the training data.\nCompared to other deep learning papers, we appreciated that the authors did attempt to include many details of the models even though like many deep learning models, we felt the explanation insufficient. For instance, it wasn’t perfectly clear of the multi-step solutions are fed as input to the model during training.\nWe had a brief discussion of model poisoning attacks because ChatGPT and its likes collect user data and we wondered how it can conduct quality control of user input. At the end of the paper, we sensed that the authors can’t have full knowledge of every training data, and then we wonder how they can be sure of what kind of data is used to generate output.\nFigure 3 and similar figures that are followed by this somewhat puzzled us because there was no clear explanation of what the majority voting baseline was. Besides, Figure 3 shows that when the solutions are simple, there wasn’t much performance between the two models. Also, model performance was generally lower when the solution was simple. If LLMs can’t solve simple math problems, we wonder their positive findings matter or not.\nFigure 4 shows performance that is quite low (close to 50%) and we weren’t entirely sure whether this is an acceptable level of performance or not.\nSome of us criticized the use of the term “alignment tax”, which makes AI safety quite negative, which is essentially requirements for any public-facing AI applications.\nUnder 6.3, the authors claim that there wasn’t “clear sign of memorization”, but they did not provide a clear definition of this clear sign. Are they talking about overfitting? Regardless, this requires quantifiable evidence.\nThe multi-step learning model made us think about implementing course-correct steps in model training. For instance, using a callback function, if we can somehow detect the model is on a clearly wrong path, how can we pull it back so that we don’t waste compute resource and train the model in a more efficient way? Some of us thought it might have to do with devising a more interactive and granular cost function."
  },
  {
    "objectID": "posts/20221215/index.html",
    "href": "posts/20221215/index.html",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Zero-Shot Text-to-Image Generation\nPresenter: Hongsup\nAttendees: Hannah, Hongsup, Joel, and Steve"
  },
  {
    "objectID": "posts/20221215/index.html#why-this-paper",
    "href": "posts/20221215/index.html#why-this-paper",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Why this paper",
    "text": "Why this paper\nGenerative AI has a lot of hype in ML community these days. OpenAI’s DALL·E, GPT-3, and ChatGPT are good examples. And there’s also stable diffusion. Since they all have public API, not just ML practitioners but general public can use the models to generate texts or images, which creates even bigger hype around generative AI.\nBut whenever there is hype around something, I think we should be more curious about what’s going on behind the scene. Understanding how it works helps us see through the hype and that is why I chose this paper. We can understand how DALL·E’s text-to-image generative model works, what the authors did to make this happen, and how they validated the result.\nTo understand this paper thoroughly, you need to know other deep learning model frameworks such as transformer, variational autoencoder, and OpenAI’s CLIP (Contrastive Language-Image Pre-training) model. I found these two articles extremely useful, written by Charlie Snell at UC Berkeley. In this post, I will talk about a high-level summary and the interesting discussion we had as a group. If you are interested in more detailed summary of the paper itself, I recommend those two posts."
  },
  {
    "objectID": "posts/20221215/index.html#big-picture",
    "href": "posts/20221215/index.html#big-picture",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Big picture",
    "text": "Big picture\nThe authors created a deep learning model which generate images from a text input. For instance, if you type “hands”, the model will generate images of hands. As the title says, this is done in zero-shot way, meaning that it can generate images that it hasn’t seen before. To be clear, the authors of this paper are not the first ones who created a model like this. There have been precedents but the authors say that the generated images from those still suffer from severe artifacts such as object distortion, illogical object placement, or unnatural blending of foreground and background elements. So the authors made improvements by adopting these two approaches: using a large set of training data and building a bigger model.\nBefore we look into the results, let’s first talk about the model architecture. Their model consists of two parts: variational autoencoder (VAE) and transformer."
  },
  {
    "objectID": "posts/20221215/index.html#variational-autoencoder-vae",
    "href": "posts/20221215/index.html#variational-autoencoder-vae",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Variational autoencoder (VAE)",
    "text": "Variational autoencoder (VAE)\nThe VAE contributes to the generative nature of the model because VAEs have latent representation in the middle that is a probability distribution. Once trained, we can use this distribution to draw samples from it, providing a generative framework. To train the VAE, the authors assumed uniform prior over the latent space. The model can learn the actual prior from the transformer later to generate images that match to text input. To train the VAE, the authors used images with text captions from various sources such as Wikipedia images.\nWhat is interesting about the VAE they used is that it assumes discrete latent distribution instead of continuous. This variant of VAE is called vector-quantized VAE (VQ-VAE). The motivation is that images and texts are discrete than continuous. But this assumption comes with a major complication: a discrete space is non-differentiable (i.e., can’t back-propagate). That’s why VQ-VAE has a codebook, which is essentially a look-up table where a discrete representation is associated with a codebook vector. To be accurate, this paper used a variant of VQ-VAE called dVAE where they made this look-up as a weighted average to further smooth out the space.\nThis VAE also acts as a dimensionality reduction technique because the discrete latent space the authors used has a resolution of 32x32 instead of 256x256, the resolution of the original training images. This brings compression benefit so that the transformer doesn’t have to memorize extremely long sequence but a sequence of length 1024 (=32*32)."
  },
  {
    "objectID": "posts/20221215/index.html#transformer",
    "href": "posts/20221215/index.html#transformer",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Transformer",
    "text": "Transformer\nOnce the VAE is learned, we can abandon the uniform prior assumption and use transformer to learn the actual prior. Transformers help image generation by pixel-wise prediction in an autoregressive way. For instance, given the sequence of previous pixels, the transformer can predict what the next pixel would look like.\nOnce the transformer is trained, when we give a text prompt to the model, the transformer makes predictions for the image latents (32x32 space) in an autoregressive way. Once we have all predictions, we use the dVAE codebook to lookup the vectors and generate the image. Since we can sample the sequence in a new way, we can generate multiple images. The authors used a top k approach to return the best images by ranking the generated images from a candidate pool based on the scores from OpenAI’s CLIP model, which represents how well the images match the caption.\nThe transformer has 12 billion parameters and a good chunk of the paper is dedicated to all the tricks the authors came up with to fit the model in GPU."
  },
  {
    "objectID": "posts/20221215/index.html#discussion",
    "href": "posts/20221215/index.html#discussion",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Discussion",
    "text": "Discussion\n\nAre the results representative enough?\nMost of us were somewhat disappointed by the authors’ model validation. Figures 3 and 4 in the paper gave some idea of how realistic the generated images are but we were not sure whether these were cherry-picked or not because the spectrum of images the model can generate is so wide. Figure 7 showed results from human evaluators. Most of them said the authors’ model was more realistic than the competitors’. Aside from the ethical issues surrounding hiring mturk workers, we thought the number of mturk workers was small (5 people) and the number of images they evaluated was small as well.\n\n\nWhy not investigate model failures?\nWhat was more interesting to us was Fig. 8, the CUB dataset which have images of birds. The example images here looked worse than others and the authors speculated that this was due to the detail-oriented text information of images, which might have been lost during the compression in dVAE. This was a plausible explanation but we wanted to see more in-depth investigation on model failures. There are numerous examples of terrifyingly looking images of hands generated by DALL·E because apparently it keeps failing at generating images of humans hands with five fingers.\nWe also discussed the lack of investigation on model failure from an ethical and responsible AI perspective. If OpenAI was going to publish a public API for a model like this, which would have varying degrees of socio-technical impact (look at all the issues ChatGPT has been creating these days), it would have been more responsible for them to test the model’s capacity more thoroughly and rigorously before rolling it out.\nWe found a model card from their repository and it was disappointingly short and did not address any possible ethical and social ramifications that would be caused by the model.\n\n\nValidity of the scoring metrics\nThe authors used FID and IS scores (generated by the CLIP model) to assess how well the images reflect the text input. The scores were used to rank a pool of candidate images and the model returned top k results. We questioned the validity of the decision behind using these scores because they are model-dependent, which means they are training-data-dependent. Plus, there was no mention of (at least) a qualitative comparison between the training datasets of this paper and the CLIP paper. This made us question the reliability of the CLIP model scores. It might have been interesting to see a batch of images that were ranked high (or low) so that we could judge the validity of the scores and understand the model behavior better.\n\n\nQualitative contribution\nAs in other deep learning papers, it was difficult for us to understand which decisions they made led to their results and advancement. For instance, they highlighted the larger training dataset and the larger model size. What was the measurable impact of each, and which one was more important? Similar to this, it would have been nice if they had some guidance on model tuning and hyperparameter selection to inform other researchers on model architecture design.\n\n\nReproducibility and novelty\nTo be blunt, the main highlight of this paper seemed to be the scale. They were able to use bigger datasets with a bigger model. But let’s be honest, how many academic institutions or companies are able to afford to train a model with 12 billion parameters? Especially without proper model inspection, how can we understand the model properly when we can’t reproduce it easily? Although there were certain elements of novelty especially on their tricks of utilizing GPU resources to train the model, if the scale is the main factor of success, can we really call this as a novel invention?"
  },
  {
    "objectID": "posts/20221215/index.html#final-thoughts",
    "href": "posts/20221215/index.html#final-thoughts",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Final thoughts",
    "text": "Final thoughts\nThanks to the paper, we learned that VQ-VAE and transformer together can generate images from text inputs. However, we questioned the results and model validation especially due to the lack of investigation on model failure. We also thought about ethical aspect of this model being available in public. Just because it belongs to computer vision, which tends to amuse general audience, it does not mean that it is exempt from any social responsibility. And in deep learning with image and speech data, it is often the case that model validation is often looser than tabular data used in industries with higher stakes such as health care, finance, or risk assessment. That said, we would like to learn more about other techniques mentioned in the paper to have a deeper understanding of how they work."
  },
  {
    "objectID": "posts/20221027/index.html",
    "href": "posts/20221027/index.html",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI\nPresenter: Hongsup\nAttendees: Athula, Hongsup, Kate, and Saina"
  },
  {
    "objectID": "posts/20221027/index.html#why-this-paper",
    "href": "posts/20221027/index.html#why-this-paper",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Why this paper",
    "text": "Why this paper\nMany ML articles exist but few talk about how the sausage gets made. They are often based on toy or clean benchmark datasets, which are quite different from what we get in real world. That’s why I enjoy reading survey papers. They interview people in the field like us and try to address common pain points to find a broader picture.\nFor the past several years, I have been noticing a trend in ML community. Many practitioners tend to ignore data quality but instead put all their efforts into models and algorithms. I find it very troubling because many problems in real-world ML are caused by data-related issues. “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI talks about this pattern based on the interviews from dozens of ML practitioners all over the globe."
  },
  {
    "objectID": "posts/20221027/index.html#data-cascades",
    "href": "posts/20221027/index.html#data-cascades",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Data cascades",
    "text": "Data cascades\nThe paper focuses on “data cascades,” a series of compounding negative events due to data-related issues. The authors say the problems are wildly prevalent (92% reported experience at least one type of data cascades). During the discussion, we thought about sampling bias because those who are ignorant of these problems may not be able to recognize them. Indeed, they are often opaque because there are no clear indicators and often discovered later. Data cascades often lead to technical debt and harm to beneficiary communities. They can sour relationships between stakeholders and in extremely cases, force ML practitioners discard entire datasets. Figure 1 shows the schematic of the data cascades. We thought the figure wasn’t particularly informative because too many arrows exist between the points. We hoped that the authors explained why there aren’t arrows between certain points.\n\n\n\nFigure 1. Data cascades in high-stakes AI"
  },
  {
    "objectID": "posts/20221027/index.html#high-stake-domains",
    "href": "posts/20221027/index.html#high-stake-domains",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "High-stake domains",
    "text": "High-stake domains\nData cascades are more critical in high stake domains such as landslide detection, suicide prevention, and cancer detection. There are several reasons:\n\nMore ML applications are deployed in these domains where more direct humanitarian impact exists.\nThis impact can be disproportionate towards vulnerable communities.\nIt is often very challenging to acquire high quality data in these domains.\nThe problems frequently require more multidisciplinary approach.\n\n\n\n\nTable 1. Summary of participant demographics; Domain\n\n\nThe authors find that the problems are due to human factors. Unfortunately solutions have been focusing on other issues such as database, legal, or license. To gather firsthand experience in the field, the authors interviewed 50+ ML practitioners all over the world, ranging from the US to India and African countries, and from founders to developers. Table 1 summarizes various high-stake domains. It was fascinating for us to learn that ML is used in areas as landslide detection, poaching prevention, or regenerative farming. It would have been more interesting to see how data cascades create specific negative consequences in some domains."
  },
  {
    "objectID": "posts/20221027/index.html#data-cascade-triggers",
    "href": "posts/20221027/index.html#data-cascade-triggers",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Data cascade triggers",
    "text": "Data cascade triggers\nThe authors introduce three triggers that cause data cascades.\n\nPhysical world brittleness\nPhysical world changes over time and thus often ML systems can’t produce robust results. Data drifts due to hardware (measurements) and environmental changes are commonly mentioned in ML Ops literature. In high-stake domains, they become more pronounced because training data are very limited and policy or regulation changes can impact the ML systems in various ways.\n\n\nInadequate application-domain expertise\nMost ML practitioners are not equipped with domain knowledge. All of us admitted that our academic background does not match to the domain that we work in. Even though close collaboration between domain experts and ML practitioners is always emphasized, in practice the authors find that domain experts are often detached from the larger impact of the applications. The authors explain two specific types of problems:\nSubjectivity in ground truth: Areas such as insurance claim approval or medical imaging for cancer detection involve highly specialized and often subjective decision-making. To build a reliable and robust ML system, it is necessary to standardize the decision-making criteria and find consensus. However, ML practitioners are asked to rush through the development process, and thus do not have time to address it.\nPoor domain expertise in finding representative data: ML practitioners often start building ML applications without involving domain experts much because the practitioners simply believe that data are reliable. However, because they lack domain knowledge, practitioners make incomplete assumptions, which results in disparity between data collection and deployment. This often leads to poor and unreliable model performance.\n\n\nConflicting reward system\nData collection and any data-related work are often considered non-technical and undervalued. The situation gets worse for frontline workers because they are asked to collect and curate field data on top of their existing responsibilities but they are not well compensated.\n\n\nPoor cross-organizational documentation\nMetadata about data collection, quality, and curation are also often missing. Some good practices the authors suggest involve keeping good documentation on reproducible assets such as data collection plan, data strategy handbooks, design documents, file conventions, field notes, and so on."
  },
  {
    "objectID": "posts/20221027/index.html#broader-context",
    "href": "posts/20221027/index.html#broader-context",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Broader context",
    "text": "Broader context\nData cascades discussion can be extended to bigger problems in ML community.\n\nIncentives and currency in AI\nBecause of the low incentives, data-related work are not rewarded or even tracked. This makes us difficult to get buy-in from stakeholders. The situation is similar in academia as well. Most practitioners and researchers focus on developing algorithms but they rarely mention or work on data. The title “Everyone wants to do the model work, not the data work” was a verbatim from an interviewee. Unfortunately, we were all able to relate to this quote.\n\n\nData education\nMost ML or data science curricula lack any mention of data quality or ethics. They use toy datasets or very clean benchmark datasets. As experience ML practitioners, we wholeheartedly agreed with this finding. We lamented that these courses do not prepare students with practical knowledge because one never works with clean datasets in real world. Some of us have experienced this pattern firsthand because we have been interviewing candidates for an ML practitioner position and found that the candidates who only worked with clean datasets (or primarily worked on algorithms) often lack basic practical ML knowledge.\n\n\nData bootstrapping\nData bootstrapping describes ML practitioners’ use of other data sources such as established data services or existing datasets to create their own dataset. For most of us, it was surprising to learn that many ML practitioners in high-stake domains in countries from Global South had to collect data from scratch. We agreed that challenges in data collection and lack of access to quality data would create inequality between countries."
  },
  {
    "objectID": "posts/20221027/index.html#how-to-address-data-cascades",
    "href": "posts/20221027/index.html#how-to-address-data-cascades",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "How to address data cascades",
    "text": "How to address data cascades\nThe authors introduce several ways of addressing the problem of data cascades. They introduce the concept of “data excellence”, an effort to “focus on the practices, politics, and values of humans of the data pipeline to improve the quality and sanctity of data, through the use of processes, standards, infrastructure and incentives”.\n\nFrom goodness-of-fit to goodness-of-data\nThe first is to use the right metric to evaluate data quality. Many ML practitioners use model performance metrics such as accuracy and RMSE to evaluate data quality. Some of us had a similar experience. We had to argue that model metrics shouldn’t be used to make a decision on data-pipeline and data-quality features. We hope that the authors can introduce specific examples of goodness-of-data metrics in the future.\n\n\nIncentives for data excellence\nThere are several ways to address the low incentives of data work. First, journals and conferences should require dataset documentation, provenance, and ethics as mandatory disclosure. Second, organizations should reward data-related work similar to how good software engineering is rewarded. Finally, partnership between stakeholders can nurture data excellence by sharing the reward on data-related work such as data collection, anomaly identification, and model verification.\n\n\nEducation and visibility\nThe authors argue for real-world data literacy in AI education, which includes training on data collection, infrastructure building, data documentation, data sense-making, and data ethics and responsible AI in general. Increasing the data visibility in ML lifecycle is important as well; implementing good monitoring system is one of the most important ML Ops practices anyway."
  },
  {
    "objectID": "posts/20221027/index.html#final-thoughts",
    "href": "posts/20221027/index.html#final-thoughts",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Final thoughts",
    "text": "Final thoughts\nSome of us found this paper vindicating because we have been advocating for data quality at work and had to fight for the attention it deserves. The paper helped us share our own practices at work that address data-related issues especially data collection, curation, and post-deployment data problems. Even though we generally agreed with authors’ suggestions, some of us wanted something more specific, like a case study. Overall, we found the paper interesting and insightful. We thought it would be beneficial to read the paper with our colleagues at work to start a discussion for data excellence."
  },
  {
    "objectID": "posts/20230720/index.html",
    "href": "posts/20230720/index.html",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types\nPresenter: Saina\nAttendees: Brian, Hari, Hongsup, Meghann"
  },
  {
    "objectID": "posts/20230720/index.html#why-this-paper",
    "href": "posts/20230720/index.html#why-this-paper",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "Why this paper?",
    "text": "Why this paper?\nProduct discovery plays a pivotal role in the success of an online store, as it not only fuels revenue generation but also exerts a profound influence on the overall customer experience. In the vast landscape of e-commerce, where countless products are bought and sold through platforms like Amazon, the task of efficiently arranging and presenting product information for indexing and seamless search functionality stands as a continuous and formidable challenge.\nAnother challenge arises due to the lack of a unified language in product descriptions. Given that retailers curate the information, the distinct characteristics and attributes of products are predominantly delineated within titles or descriptions, rather than being uniformly collected. Consequently, the extraction of this information necessitates delving into unstructured textual data.\nResearchers at Amazon address this issue by introducing a pipeline named AUTOKNOW. This pipeline comprises a set of machine learning algorithms and techniques that create taxonomy, recognize products, and ultimately construct a product knowledge graph. This graph serves the purpose of organizing catalog information and enhancing product discovery through indexing and downstream functionalities of search and product recommendation."
  },
  {
    "objectID": "posts/20230720/index.html#summary-of-techniques",
    "href": "posts/20230720/index.html#summary-of-techniques",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "Summary of techniques",
    "text": "Summary of techniques\n\n\n\nFigure 2: AutoKnow architecture, containing ontology suite to enrich product ontology and data suite to enrich product structured data.\n\n\nThis paper delves into the intricacies of the Retail Product Domain, where challenges compound due to the rapid influx of new products and the extensive range of associated attributes, such as color that can have many values. This domain presents a diverse spectrum of attributes spanning various product types, even as some attributes are shared among products. To address the inherent complexity, the study narrows its focus to four domains: grocery, health, beauty, and baby. Eligible products for the study must have at least one monthly page view. It is observed that products are characterized by dozens to hundreds of attributes per product, with the attribute count varying from 100 to 250 across different product types.\nThey start organizing the product taxonomy by focusing on types and hypernyms relationships, and curating attributes and synonyms for each type. They also focus on a knowledge graph (KG) that consists of triples in the form of (subject, predicate, object), where subjects are entities with IDs belonging to various types, objects can be entities or atomic values, and predicates signify relationships. This study concentrates on broad graphs where the topic type is products. Product types are organized hierarchically, forming a taxonomy. The paper assumes input sources, including a product catalog with attributes and customer behavior logs, query/purchase logs, customer reviews and question/answer. Intuitively, most product types appear in title, and search queries.\nThe taxonomy enrichment process consists of two steps. Firstly, a type extractor is trained to identify new product types from titles and queries, utilizing an open-world tagging model. Secondly, type attachment is achieved through a binary classification task, employing a graph neural network (GNN) module that captures signals from customer behaviors. This GNN-based approach refines type representations and combines them with semantic features for classification. Distant supervision is applied for model training, utilizing existing taxonomy and customer behavior data for positive and negative labels.\nThe Data Imputation component addresses the Structure-sparsity challenge by enhancing coverage through structured value extraction from product profiles. This process involves extracting new (attribute, value) pairs for each product from its profiles. State-of-the-art techniques using BIOE sequential labeling and active learning have been effective for solving this at the type-attribute level, as demonstrated in Equation (1) with BiLSTM and CRF.\n\\[(y_{1},y_{2},...y_{L}) = \\text{CRF}(\\text{BiLSTM}(e_{x_{1}},e_{x_{2}},...,e_{x_{L}}))\\]\nHowever, scalability to handle large numbers of product types and attributes is an issue. To overcome this, the paper introduces a novel taxonomy-aware sequence tagging approach conditioned on product type. The model’s predictions are influenced by the product type’s pre-trained hyperbolic-space embedding, preserving hierarchical relationships within the taxonomy. The approach also employs multi-task learning to enhance the identification of tokens indicating product type and tackle catalog misclassification or missing type information through shared BiLSTM training for sequence tagging and product categorization.\nNext step is the relation discovery which is based on product attributes where various sets of attributes apply to different product types, and only a fraction significantly affects shopping decisions. Identifying applicable and important attributes for thousands of types is essential. Formally, the task involves determining if an attribute applies to a product type and its importance for purchasing decisions. This paper employs classification and regression models—specifically, Random Forest—to assess attribute applicability and importance. The models utilize seller behavior (attribute value coverage and frequency in profiles) and buyer behavior (frequency in search queries, reviews, Q&A). Annotations from in-house and MTurk sources are used for model training. The approach distinguishes between subtle differences, such as attributes required for coverage but not always pivotal for decisions. The trained models are applied to all attribute-type pairs for decision-making.\nThis model is designed to figure out if a suggested attribute value matches the context provided by product descriptions and the product categories they belong to. The model takes raw input, which is a combination of words from product descriptions, product categories, and the suggested attribute value. These words are turned into embedding vectors that capture their meaning, source, and position in the text. The model then processes these embeddings using a multi-layer transformer to create a representation that summarizes all this information. This summarized representation is used to calculate a score, which tells us how well the suggested attribute value fits in with the context. To train the model, they use examples from the product catalog. They generate both correct and incorrect examples, and the model learns to distinguish between them. During testing, the model is applied to different attribute-value pairs, and those with low scores are considered as incorrect matches."
  },
  {
    "objectID": "posts/20230720/index.html#final-points",
    "href": "posts/20230720/index.html#final-points",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "Final Points",
    "text": "Final Points\nAt the time of this paper presentation, Autoknow is deployed in production in collecting over 1B product knowledge facts for over 11K distinct product types. The approach is validated by demonstrating the expansion of existing Amazon ontology by 2.9 times where precision is improved by 7.6% and recall by 16.4%. The collection suite of technologies are Apache spark distributed system, tensorflow for deep learning, amazon deep graph library for graph neural network for taxonomy. The relationships are derived using Spark ML, and an AWS SageMaker instance is used for training the imputation component.\nFinally, they mention an important lesson, which is that sometimes they need to move away from the product tree and consider multiple parents. In other words, a single sub-type cannot be clearly defined in many instances."
  },
  {
    "objectID": "posts/20230720/index.html#discussion",
    "href": "posts/20230720/index.html#discussion",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "Discussion",
    "text": "Discussion\nFirst, Saina gave us a great introduction about the problem domain, knowledge discovery. She said extracting and understanding attributes is essential in knowledge discovery in retail, which has been relatively easy before the era online shopping because merchants usually provided well-organized product catalogs. However, widely variable free text formats started emerging with the rise of e-commerce. Search engines have been trying to automatically extract attributes from the text input and to build a good taxonomy to create hierarchical relationship between products. Since products come and go constantly, this process requires interactive process of attribute extraction and taxonomy building. As the authors mentioned in the paper, some of the main challenges are to clean, curate, and understand the data, and to correct errors and prevent abuse of attributes.\nWe spent a lot of time trying to understand the sequential labeling problem the paper used extensively. The authors frequently cited their OpenTag paper, which they used to automatically generate training labels. These tags are used in many stages of their model pipeline including taxonomy enrichment and type attachment. The algorithm takes a sequence of text tokens and return a sequence of BIOE (“beginning”, “inside”, “outside”, “end”) labels. Then the authors used the returned tags as true labels for subsequent ML algorithms (distant supervision and regular supervision). We initially thought this BIOE labeling system came from the OpenTag paper but we later learned that it was based on a 2015 paper about bidirectional LSTM-CRT (conditional random field) for sequential tagging, which the authors used in this paper as well. In short, the final CRF layer in this deep learning model returns a probability for every plausible label sequences (sequence likelihood). We thought it would have been nice for the authors to give a quick overview of the algorithm and address potential risk of ML-generated labels as true labels in downstream ML modeling.\n\n\n\nA BI-LSTM-CRF model (Huang et al. 2015)\n\n\nWe also discussed other algorithms used in the paper to improve our understanding. First, we suspected that the regression problem they attempted to solve was based on MTurk workers’ subjective level of attribute importance (not a feature importance of an ML model) even though we doubted the choice of using only 6 MTurk workers. We also discussed the difference between semi-supervised and weak-supervised learning models: the former is about “based on what is already labeled, label some more” and the latter is about “based on your knowledge, label some more”. Finally, we talked about hyperbolic space embedding, which is excellent at preserving graph distances and complex relationships in very few dimensions Finally, as the authors mentioned at the end, we thought it would make sense to have a single model that takes care of data imputation and cleaning because in this paper, the cleaning is more of correction in imputed values.\nWe generally enjoyed reading this paper because it uses a plethora of various ML algorithms extensively, which gave us a good opportunity to get to know them. However, some of us questioned how this model pipeline can be maintained especially with the arrival of new information. We thought some stages of the pipeline might be more resilient to changes (such as the attribute importance estimation) but taxonomy changes may not. As we’ve seen in deep learning papers many times, we also questioned the validation aspect because the number of samples used in validation (a few hundreds triples) was much smaller than the actual dataset (one billion triples)."
  },
  {
    "objectID": "posts/20230329/index.html",
    "href": "posts/20230329/index.html",
    "title": "Visualization in Bayesian workflow",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Visualization in Bayesian workflow\nPresenter: Hongsup\nAttendees: Kate, Saina"
  },
  {
    "objectID": "posts/20230329/index.html#why-this-paper",
    "href": "posts/20230329/index.html#why-this-paper",
    "title": "Visualization in Bayesian workflow",
    "section": "Why this paper?",
    "text": "Why this paper?\nThis paper summarizes types of data visualization that we can use in Bayesian modeling and inference. This is also a good overview of how to do Bayesian modeling properly, including validating results. The fact that the main author is one of the maintainers of the stan package, is another motivating factor."
  },
  {
    "objectID": "posts/20230329/index.html#paper-summary",
    "href": "posts/20230329/index.html#paper-summary",
    "title": "Visualization in Bayesian workflow",
    "section": "Paper summary",
    "text": "Paper summary\nGiven a problem, we incorporate our scientific knowledge into a causal (generative) model to simulate how the relevant variables are produced (input and output). Researchers need more than null hypothesis because it doesn’t talk about how your observation is generated. We can use a DAG as a scientific causal model and data generation process can be expressed in a generative model, which is often accompanied with Bayesian data analysis (BDA). BDA is particularly useful because we can simulate data from the model directly to design and debug during inference. To effectively estimate a posterior distribution, we need computational methods such as MCMC and others. One may say Bayesian might be an overkill but it’s extremeley useful for typical modeling problems such as measurement error, missing data, latent variables, and regularization. Again, it’s also generative!\nThe paper uses data visualization to express the followings: - Exploratory data analysis to come up with a proper model - Prior predictive distribution check to check model’s assumption - MCMC computational check to evaluate the sampling process - Posterior predictive check to validate inference process\nThis paper is based on R’s bayesplot but there are several python equivalents to this such as pymc, arviz, and numpyro. It uses a global air polllution dataset (pm2.5 particles) measured from satellite images. The goal of modeling is to predict the level of pm2.5 from the images. Hence, this is a regression problem. Fig. 1 shows the linear trend between the two variables of interest but also shows how sparse the data is depending on groups.\n\nExploratory data analysis (EDA)\nEDA is essential to understand and capture features and heterogeneity of data. The data pattern helps building a group-up modeling strategy to address the imbalance and sparsity of data. The authors emphasize that the top-down approach in typical ML communities these days is to throw everything into a non-parametric procedure, which can severely overfit. Fig. 2 shows that simple regression works pretty well, especially when the group identity is taken into account, which means we need a hierarchical approach.\n\n\n\nFig. 4: Visualizing the prior predictive distribution\n\n\n\n\nPrior predictive check\nInstead of using a non-informative or uniform prior, weakly informative prior is always recommended, which takes into account modeler’s perspective. In the paper, we assume that the target varialbe follows a normal distribution defined by a mean and a \\(\\sigma\\) where the mean is a linear function of input variable (satellite data) and linear coefficients, which also have priors (0 mean and std (\\(\\tau\\))).\nPrior predictive checks are useful to visualize the impact of our assumption for prior definition. If we use a vague prior (very wide range, Fig. 4a), ranges from the sample don’t match the observation. Fig. 4b shows a much tighter prior where the simulated data points still overestimate but are in a much reasonable range. Obviously, tighter and sensible priors are better.\n\n\n\nFig. 5: Diagnostic plots for Hamiltonian Monte Carlo\n\n\n\n\nMCMC diagnostics\nSuccess of Hamiltonion Monte Carlo (HMC) depends on how smooth the posterior distribution is; if not smooth, HMC proposal diverges from the true trajectory, which may signal that the trajectories are stuck. Healthy MCMC samples, shown as a bivariate plot in Fig. 5a, shouldn’t have obvious patterns. The funnel shape there is due to \\(\\beta_{11} \\sim N(0, \\, \\tau_{1}^{2})\\) where small \\(\\tau_{1}\\) means \\(\\beta_{11}\\) distribution is narrow. The parallel co-ordinate plot (Fig. 5b) also shouldn’t have any particular structure.\n\n\n\nFig. 9: Graphical check of leave-one-out cross-validated probability integral transform (LOO-PIT)\n\n\n\n\nPosterior predictive check\nIf a trained model has a good fit, generated data from the model should follow observations. Posterior predictive checking is mostly qualitative but it’s effective to compare empirical and simulated values (Fig. 6). Fig. 7 shows checking whether samples from models captures other statistics such as skewness (kurtosis) and Fig. 8 shows how we can evaluate whether samples from models capture summary statistics such as median Fig. 9 shows using visualization that checks whether leave-one-out cross-validation (LOO-CV) predictive cumulative density function is uniform or not, similar to the idea of a K-S test.\n\n\n\nFig. 10a: Model comparisons using leave-one-out (LOO) cross-validation. The difference in pointwise ELPD values obtained from PSIS-LOO\n\n\n\n\nModel comparison\nWhen comparing models, Bayesian data analysis allows detailed examination of individual data points on a given model. We can use cross-validated LOO predictive distribution to do so; it shows the distribution of a data point from a model that’s built without that data point (i.e., LOO). We can use expected log-predictive densities (ELPD), which is essentially the mean of the log probability of each data point i, computed with posterior that omits the point i (the bigger the better). We use Pareto-smoothed importance sampling (PSIS) to compute this metric (we don’t have to fit the models N times). Once we have ELPD value for every data point of a model, we can repeat this for all the models we have and make comparison (Fig. 10a).\n\n\n\nFig. 10b: Model comparisons using leave-one-out (LOO) cross-validation. The \\(\\hat{k}\\) diagnostics from PSIS-LOO for Model 2\n\n\nSimilarly, we can compute \\(\\hat{k}\\) as well which represents degree of influence of a specific observation. High value means this data point is “unexpected”, meaning that it is likely to be an outlier or the model struggles to make valid prediction for this data point."
  },
  {
    "objectID": "posts/20230329/index.html#discussion",
    "href": "posts/20230329/index.html#discussion",
    "title": "Visualization in Bayesian workflow",
    "section": "Discussion",
    "text": "Discussion\nWe had a lengthy discussion about the choice of prior and how tricky it can be. As the authors mentioned in conclusion, we were also slightly worried about double-dipping the data when running prior predictive checks and potential data leakage. It was also interesting to share our own experience on Bayesian inference ranging from dealing with prior assumptions, model comparison, to decision-making with uncertainty. But we all agreed that Bayesian data analysis is more empowering for us modellers compared to the typical top-down approach in ML where we often don’t have any generative models about data. We also agreed that Bayesian data anlsysis is absolutely more powerful when we suffer from a small data problem.\nBut we also some downsides of Bayesian data analysis too. It’s difficult to scalable and someone we ML practitioners are not the domain experts and without the domain expertise, it’s difficult to come up with a good DAG. Due to the nature of Bayesian analysis where we don’t often make a point-estimate summary, we appreciated that the paper spent a good amount of time discussing how to summarize a posterior distribution. We also discussed the importance of loss function when decision making with uncertainty.\nIn general, we liked the paper but we thought it fell slightly short because it wasn’t focusing on understanding scientific mechanism but rather on predictive modeling nature of Bayesian analysis. When it comes to model comparison particularly, we thought it’s important to evaluate the structure of the model too in addition to evaluating the goodness of fit. For instance, if the model performance varies across the regions, the way we compare the models would like to change as well, and potentially the DAGs too."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Austin ML Journal Club",
    "section": "",
    "text": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types\n\n\n\n\n\n\n\npaper\n\n\n\n\nIn e-commerce, it is challenging to organize and categorize products that are described by merchants in various ways. Finding a unified language and taxonomy has always been an underlying effort with commerce. This paper uses various ML algorithms to address this challenge.\n\n\n\n\n\n\nJul 20, 2023\n\n\nSaina Lajevardi, Hongsup Shin\n\n\n\n\n\n\n  \n\n\n\n\nLet’s Verify Step by Step\n\n\n\n\n\n\n\npaper\n\n\n\n\nThis is an interesting new paper by OpenAI that discusses how we can apply the same principles we use to solve math problems to AI. The paper evaluates different approaches to solving a dataset comprising of math problems. With this approach, they trained the model to get the right answer and but also “think” through the problem to arrive at the right answer.\n\n\n\n\n\n\nJun 22, 2023\n\n\nAkshata Mohan, Hongsup Shin\n\n\n\n\n\n\n  \n\n\n\n\nVisualization in Bayesian workflow\n\n\n\n\n\n\n\npaper\n\n\n\n\nThis paper summarizes types of data visualization that we can use in Bayesian modeling and inference. It also provides a good overview of how to do Bayesian data analysis properly, including model validation such as prior and posterior predictive checks.\n\n\n\n\n\n\nMar 29, 2023\n\n\nHongsup Shin\n\n\n\n\n\n\n  \n\n\n\n\nReviwing a Case Study About Real Estate Market Prediction\n\n\n\n\n\n\n\npaper\n\n\n\n\nThe paper reviewed here attempts to predict outcomes about the real estate market with binary classification. Though the paper’s research design and results were lacking, it gave us a chance to have a discussion about practices for experimental design.\n\n\n\n\n\n\nFeb 23, 2023\n\n\nAthula Pudhiyidath\n\n\n\n\n\n\n  \n\n\n\n\nLeakage and the Reproducibility Crisis in ML-based Science\n\n\n\n\n\n\n\npaper\n\n\n\n\nData leakage in a common problem in ML-based science leading to reproducibility failures and overly optimistic conclusions. We discussed 8 types of data leakage and the use of model info sheets to identify and reduce all leakage types.\n\n\n\n\n\n\nJan 26, 2023\n\n\nKate Behrman\n\n\n\n\n\n\n  \n\n\n\n\nZero-Shot Text-to-Image Generation\n\n\n\n\n\n\n\npaper\n\n\n\n\nThere is so much hype in generative AI. But how does it actually work? We discuss OpenAI’s DALL-E paper to understand model architecture but more importantly, whether their model validation is solid and reasonable.\n\n\n\n\n\n\nDec 15, 2022\n\n\nHongsup Shin\n\n\n\n\n\n\n  \n\n\n\n\n“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI\n\n\n\n\n\n\n\npaper\n\n\n\n\nGarbage in, garbage out. It seems like a lot of people in the ML community still don’t understand this logic. We discuss poor data-handling practices and their critical ramifications.\n\n\n\n\n\n\nOct 27, 2022\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy ML Journal Club\n\n\n\n\n\n\n\nnews\n\n\n\n\nWelcome to our journal club! I talk about why I organized an in-person journal club with my fellow ML practitioner friends in Austin, TX.\n\n\n\n\n\n\nOct 17, 2022\n\n\nHongsup Shin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "Blogging",
    "section": "",
    "text": "Our blog uses quarto platform and it is hosted via GitHub Pages. This is a short guide for uploading blog posts using quarto."
  },
  {
    "objectID": "quarto.html#set-up",
    "href": "quarto.html#set-up",
    "title": "Blogging",
    "section": "Set up",
    "text": "Set up\n\nInstall quarto. The VS Code option is straightforward.\nClone the blog repository.\nCreate an issue (title example: “Upload a blog post for the Feb 2023 meeting”) and assign yourself to the issue.\nCheck out a new branch for the issue (branch name example: “16-write_blog”)."
  },
  {
    "objectID": "quarto.html#write",
    "href": "quarto.html#write",
    "title": "Blogging",
    "section": "Write",
    "text": "Write\n\nCreate a folder in /posts. The name of the new folder should be the date of the meeting in YYYYMMDD format (e.g., posts/20221027/).\nCreate index.qmd inside the folder. This is the main file for your post. The top section of index.qmd is a YAML block with document options. You must fill out title, author, date, description, categories.\n---\ntitle: \"Paper title\"\nauthor: \"Author full name\"\ndate: \"YYYY-MM-DD\"\nimage: \"This image will show on the main page (this is optional).jpg\"\ndescription: Short description of the post. It will show on the main page.\ncategories: [paper]\n---\nBefore writing the body, please include the following right after the YAML block:\n:::{.callout-note}\n- Paper: [paper title](link to original paper)\n- Presenter: [first name](linkedin profile)\n- Attendees: [first name 1 (alphabetical order)](linkedin profile), [first name 2](linkedin profile), ... \n:::\nTo give everyone some context, start the body with “Why this paper” paragraph.\nYou can include any image files in the same folder as index.qmd.\nRun quarto preview <file name> to preview the rendered look of your post on the website."
  },
  {
    "objectID": "quarto.html#render-and-review",
    "href": "quarto.html#render-and-review",
    "title": "Blogging",
    "section": "Render and Review",
    "text": "Render and Review\n\nRun quarto render once you are done. This automatically generates and modifies html files in the /docs and the main folders.\nAdd all the changes, commit, and make a merge request to main.\nAssign the attendees of the meeting as reviewers, and go through a review process. Leave a message on Slack about the merge request.\nUpdate the manuscript based on reviewers’ comments.\nOnce approved, merge the branch, close your issue ticket, and delete the remote branch.\n\nCurrently the blog is hosted via gh-pages branch (not main). Thus, after main is updated, we will rebase it to gh-pages, and the blog will be automatically updated with a new post."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, we are a group of machine learning (ML) practitioners in Austin, TX. We read ML papers (or blog posts, etc.) together and discuss them in person once a month. If you are interested in learning about how our journal club works and how to join, plesae visit our Tips page."
  },
  {
    "objectID": "about.html#hongsup-shin",
    "href": "about.html#hongsup-shin",
    "title": "About",
    "section": "Hongsup Shin",
    "text": "Hongsup Shin\nHongsup is the organizer of the Austin ML Journal Club. He organizes the club meetings, and maintains the blog and the journal club GitHub repository.\nHe is a Principal ML Research Engineer at Arm and used to be a volunteer data scientist at Texas Justice Initiative, a criminal justice non-profit in Austin, TX. His background is computational neuroscience and behavioral ecology. He is interested in ML Ops, responsible AI, and AI ethics."
  },
  {
    "objectID": "about.html#kathrine-kate-behrman",
    "href": "about.html#kathrine-kate-behrman",
    "title": "About",
    "section": "Kathrine (Kate) Behrman",
    "text": "Kathrine (Kate) Behrman\nShe is a Staff ML Research Engineer at Arm in Austin, TX. She used to work for the USDA-NRCS and USDA-ARS as a Research Ecologist. Her background is in landscape and evolutionary ecology. She is interested in human AI interactions and responsible use of AI."
  },
  {
    "objectID": "about.html#athula-pudhiyidath",
    "href": "about.html#athula-pudhiyidath",
    "title": "About",
    "section": "Athula Pudhiyidath",
    "text": "Athula Pudhiyidath\nShe is a Data Scientist at PrimeStreet and is based in Austin, TX. Her background is in cognitive neuroscience. She is interested in keeping up with the ever-changing landscape of AI and ML and its applications to daily life and business."
  },
  {
    "objectID": "about.html#joel-afriyie",
    "href": "about.html#joel-afriyie",
    "title": "About",
    "section": "Joel Afriyie",
    "text": "Joel Afriyie\nJoel is a Machine Learning Engineer at Acrisure based here in Austin, TX. His background is in Computer Science and Fair ML. He is interested in MLOps, Music ML, Fair and Responsible AI, and Software and Data Engineering."
  },
  {
    "objectID": "about.html#akshata-mohan",
    "href": "about.html#akshata-mohan",
    "title": "About",
    "section": "Akshata Mohan",
    "text": "Akshata Mohan\nAkshata Mohan does machine learning at Cloudflare. She is also one of the organizers of the local women in data science meetup in Austin. Her background is in information science and analytics and she’s interested in the applications of ML to cybersecurity."
  },
  {
    "objectID": "about.html#saina-lajevardi",
    "href": "about.html#saina-lajevardi",
    "title": "About",
    "section": "Saina Lajevardi",
    "text": "Saina Lajevardi\nSaina is a senior Data Scientist at Adobe Commerce, located in Austin, TX. Previously, she worked at Arm Cloud Technologies, where she developed machine-learning based frameworks in the IoT space. Her primary interests lie in problem-solving with a pragmatic approach."
  },
  {
    "objectID": "notes/2023-02-23.html",
    "href": "notes/2023-02-23.html",
    "title": "Austin ML Journal Club",
    "section": "",
    "text": "Meeting notes\nAll of us found a lot of problems with this paper. Here are some discussions points.\n\nFrom the abstract, having conclusion as “model X is the best” is honestly not publishable. It’s simply stating that they did model tuning, which is really a few lines of code and doesn’t mean anything more.\nThe fact that they are framing this as classification, not regression is highly problematic and misleading. Making this as classification ignores the distribution and magnitude of difference between appraisal and selling price.\nData leakage in feature space: we suspect some features may contain information about the target or they may not be available at the time of prediction\nEven though they didn’t have any class imbalance (class ratio was almost 1:1), according to their graphs, it’s possible that class imbalance would have changed over time.\nIt was not clear what the goal of the paper was especially on what they want to do with the ML model. We had a discussion about whether the authors should’ve used listing price instead of appraisal. They also didn’t seem to consider the common sense that government appraisal might lag and how their model will account for it.\nThe dataset is limited to a single county and yet in conclusion they made overreaching conclusions.\nIt’s also unclear how they got the socioeconomic data, which is critical because they seemed to emphasize them in Introduction.\nTheir feature representation was highly problematic (Tab. 1) mainly because they treated categorical features like zip code as numeric values.\nThere were also new features (such as month) they later mentioned in the paper, which were missing in their feature set table. We also suspect that they used month as numeric variable not as ordinal/categorical variable.\nThe sale date feature also could’ve been divided into more granular and meaningful features such as year, month, quarter, school season, etc.\nEven with feature importance calculation and feature selection, we couldn’t find what they exactly want to do with the prediction and how they want to use it. What exactly can become actionable items is also missing. This is why we were confused about why they did correlation study because it wasn’t really used much in their modeling. Besides, we don’t know whether they are interested in building interpretable models and whether they care about conducting sensitivity analysis to understand the impact of feature value changes better.\nUsing too many highly correlated features and their impact on calculating feature importance should have been investigated.\nTheir feature importance calculation seems to have data leakage issue.\nMean encoding idea is not clear. Also, it seems to be prone to data leakage.\nVoting classifier description doesn’t mention what the voter algorithms are.\nThey didn’t have an independent test set and their cross validation didn’t consider temporal aspect of the data (=more data leakage).\nTheir lack of discussion on business objectives becomes problematic again when they do model comparison with various metrics. We don’t know which metric makes most sense. They seemed to have used accuracy.\nAt this point, we agreed that this paper seemed like it’s written for the sake of showcasing ML (aka “we can build an ML for this”) not for any research and business goals.\nAgain, we think the model selection should’ve considered interpretability, which is related to the actual use case of the model, which the authors never discussed.\nThey also didn’t mention any baseline and so we don’t know the achieved score is enough or not.\nMassive overreach is shown in their conclusions.\nAs a future work, they suggested building better ML predictors. First, this paper is not about creating a novel algorithm, so they can’t make this claim and second, it’s often the case that authors simply suggest building a better model even though the problem is not the model but rather it’s them making so many basic mistakes such as data leakage, feature misrepresentation, and lack of use case."
  },
  {
    "objectID": "notes/2023-01-26.html",
    "href": "notes/2023-01-26.html",
    "title": "Austin ML Journal Club",
    "section": "",
    "text": "ML-based science: Is prediction important in science? This might depend on the domain (e.g., climate change research might focus on predicting the risk)\nModel info sheet: the intent is good but it seems unfeasible (too much effort and information for the author)\nDefinition of reproducibility: it makes sense to consider the correctness of analysis and scientific integrity as a part of reproducibility\nWhy focus on the civil war papers? Perhaps these are well-studied review papers w/ code and data available"
  },
  {
    "objectID": "notes/2023-01-26.html#why-make-distinction-between-science-and-engineering-applications",
    "href": "notes/2023-01-26.html#why-make-distinction-between-science-and-engineering-applications",
    "title": "Austin ML Journal Club",
    "section": "Why make distinction between science and engineering applications?",
    "text": "Why make distinction between science and engineering applications?\n\nIt’s not like that leakage is okay in production or difficult not to notice in engineering application. Data leakage is still an issue.\nPerhaps they meant that industry engineering applications get less constraints/scrutiny about the process. But practically speaking, engineering applications can pose higher risks than scientific research because they have more immediate real-world impacts\nPlus some papers they looked into sounded very close to industry such as software engineering, cybersecurity, etc."
  },
  {
    "objectID": "notes/2023-01-26.html#taxonomy",
    "href": "notes/2023-01-26.html#taxonomy",
    "title": "Austin ML Journal Club",
    "section": "Taxonomy",
    "text": "Taxonomy\n\nL1.1 (no test set): shocking!\nL1.2 (preprocessing): This is a loaded area because many different sub-domains exist such as imputation, oversampling, encoding, etc. Thus, it would’ve been nice to separate these areas further.\nL2 (illegitimate features):\n\nDefinition of proxy sounds a bit tricky.\n\nWe had a discussion about whether highly correlated features can be considered as data leakage. We agreed that all proxies are highly correlated features but not all highly correlated features are proxies.\nLike in hypertensive drug example, proxies are those that we should not be using (either because it’s cheating or not relevant or not available when making prediction) for obviously reasons. Thinking about the causal relationship also can help.\nAs the paper says, domain knowledge might be required to identify all proxies.\n\nThis L2 problem can get worse due to the high ML hype and ML code being extremely light (only a few lines of readily usable code available).\n\nL3.2 and L3.3: they are quite different from other data leakage problems.\n\nBoth are more challenging to identify.\nIf researchers can design experiments and collect data by themselves, they have some control over it, but for engineering applications/industry, often the data is given to us and we often don’t have many options to mitigate this problem.\nFor instance, it’s usually difficult to do split the data to achieve similar distributions between train and test (except class imbalance, temporal etc.)"
  },
  {
    "objectID": "notes/2023-01-26.html#model-info-sheets-and-our-theory-of-change",
    "href": "notes/2023-01-26.html#model-info-sheets-and-our-theory-of-change",
    "title": "Austin ML Journal Club",
    "section": "3.3: Model info sheets and our theory of change",
    "text": "3.3: Model info sheets and our theory of change\n\nHigh hopes!\nIt might not be useful to believe that the authors would follow this guideline. It will be a useful sanity checkpoint where the authors can think about the process (in this sense, this could be useful for industry people as well).\nBut of course, the authors can simply lie about it. But we get that this could be a middle-ground suggestion.\nAlso, in certain filed, people are very reluctant about sharing their data and code (such as oil and gas companies where well data is essentially their most valuable asset).\nIt might be more impactful if journals make this process as mandatory (but again, this will be a big ask)."
  },
  {
    "objectID": "notes/2023-01-26.html#limitations-of-model-info-sheets",
    "href": "notes/2023-01-26.html#limitations-of-model-info-sheets",
    "title": "Austin ML Journal Club",
    "section": "3.4: Limitations of model info sheets",
    "text": "3.4: Limitations of model info sheets\n\nAuthors not being ML experts and using ML in their research seems like another side-effect of ML hype."
  },
  {
    "objectID": "notes/2023-01-26.html#fig-1",
    "href": "notes/2023-01-26.html#fig-1",
    "title": "Austin ML Journal Club",
    "section": "Fig 1",
    "text": "Fig 1\n\nIt’s interesting that logistic regression isn’t affected much by the correction. It would have been nice if the authors explained more about the why (there’s some in appendix but it’s not super clear).\nAs the appendix explains, datasets from one paper were used by others even though they were based on imputation with high data leakage. Also some imputation processes in the papers the authors examined were done when the proportion of missing data is very high (like 90%)."
  },
  {
    "objectID": "notes/2023-01-26.html#problem-of-missing-significant-testing",
    "href": "notes/2023-01-26.html#problem-of-missing-significant-testing",
    "title": "Austin ML Journal Club",
    "section": "Problem of missing significant testing",
    "text": "Problem of missing significant testing\n\nWe discussed what exactly they were supposed to compare in terms of significant testing. Since they also mentioned confidence interval, we assumed it’s the results based on bootstrapping"
  },
  {
    "objectID": "notes/2023-01-26.html#enhancing-the-reproducibility",
    "href": "notes/2023-01-26.html#enhancing-the-reproducibility",
    "title": "Austin ML Journal Club",
    "section": "5: Enhancing the reproducibility",
    "text": "5: Enhancing the reproducibility\n\nFirst of all, this paper feels very redundant. This section also.\nAgain, these are good recommendations but maybe a big ask? (We were being realistic.)\nPrivacy concern is another thing to think about. If we violate privacy when we have to disclose so much about data and distribution to prove the absence of data leakage, how would we resolve this conflict?\nWe learned about the presence of CodeOcean (pretty cool effort)."
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Tips",
    "section": "",
    "text": "If you are interested in joining the journal club, please send an email to hongsup.shin@pm.me."
  },
  {
    "objectID": "readme.html#logistics",
    "href": "readme.html#logistics",
    "title": "Tips",
    "section": "Logistics",
    "text": "Logistics\n\nWe meet every last Thursday of the month between 6 pm and 8 pm. Meeting announcement is done through our Slack channel.\nWe are flexible about where we meet. Hongsup has been hosting the meetings so far and he can provide his place for future meetings as a backup."
  },
  {
    "objectID": "readme.html#presentation",
    "href": "readme.html#presentation",
    "title": "Tips",
    "section": "Presentation",
    "text": "Presentation\n\nWe take turns to present a paper.\nWhen announcing what to read, we encourage the presenter to write a few sentences about the reason for the pick. This will motivate readers.\nIt doesn’t have to be a paper. It can be a blog post, newspaper article, or even tool documentation. We can also use the journal club as a brainstorming session if the presenter wants to discuss a specific work problem they want to talk about.\nEvery presenter will be introduced in the About page with a short bio. Participants will be mentioned in the posts.\nPresenters don’t have to know everything in the paper but they should have enough understanding of the paper to lead the discussion.\n\n\nGuidelines for choosing papers\nReading papers takes a great deal of time. To respect everyone’s time, we encourage presenters to consider the following as acceptable material for the meetings.\n\nPapers that are published or peer-reviewed\nPapers about ideas that are discussed frequently in the media\nPapers with high citation counts\nPapers from renowned research groups\nPapers that are not too niche in terms of their domain\nPapers that are not too long\nPapers that have many interesting discussing points; this means you can choose a bad paper intentionally"
  },
  {
    "objectID": "readme.html#communication",
    "href": "readme.html#communication",
    "title": "Tips",
    "section": "Communication",
    "text": "Communication\n\nAll correspondence happens in our Slack channel.\nPresenters should give others at least 2 weeks to read the paper.\nPlease send a message on Slack about whether you can come next time at least a week before the meeting."
  },
  {
    "objectID": "readme.html#blogging",
    "href": "readme.html#blogging",
    "title": "Tips",
    "section": "Blogging",
    "text": "Blogging\n\nPresenters are welcome to write a blog post about the paper they presented.\nThe blog is hosted through GitHub pages from our repository and we use the Quarto platform.\nYou can write about anything in the post as long as it captures some of our discussion during the meeting.\nBlogging is optional. If the presenter is too busy to write the post, Hongsup will write a post instead, or he will suggest the presenter to write it together."
  }
]