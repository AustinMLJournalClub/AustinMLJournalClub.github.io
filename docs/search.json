[
  {
    "objectID": "reading_list.html",
    "href": "reading_list.html",
    "title": "Reading List",
    "section": "",
    "text": "This page collects papers, books, articles, videos, and other resources suggested by our community for potential future discussions. We welcome suggestions that align with our focus on impactful and seminal machine learning content."
  },
  {
    "objectID": "reading_list.html#current-suggestions",
    "href": "reading_list.html#current-suggestions",
    "title": "Reading List",
    "section": "Current Suggestions",
    "text": "Current Suggestions\n\nThe Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity (discussed November 2025)\nAI as Normal Technology (discussed October 2025)\nOn the Theoretical Limitations of Embedding-Based Retrieval (discussed September 2025)\nLanguage models cannot reliably distinguish belief from knowledge and fact\nEnhancing Retrieval-Augmented Generation: A Study of Best Practices\nRetrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely\nAgentic Retrieval-Augmented Generation: A Survey on Agentic RAG\nEnergy-Based Transformers are Scalable Learners and Thinkers\nAngles Don’t Lie: Unlocking Training-Efficient RL Through the Model’s Own Signals\nLarge Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation\nText-to-LoRA: Instant Transformer Adaption\nPoisoning Attacks on LLMs Require a Near-constant Number of Poison Sample\nPetri: An open-source auditing tool to accelerate AI safety research\nHolistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation\nAlignment Faking in Large Language Models\nMolmoAct: Action Reasoning Models that can Reason in Space"
  },
  {
    "objectID": "reading_list.html#how-to-suggest-papers",
    "href": "reading_list.html#how-to-suggest-papers",
    "title": "Reading List",
    "section": "How to Suggest Papers",
    "text": "How to Suggest Papers\nHave a paper, book chapter, news article, lecture, or other resource you think would spark great discussion? You can:\n\nContact the organizer directly\nSubmit a suggestion via GitHub issue\nBring it up during our monthly meetings\n\nWe look for content that is impactful, thought-provoking, or offers practical insights into how ML/AI works in practice, regardless of format or technical complexity."
  },
  {
    "objectID": "reading_list.html#previously-suggested-papers",
    "href": "reading_list.html#previously-suggested-papers",
    "title": "Reading List",
    "section": "Previously Suggested Papers",
    "text": "Previously Suggested Papers\nThe following were suggested by community members during our earlier meeting phases:\n\nSolving olympiad geometry without human demonstrations\nSleeper Agents: Training Deceptive LLMs that Persist Through Safety Training\nTOFU: A Task of Fictitious Unlearning for LLMs\nQuantifying the impact of uninformative features on the performance of supervised classification and dimensionality reduction algorithms\nFine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!\nA Mulching Proposal\nEvaluating and Mitigating Discrimination in Language Model Decisions\nDive into Deep Learning: Coding Session #4 Attention Mechanism I (MLT Artificial Intelligence)\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\nMiniLLM: Large Language Models on Consumer GPUs\nThe TinyLlama project\nOn the Opportunities and Risks of Foundation Models\nChallenges in Deploying Machine Learning: a Survey of Case Studies\nMachine Learning and the Future of Bayesian Computation"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The Austin ML Journal Club brings together ML/AI practitioners to explore cutting-edge research through focused discussion and collaborative learning.\nThough “Austin” remains in our name as a nod to our origins, we meet virtually via Zoom—welcoming participants regardless of location.\nWe meet monthly(ish) to dive deep into papers that are shaping our rapidly evolving field. We aim to nurture deep conversations and practitioner insights that reveal how the sausages get made."
  },
  {
    "objectID": "about.html#join-us",
    "href": "about.html#join-us",
    "title": "About",
    "section": "Join Us",
    "text": "Join Us\nInterested in joining our discussions? Follow our LinkedIn page for meeting announcements and registration details. Registration opens before each session and closes 48 hours prior to the meeting.\nWant to present a paper? Fill out our presenter volunteer form."
  },
  {
    "objectID": "about.html#how-we-work",
    "href": "about.html#how-we-work",
    "title": "About",
    "section": "How We Work",
    "text": "How We Work\n\nMeeting Format\nWe gather virtually via Zoom for 90-minute sessions designed to fit busy professional schedules. Originally meeting in person in Austin, we transitioned to virtual format to address logistical challenges and serve the growing community of interested participants beyond Austin. Each session centers on a single paper that participants read in advance, allowing for substantive technical discussion rather than surface-level summaries.\n\n\nPaper Selection\nWe choose papers that are impactful and seminal in ML/AI, ranging from highly technical research to influential pieces that shape our field’s thinking. Our selections prioritize work that offers both theoretical insights and practical relevance, sparking meaningful debate and learning regardless of technical complexity.\n\n\nPresentation Format\nSessions are led by volunteer presenters from our community, encouraging diverse perspectives and expertise sharing. The format is entirely up to the presenter - we’ve never required formal slidedecks or PowerPoints, as sharing the paper itself and guiding discussion is sufficient. This approach reduces preparation barriers and keeps our focus on substantive conversation rather than presentation polish. To ensure sustainability and consistency, the organizer serves as a backup presenter when needed, maintaining our regular meeting schedule while fostering community ownership of the learning process.\n\n\nDiscussion Philosophy\nWe operate under the Chatham House Rule - participants are free to use information shared during meetings, but cannot reveal the source or identity of speakers. This creates a safe space for open intellectual exchange where practitioners can explore ideas, ask questions, and engage in constructive criticism without professional concerns.\n\n\nKnowledge Sharing\nWe publish summaries of our discussions on this blog, capturing key insights and diverse perspectives that emerge from our conversations. These summaries serve both as records for participants and resources for the broader ML community. Our blog is built using Quarto and hosted on GitHub Pages, with pull requests automatically published via GitHub Actions, ensuring a streamlined process for sharing our collective insights. See our quarto guideline for blogging tips.\n\n\nCommunity Standards\nWe maintain a welcoming environment for practitioners at all experience levels - from industry engineers to academic researchers to passionate learners. Our community represents diverse industry domains including hardware, software, fintech, and healthcare, as well as academic backgrounds spanning neuroscience, statistics, electrical engineering, astrophysics, biology, and more. This diversity enriches our discussions by bringing varied perspectives to how machine learning intersects with different fields and problem domains."
  },
  {
    "objectID": "about.html#schedule-participation",
    "href": "about.html#schedule-participation",
    "title": "About",
    "section": "Schedule & Participation",
    "text": "Schedule & Participation\nWe typically meet late in the month on Thursday evenings at 5:00 PM CT. Registration opens before each session and closes 48 hours prior to maintain focused group dynamics. The club is free and open to anyone interested in advancing their understanding of ML/AI."
  },
  {
    "objectID": "about.html#our-story",
    "href": "about.html#our-story",
    "title": "About",
    "section": "Our Story",
    "text": "Our Story\nThe Austin ML Journal Club emerged from a simple recognition: the ML field moves so quickly that staying current with meaningful research is challenging for individual practitioners. While numerous newsletters and articles provide summaries, there’s real value in reading papers deeply, examining methodologies critically, and understanding what actually works in practice.\nA journal club creates the structure and accountability for this kind of sustained engagement while transforming what could be isolated study into collaborative learning. What began as a group of Austin-based ML practitioners gathering to critique papers and share workplace insights has evolved to serve a broader community of engineers and researchers who value rigorous discussion over surface-level summaries."
  },
  {
    "objectID": "about.html#team",
    "href": "about.html#team",
    "title": "About",
    "section": "Team",
    "text": "Team\n\nOrganizer\nHongsup Shin organizes the Austin ML Journal Club, managing meeting coordination, paper selection, and blog maintenance. He is a Principal AI Engineer at Arm with a background in computational neuroscience and behavioral ecology. His interests span AI engineering, MLOps, responsible AI, and AI ethics. Previously, he contributed as a volunteer data scientist at Texas Justice Initiative, a criminal justice non-profit in Austin. He current volunteers as a co-chair of the SciPy conference Proceedings Committee.\n\n\nCommunity Contributors\nThe Austin ML Journal Club has evolved over time with contributions from many dedicated participants. We’re grateful to Athula Pudhiyidath, Ivan Perez Avellaneda, Joel Afriyie, Brian King, Kshitij Aggarwal, Meghann Agarwal, Saina Lajevardi, Akshata Mohan, and other community members who helped establish the foundations of our collaborative learning environment during the club’s earlier phases."
  },
  {
    "objectID": "about.html#license",
    "href": "about.html#license",
    "title": "About",
    "section": "License",
    "text": "License\nOur blog posts are licensed under Creative Commons Attribution 4.0 International (CC BY 4.0). You’re free to share and adapt our content with proper attribution to Austin ML Journal Club."
  },
  {
    "objectID": "archives.html",
    "href": "archives.html",
    "title": "Archives",
    "section": "",
    "text": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\nAI as Normal Technology\nOn the Theoretical Limitations of Embedding-Based Retrieval"
  },
  {
    "objectID": "archives.html#section",
    "href": "archives.html#section",
    "title": "Archives",
    "section": "",
    "text": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\nAI as Normal Technology\nOn the Theoretical Limitations of Embedding-Based Retrieval"
  },
  {
    "objectID": "archives.html#section-1",
    "href": "archives.html#section-1",
    "title": "Archives",
    "section": "2024",
    "text": "2024\n\nModeling Tabular Data using Conditional GAN\nGorilla: Large Language Model Connected with Massive APIs\nDissecting Racial Bias in an Algorithm Used to Manage the Health of Populations\nAre Emergent Abilities of Large Language Models a Mirage?\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model"
  },
  {
    "objectID": "archives.html#section-2",
    "href": "archives.html#section-2",
    "title": "Archives",
    "section": "2023",
    "text": "2023\n\nWhy Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?\nNeural Machine Translation by Jointly Learning to Align and Translate\nConstitutional AI: Harmlessness from AI Feedback\nAutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types\nLet’s Verify Step by Step\nVisualization in Bayesian workflow\nReviewing a Case Study About Real Estate Market Prediction\nLeakage and the Reproducibility Crisis in ML-based Science"
  },
  {
    "objectID": "archives.html#section-3",
    "href": "archives.html#section-3",
    "title": "Archives",
    "section": "2022",
    "text": "2022\n\nZero-Shot Text-to-Image Generation\n“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI"
  },
  {
    "objectID": "posts/20240328/index.html",
    "href": "posts/20240328/index.html",
    "title": "Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations\nPresenter: Hongsup\nAttendees: Allie, Brian, Kate, Kshitij, Swati"
  },
  {
    "objectID": "posts/20240328/index.html#why-this-paper",
    "href": "posts/20240328/index.html#why-this-paper",
    "title": "Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "Why this paper?",
    "text": "Why this paper?\nFresh out of academia and at my first job, I remember being surprised by the great power I was able to wield as the main data scientist in the team. A few lines of my code could easily have cascading impact on business decisions. And (as is often the case) since management didn’t care much about technical details, this power gave me a sense of grave responsibility, which was honestly often terrifying. To this day, this sense is something I try to remind myself of, especially because ML systems are getting more complex and we still have very little accountability for ML. So in a way, every ML practitioner is the first line of defense. And this responsibility is more critical if one is working in an high-impact domain such as healthcare.\nThese days it feels like ML is all about LLMs and AI assistants. But algorithmic bias is still widespread, and unfortunately it’s even more overshadowed by this massive hype. This seminal paper from 2019 identified racial bias in a healthcare algorithm and discussed the problem of label choice bias. I find this paper still relevant because this bias can easily sneak when building datasets and algorithms. Since the paper is a few years old, it will be interesting to discuss what’s happened since then."
  },
  {
    "objectID": "posts/20240328/index.html#paper-summary",
    "href": "posts/20240328/index.html#paper-summary",
    "title": "Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "Paper summary",
    "text": "Paper summary\nThis is a famous paper in Fair ML field that’s cited frequently. One reason is that this is one of the first algorithm audit papers that used a creative approach to dissect an algorithm already used in public. The first author Ziad Obermeyer is an active figure in algorithmic bias specifically in healthcare, and the corresponding author Sendhil Mullainathan is a seminal figure in behavioral economics.\nThe main message of the paper is simple and straightforward. In a widely used healthcare algorithm, the authors found that given a score generated by a trained algorithm, which predicts how sick a patient is, Black patients were much sicker than the White. By using creative approaches, the authors found that this algorithmic behavior originated from label choice bias. The algorithm used healthcare cost as target, which the ML practitioners considered as a proxy for a patient’s sickness. The authors cautioned that seemingly effective proxies for ground truth can cause algorithmic bias.\n\nBackground\nThe authors discussed the difficulty of empirical investigation on commercially used algorithms because they are considered proprietary, and thus researchers have to work “from the outside”. They can come up with creative methods but these can be still limiting because without knowing how the model works, it is difficult to understand how and why. For instance, we need training data, objective function, prediction method, and so on, to have a deeper understanding of an algorithm.\n\nHigh-risk care management system\nThis study is about a high-risk care management system often used in hospitals to provide care for complex medical needs, which tend to be quite expensive. In essence, hospitals want to prioritize patients so that they can optimize which patients will benefit the most by optimizing resource allocation. For instance, early identification of a high risk patient can reduce expensive cost later, like an ER visit. The authors said healthy care systems rely on algorithms extensively for this type of care management. This type of algorithm actually has many analogies, and an algorithm like this is often called as a risk assessment tool, which is also used in finance, criminal justice, and so on.\n\n\n“Who will benefit the most?”\nThe authors emphasized that this is a difficult causal inference problem. We need to estimate individual treatment effects after the resources are allocated. The underlying assumption is “those with the greatest needs will benefit the most,” and thus we are setting future health care needs as main target.\n\n\n\nThe algorithm\nFor this study, the authors had a unique advantage to have access to training data, objective function, and prediction. This is mainly due to that the first author worked at the hospital which used the algorithm and he had access.\nThe data are from a large academic hospital, collected between 2013 and 2015. The authors focused on the white vs. black relationship to examine racial bias. There were about 6k black patients and 40k white patients. For the algorithm, total medical expenditure (\\(C\\)) is used as target variable. The input feature data are collected from insurance claim data, which includes demographics, insurance type, ICD-9 (international classification of diseases code), medications, medical encounters such as surgery or radiology, and so on. The authors shared the code and data here and we can take a look at the data ourselves.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%config InlineBackend.figure_format='retina'\nimport seaborn as sns\n\n\n\n\nCode\ndata = pd.read_csv(\"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv\")\ndata.head()\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\nNote that the risk score (\\(R\\), risk_score_t) and the predicted cost (\\(C\\), cost_t) are not the same. We do NOT know how the risk score is calculated based on the predicted score.\n\n\nCode\nplt.figure(figsize=(6, 6))\nplt.plot(data['cost_t'], data['risk_score_t'], '.', alpha=0.01)\nplt.loglog()\nplt.axis('equal')\nplt.xlabel('Predicted Cost')\nplt.ylabel('Risk Score')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe algorithm score was generated for each patient during the enrollment period. After ranking this prediction scores from a group, the top 3% (97th percentile) is automatically flagged as required to be enrolled although this does not guarantee enrollment. The top 45% (55th percentile) is referred to their PCP for further examination.\n\n\nThe algorithm audit\nAs a main method, the authors measured the algorithm calibration to assess fairness across different racial groups. This means that the authors focused on parity of how risk score is calibrated across race. Fair result would satisfy \\[E[Y|R, W] = E[Y|R, B]\\]\nwhere \\(Y\\) is our main interest, \\(W\\) is the White, \\(B\\) is the Black, and \\(R\\) is the risk score.\nTo dissect the algorithm, the authors compared the distribution of \\(R_{i, t}|H_{i, t}\\) and \\(R_{i, t}|C_{i, t}\\) between White and Black patients where - \\(R_{i, t}\\): risk score given patient \\(i\\) in year \\(t\\) given the input feature \\(X_{i, t-1}\\) from the previous year - \\(H_{i, t}\\): realized health - \\(C_{i, t}\\): actual cost\nTo measure \\(H\\), the authors used health record data, diagnoses, lab results, and vital sign data. As a main measure of realized health, the authors used comorbidity score, the total number of chronic illnesses over a year, which is a measure of medical complexity. The cost variable \\(C\\) includes insurance claims on utilization, outpatient/ER visit, hospitalization, and general health care costs.\n\n\nUsing a counterfactual scenario to describe racial bias (Fig. 1B)\nFig. 1A shows mean comorbidity vs. risk score. At a given risk score, Blacks have significantly more illness than Whites. For instance, at the 97th percentile, mean comorbidity score was 4.8 for Black and 3.8 for White. This means that sicker black patients and healthier white patients can have the same score, causing substantial disparities in program screening. The authors designed a counterfactual scenario with no racial bias, and described the severity of the disparity.\nWe found it difficult to understand how the authors measured the disparity between the counterfactual and actual. Fortunately, the code that the authors shared helped us. Here is how this assessment was done in the paper:\n\nAt a given percentile (e.g., 97th percentile) identify the following:\n\nGroup “White-above”: White patients whose risk score is above the threshold\nGroup “Black-above”: Black patients whose risk score is above the threshold\n\nIf the comorbidity score of the healthiest patient in the White-above group is lower than that of the sickest patient in the Black-lower group, remove the healthiest white patients from White-above, and move the sickest Black patient from the Black-lower above the threshold.\nRepeat this process until patients cannot be moved any more.\n\nThe second step can be considered as a White patient who shouldn’t be in the enrollment group (above the threshold) giving their spot to the sicker black patient, who was supposed to be in the enrollment group. The authors’ original code was written in R but I translated it into python to further investigate this simulation.\n\n\nCode\ndef setup(data, default_in_percentile=[95, 97]):\n    cohort = data[['race', 'risk_score_t', 'gagne_sum_t']]\n    dt = cohort.copy()\n    dt['risk_pctile'] = pd.cut(dt['risk_score_t'], bins=np.unique(np.percentile(dt['risk_score_t'], np.arange(0, 101))), include_lowest=True, labels=False)\n    \n    enroll_stats = np.zeros((len(default_in_percentile), 3))\n    enroll_stats = pd.DataFrame(enroll_stats, columns=['black_before', 'black_after', 'ratio'])\n    enroll_stats.index = default_in_percentile\n    \n    return {'dt': dt, 'enroll_stats': enroll_stats}\n\n\n\n\nCode\ndefault_in_percentile = [97]\nj = 0\n\ndt = setup(data, default_in_percentile)['dt']\nenroll_stats = setup(data, default_in_percentile)['enroll_stats']\n\nprior_enrolled = dt[dt['risk_pctile'] &gt;= default_in_percentile[j]]\nprior_w = prior_enrolled[prior_enrolled['race'] == 'white']\nprior_b = prior_enrolled[prior_enrolled['race'] == 'black']\n\nupperb = dt[(dt['risk_pctile'] &gt;= default_in_percentile[j]) & (dt['race'] == 'black')]\nupperw = dt[(dt['risk_pctile'] &gt;= default_in_percentile[j]) & (dt['race'] == 'white')]\nlowerb = dt[(dt['risk_pctile'] &lt; default_in_percentile[j]) & (dt['race'] == 'black')]\n\nupperw = upperw.sort_values('gagne_sum_t')\nlowerb = lowerb.sort_values(['risk_score_t', 'gagne_sum_t'], ascending=[False, False])\n\nupperb_actual = upperb.copy()\nupperw_actual = upperw.copy()\nlowerb_actual = lowerb.copy()\n\n\n\n\nCode\nsw = 0\nsb = 0\nswitched_count = 0\nswitched_w = []\nswitched_b = []\n\nwhile sb &lt; lowerb.shape[0]:\n    if upperw.iloc[sw]['gagne_sum_t'] &lt; lowerb.iloc[sb]['gagne_sum_t']:\n        switched_w.append(upperw.iloc[sw])\n        switched_b.append(lowerb.iloc[sb])\n\n        # healthiest white patient is dropped and sickest black patient is added\n        upperb = pd.concat([upperb, pd.DataFrame(lowerb.iloc[sb]).T], axis=0)\n        upperw = upperw.drop(upperw.index[sw])\n        upperw = upperw.sort_values('gagne_sum_t')\n        \n        sb += 1\n        switched_count += 1\n    else:\n        sb += 1\n        switched_count = switched_count\n\n\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\nsns.kdeplot(upperw_actual['gagne_sum_t'], label='White-upper', ax=axes[0])\nsns.kdeplot(upperb_actual['gagne_sum_t'], label='Black-upper', ax=axes[0])\naxes[0].set(xlabel='Comorbidity score', title='Actual')\naxes[0].legend()\n\nsns.kdeplot(upperw['gagne_sum_t'], label='White-upper', ax=axes[1])\nsns.kdeplot(upperb['gagne_sum_t'], label='Black-upper', ax=axes[1])\naxes[1].set(xlabel='Comorbidity score', title='Counterfactual')\naxes[1].legend()\n\nfig.suptitle('97th percentile', x=0.5, y=0.95, fontsize=15)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nThe left figure shows the distribution of comorbidity score of white and black patients who are above the 97th percentile threshold, identified by the algorithm as those who need the high-risk care. The white patient distribution is tilted to the left and the black patient’s has more density towards right, indicating that white patients are generally healthier thant the black patients in the above-threshold group. The right figure is after we simulate the counterfactual scenario by swapping the healthiest white patient’s slot above the threshold with the sickest black patient below the threshold until the swapping cannot be done any more. The distribution now shows that the two distributions are similar.\n\n\nMechanisms of bias\nBased on Fig. 3A, the algorithm calibrates cost (target variable) quite well, meaning this is not a problem of prediction quality. Fig. 3B shows that at a given level of health score, black patients generate lower costs than the White, making the algorithm prioritizing the White patients. Tab. S2 shows that how healthcare cost is spent between the black and white patients vary by categories. For instance, the black patients have higher costs related to ER and dialysis and fewer inpatient surgical and outpatient specialist costs, which also suggest that their healthcare cost occurs when the worst scenario happens (ER visit). The negative values mean black patients spend less money in that category.\n\nThere are several socio-economic reasons. First, poor patients face substantial barriers to accessing health care, even the insured ones such as geography or transportation, demands from job and child care, and knowledge of reasons to seek care. There is also cost that is directly affected by race. Taste-based discrimination, employers’ prejudice or dislikes in an organizational culture rooted in prohibited grounds can have negative results in hiring minority workers, is one, and the doctor-patient relationship is another. It is known that black patients would take preventive care suggested by black provider more and they also have lower trust in health care system in general due to an incident like Tuskegee study. Doctors also may have different perceptions of black patients regarding such as intelligence, affiliation, or pain tolerance.\n\n\nExperiments on label choice and human-AI interaction\nThe authors then conducted experiments to see alternative label choices other than the total health care cost can result in fairer results. They compared the original model (total cost) against the two alternatives: avoidable cost such as ER visit, and health (comorbidity score). Even though it was not perfect, they found that the algorithms trained on alternative labels produced fairer results.\nFinally, the authors examined the interaction between human doctors and the algorithm. The algorithm’s risk score suggests a pool of patients candidates to doctors and they make the ultimate decision on the enrollment. The experiment showed that the doctors can “redress” (=correct) algorithm’s bias but not by much. This indicates that the racial bias produced by the algorithm can have drastic disparaging effect on healthcare management across different race groups that humans cannot correct completely.\n\n\nWorking on solutions together\nInterestingly, the authors mentioned that they reached out to the manufacturer of the algorithm with their results. Fortunately, they were able to replicate the authors’ results on their own and by collaborating with the authors, they were able to reduce the bias by 84%. With this, the authors ended the paper on a hopeful note. They emphasized that algorithmic bias is fixable and this does not necessarily require changes in the algorithm itself.\nThe paper triggered multiple investigations and stimulated algorithm audit research in the following years. In the same year in 2019, New York regulators started investing UnitedHealth algorithm for racial bias and similarly in California in 2022. In 2021, FTC published a guidance document on truth, fairness, and equity in the use of AI in industry. In 2024, there has been a Senate Finance Committee Hearing on AI in healthcare as well."
  },
  {
    "objectID": "posts/20240328/index.html#discussion",
    "href": "posts/20240328/index.html#discussion",
    "title": "Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "Discussion",
    "text": "Discussion\nWe first had several questions about the care management system described in the paper. The cost categories listed in the paper seemed to include cost for both patients and hospitals and we were not sure whether the cost allocation mattered at all. Some of us were also curious about the actual outcome of the care management system especially for the patients who were enrolled (and potential racial bias of the realized outcome). We also talked about other demographic features that can cause medical bias such as gender and age, and especially their intersectionality.\nWe agreed that the technical description of the counterfactual scenario was not clear in the paper, and we spent some time understanding how it worked. Similarly, Tab. 2, where the authors created different models with alternative labels and measured the proportion of the high-risk patients identify each model, was a bit confusing to understand, but fortunately, because the authors shared their code for all analyses, we were able to follow eventually.\nWe then talked about how to satisfy certain criteria during model training. We questioned the feasibility of satisfying multiple criteria (e.g., fairness metric and accuracy) in healthcare where we often have to work with very small data. This problem will get worsened when considering intersectionality, for instance. And the small data problem naturally leads to safety and privacy issues, especially when data contain sensitive information like in healthcare.\nWe also discussed practical aspects such as how to roll out an algorithm carefully especially when it touches millions of people. Some companies release their models gradually and share training data publicly, but we were wondering whether there were more specific tips on how ML deployment can evolve while having human in the loop, especially those who are impacted by the algorithm. And finally, we agreed that even though it’s important to understand how algorithmic bias damages our society and people, we were hopeful that we would like to learn about positive and beneficial use case of ML.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2024-racial-bias-healthcare,\n    author = {Hongsup Shin},\n    title = {Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations},\n    year = {2024},\n    month = {03},\n    day = {28},\n    howpublished = {\\url{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20240328/},\n}"
  },
  {
    "objectID": "posts/20230329/index.html",
    "href": "posts/20230329/index.html",
    "title": "Visualization in Bayesian workflow",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Visualization in Bayesian workflow\nPresenter: Hongsup\nAttendees: Kate, Saina"
  },
  {
    "objectID": "posts/20230329/index.html#why-this-paper",
    "href": "posts/20230329/index.html#why-this-paper",
    "title": "Visualization in Bayesian workflow",
    "section": "Why this paper?",
    "text": "Why this paper?\nThis paper summarizes types of data visualization that we can use in Bayesian modeling and inference. This is also a good overview of how to do Bayesian modeling properly, including validating results. The fact that the main author is one of the maintainers of the stan package, is another motivating factor."
  },
  {
    "objectID": "posts/20230329/index.html#paper-summary",
    "href": "posts/20230329/index.html#paper-summary",
    "title": "Visualization in Bayesian workflow",
    "section": "Paper summary",
    "text": "Paper summary\nGiven a problem, we incorporate our scientific knowledge into a causal (generative) model to simulate how the relevant variables are produced (input and output). Researchers need more than null hypothesis because it doesn’t talk about how your observation is generated. We can use a DAG as a scientific causal model and data generation process can be expressed in a generative model, which is often accompanied with Bayesian data analysis (BDA). BDA is particularly useful because we can simulate data from the model directly to design and debug during inference. To effectively estimate a posterior distribution, we need computational methods such as MCMC and others. One may say Bayesian might be an overkill but it’s extremeley useful for typical modeling problems such as measurement error, missing data, latent variables, and regularization. Again, it’s also generative!\nThe paper uses data visualization to express the followings: - Exploratory data analysis to come up with a proper model - Prior predictive distribution check to check model’s assumption - MCMC computational check to evaluate the sampling process - Posterior predictive check to validate inference process\nThis paper is based on R’s bayesplot but there are several python equivalents to this such as pymc, arviz, and numpyro. It uses a global air polllution dataset (pm2.5 particles) measured from satellite images. The goal of modeling is to predict the level of pm2.5 from the images. Hence, this is a regression problem. Fig. 1 shows the linear trend between the two variables of interest but also shows how sparse the data is depending on groups.\n\nExploratory data analysis (EDA)\nEDA is essential to understand and capture features and heterogeneity of data. The data pattern helps building a group-up modeling strategy to address the imbalance and sparsity of data. The authors emphasize that the top-down approach in typical ML communities these days is to throw everything into a non-parametric procedure, which can severely overfit. Fig. 2 shows that simple regression works pretty well, especially when the group identity is taken into account, which means we need a hierarchical approach.\n\n\n\nFig. 4: Visualizing the prior predictive distribution\n\n\n\n\nPrior predictive check\nInstead of using a non-informative or uniform prior, weakly informative prior is always recommended, which takes into account modeler’s perspective. In the paper, we assume that the target varialbe follows a normal distribution defined by a mean and a \\(\\sigma\\) where the mean is a linear function of input variable (satellite data) and linear coefficients, which also have priors (0 mean and std (\\(\\tau\\))).\nPrior predictive checks are useful to visualize the impact of our assumption for prior definition. If we use a vague prior (very wide range, Fig. 4a), ranges from the sample don’t match the observation. Fig. 4b shows a much tighter prior where the simulated data points still overestimate but are in a much reasonable range. Obviously, tighter and sensible priors are better.\n\n\n\nFig. 5: Diagnostic plots for Hamiltonian Monte Carlo\n\n\n\n\nMCMC diagnostics\nSuccess of Hamiltonion Monte Carlo (HMC) depends on how smooth the posterior distribution is; if not smooth, HMC proposal diverges from the true trajectory, which may signal that the trajectories are stuck. Healthy MCMC samples, shown as a bivariate plot in Fig. 5a, shouldn’t have obvious patterns. The funnel shape there is due to \\(\\beta_{11} \\sim N(0, \\, \\tau_{1}^{2})\\) where small \\(\\tau_{1}\\) means \\(\\beta_{11}\\) distribution is narrow. The parallel co-ordinate plot (Fig. 5b) also shouldn’t have any particular structure.\n\n\n\nFig. 9: Graphical check of leave-one-out cross-validated probability integral transform (LOO-PIT)\n\n\n\n\nPosterior predictive check\nIf a trained model has a good fit, generated data from the model should follow observations. Posterior predictive checking is mostly qualitative but it’s effective to compare empirical and simulated values (Fig. 6). Fig. 7 shows checking whether samples from models captures other statistics such as skewness (kurtosis) and Fig. 8 shows how we can evaluate whether samples from models capture summary statistics such as median Fig. 9 shows using visualization that checks whether leave-one-out cross-validation (LOO-CV) predictive cumulative density function is uniform or not, similar to the idea of a K-S test.\n\n\n\nFig. 10a: Model comparisons using leave-one-out (LOO) cross-validation. The difference in pointwise ELPD values obtained from PSIS-LOO\n\n\n\n\nModel comparison\nWhen comparing models, Bayesian data analysis allows detailed examination of individual data points on a given model. We can use cross-validated LOO predictive distribution to do so; it shows the distribution of a data point from a model that’s built without that data point (i.e., LOO). We can use expected log-predictive densities (ELPD), which is essentially the mean of the log probability of each data point i, computed with posterior that omits the point i (the bigger the better). We use Pareto-smoothed importance sampling (PSIS) to compute this metric (we don’t have to fit the models N times). Once we have ELPD value for every data point of a model, we can repeat this for all the models we have and make comparison (Fig. 10a).\n\n\n\nFig. 10b: Model comparisons using leave-one-out (LOO) cross-validation. The \\(\\hat{k}\\) diagnostics from PSIS-LOO for Model 2\n\n\nSimilarly, we can compute \\(\\hat{k}\\) as well which represents degree of influence of a specific observation. High value means this data point is “unexpected”, meaning that it is likely to be an outlier or the model struggles to make valid prediction for this data point."
  },
  {
    "objectID": "posts/20230329/index.html#discussion",
    "href": "posts/20230329/index.html#discussion",
    "title": "Visualization in Bayesian workflow",
    "section": "Discussion",
    "text": "Discussion\nWe had a lengthy discussion about the choice of prior and how tricky it can be. As the authors mentioned in conclusion, we were also slightly worried about double-dipping the data when running prior predictive checks and potential data leakage. It was also interesting to share our own experience on Bayesian inference ranging from dealing with prior assumptions, model comparison, to decision-making with uncertainty. But we all agreed that Bayesian data analysis is more empowering for us modellers compared to the typical top-down approach in ML where we often don’t have any generative models about data. We also agreed that Bayesian data anlsysis is absolutely more powerful when we suffer from a small data problem.\nBut we also some downsides of Bayesian data analysis too. It’s difficult to scalable and someone we ML practitioners are not the domain experts and without the domain expertise, it’s difficult to come up with a good DAG. Due to the nature of Bayesian analysis where we don’t often make a point-estimate summary, we appreciated that the paper spent a good amount of time discussing how to summarize a posterior distribution. We also discussed the importance of loss function when decision making with uncertainty.\nIn general, we liked the paper but we thought it fell slightly short because it wasn’t focusing on understanding scientific mechanism but rather on predictive modeling nature of Bayesian analysis. When it comes to model comparison particularly, we thought it’s important to evaluate the structure of the model too in addition to evaluating the goodness of fit. For instance, if the model performance varies across the regions, the way we compare the models would like to change as well, and potentially the DAGs too.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2023-bayesian-visualization,\n    author = {Hongsup Shin},\n    title = {Visualization in Bayesian workflow},\n    year = {2023},\n    month = {03},\n    day = {29},\n    howpublished = {url{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20230329/},\n}"
  },
  {
    "objectID": "posts/20230720/index.html",
    "href": "posts/20230720/index.html",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types\nPresenter: Saina\nAttendees: Brian, Hari, Hongsup, Meghann"
  },
  {
    "objectID": "posts/20230720/index.html#why-this-paper",
    "href": "posts/20230720/index.html#why-this-paper",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "Why this paper?",
    "text": "Why this paper?\nProduct discovery plays a pivotal role in the success of an online store, as it not only fuels revenue generation but also exerts a profound influence on the overall customer experience. In the vast landscape of e-commerce, where countless products are bought and sold through platforms like Amazon, the task of efficiently arranging and presenting product information for indexing and seamless search functionality stands as a continuous and formidable challenge.\nAnother challenge arises due to the lack of a unified language in product descriptions. Given that retailers curate the information, the distinct characteristics and attributes of products are predominantly delineated within titles or descriptions, rather than being uniformly collected. Consequently, the extraction of this information necessitates delving into unstructured textual data.\nResearchers at Amazon address this issue by introducing a pipeline named AUTOKNOW. This pipeline comprises a set of machine learning algorithms and techniques that create taxonomy, recognize products, and ultimately construct a product knowledge graph. This graph serves the purpose of organizing catalog information and enhancing product discovery through indexing and downstream functionalities of search and product recommendation."
  },
  {
    "objectID": "posts/20230720/index.html#summary-of-techniques",
    "href": "posts/20230720/index.html#summary-of-techniques",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "Summary of techniques",
    "text": "Summary of techniques\n\n\n\nFigure 2: AutoKnow architecture, containing ontology suite to enrich product ontology and data suite to enrich product structured data.\n\n\nThis paper delves into the intricacies of the Retail Product Domain, where challenges compound due to the rapid influx of new products and the extensive range of associated attributes, such as color that can have many values. This domain presents a diverse spectrum of attributes spanning various product types, even as some attributes are shared among products. To address the inherent complexity, the study narrows its focus to four domains: grocery, health, beauty, and baby. Eligible products for the study must have at least one monthly page view. It is observed that products are characterized by dozens to hundreds of attributes per product, with the attribute count varying from 100 to 250 across different product types.\nThey start organizing the product taxonomy by focusing on types and hypernyms relationships, and curating attributes and synonyms for each type. They also focus on a knowledge graph (KG) that consists of triples in the form of (subject, predicate, object), where subjects are entities with IDs belonging to various types, objects can be entities or atomic values, and predicates signify relationships. This study concentrates on broad graphs where the topic type is products. Product types are organized hierarchically, forming a taxonomy. The paper assumes input sources, including a product catalog with attributes and customer behavior logs, query/purchase logs, customer reviews and question/answer. Intuitively, most product types appear in title, and search queries.\nThe taxonomy enrichment process consists of two steps. Firstly, a type extractor is trained to identify new product types from titles and queries, utilizing an open-world tagging model. Secondly, type attachment is achieved through a binary classification task, employing a graph neural network (GNN) module that captures signals from customer behaviors. This GNN-based approach refines type representations and combines them with semantic features for classification. Distant supervision is applied for model training, utilizing existing taxonomy and customer behavior data for positive and negative labels.\nThe Data Imputation component addresses the Structure-sparsity challenge by enhancing coverage through structured value extraction from product profiles. This process involves extracting new (attribute, value) pairs for each product from its profiles. State-of-the-art techniques using BIOE sequential labeling and active learning have been effective for solving this at the type-attribute level, as demonstrated in Equation (1) with BiLSTM and CRF.\n\\[(y_{1},y_{2},...y_{L}) = \\text{CRF}(\\text{BiLSTM}(e_{x_{1}},e_{x_{2}},...,e_{x_{L}}))\\]\nHowever, scalability to handle large numbers of product types and attributes is an issue. To overcome this, the paper introduces a novel taxonomy-aware sequence tagging approach conditioned on product type. The model’s predictions are influenced by the product type’s pre-trained hyperbolic-space embedding, preserving hierarchical relationships within the taxonomy. The approach also employs multi-task learning to enhance the identification of tokens indicating product type and tackle catalog misclassification or missing type information through shared BiLSTM training for sequence tagging and product categorization.\nNext step is the relation discovery which is based on product attributes where various sets of attributes apply to different product types, and only a fraction significantly affects shopping decisions. Identifying applicable and important attributes for thousands of types is essential. Formally, the task involves determining if an attribute applies to a product type and its importance for purchasing decisions. This paper employs classification and regression models—specifically, Random Forest—to assess attribute applicability and importance. The models utilize seller behavior (attribute value coverage and frequency in profiles) and buyer behavior (frequency in search queries, reviews, Q&A). Annotations from in-house and MTurk sources are used for model training. The approach distinguishes between subtle differences, such as attributes required for coverage but not always pivotal for decisions. The trained models are applied to all attribute-type pairs for decision-making.\nThis model is designed to figure out if a suggested attribute value matches the context provided by product descriptions and the product categories they belong to. The model takes raw input, which is a combination of words from product descriptions, product categories, and the suggested attribute value. These words are turned into embedding vectors that capture their meaning, source, and position in the text. The model then processes these embeddings using a multi-layer transformer to create a representation that summarizes all this information. This summarized representation is used to calculate a score, which tells us how well the suggested attribute value fits in with the context. To train the model, they use examples from the product catalog. They generate both correct and incorrect examples, and the model learns to distinguish between them. During testing, the model is applied to different attribute-value pairs, and those with low scores are considered as incorrect matches."
  },
  {
    "objectID": "posts/20230720/index.html#final-points",
    "href": "posts/20230720/index.html#final-points",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "Final Points",
    "text": "Final Points\nAt the time of this paper presentation, Autoknow is deployed in production in collecting over 1B product knowledge facts for over 11K distinct product types. The approach is validated by demonstrating the expansion of existing Amazon ontology by 2.9 times where precision is improved by 7.6% and recall by 16.4%. The collection suite of technologies are Apache spark distributed system, tensorflow for deep learning, amazon deep graph library for graph neural network for taxonomy. The relationships are derived using Spark ML, and an AWS SageMaker instance is used for training the imputation component.\nFinally, they mention an important lesson, which is that sometimes they need to move away from the product tree and consider multiple parents. In other words, a single sub-type cannot be clearly defined in many instances."
  },
  {
    "objectID": "posts/20230720/index.html#discussion",
    "href": "posts/20230720/index.html#discussion",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "Discussion",
    "text": "Discussion\nFirst, Saina gave us a great introduction about the problem domain, knowledge discovery. She said extracting and understanding attributes is essential in knowledge discovery in retail, which has been relatively easy before the era online shopping because merchants usually provided well-organized product catalogs. However, widely variable free text formats started emerging with the rise of e-commerce. Search engines have been trying to automatically extract attributes from the text input and to build a good taxonomy to create hierarchical relationship between products. Since products come and go constantly, this process requires interactive process of attribute extraction and taxonomy building. As the authors mentioned in the paper, some of the main challenges are to clean, curate, and understand the data, and to correct errors and prevent abuse of attributes.\nWe spent a lot of time trying to understand the sequential labeling problem the paper used extensively. The authors frequently cited their OpenTag paper, which they used to automatically generate training labels. These tags are used in many stages of their model pipeline including taxonomy enrichment and type attachment. The algorithm takes a sequence of text tokens and return a sequence of BIOE (“beginning”, “inside”, “outside”, “end”) labels. Then the authors used the returned tags as true labels for subsequent ML algorithms (distant supervision and regular supervision). We initially thought this BIOE labeling system came from the OpenTag paper but we later learned that it was based on a 2015 paper about bidirectional LSTM-CRT (conditional random field) for sequential tagging, which the authors used in this paper as well. In short, the final CRF layer in this deep learning model returns a probability for every plausible label sequences (sequence likelihood). We thought it would have been nice for the authors to give a quick overview of the algorithm and address potential risk of ML-generated labels as true labels in downstream ML modeling.\n\n\n\nA BI-LSTM-CRF model (Huang et al. 2015)\n\n\nWe also discussed other algorithms used in the paper to improve our understanding. First, we suspected that the regression problem they attempted to solve was based on MTurk workers’ subjective level of attribute importance (not a feature importance of an ML model) even though we doubted the choice of using only 6 MTurk workers. We also discussed the difference between semi-supervised and weak-supervised learning models: the former is about “based on what is already labeled, label some more” and the latter is about “based on your knowledge, label some more”. Finally, we talked about hyperbolic space embedding, which is excellent at preserving graph distances and complex relationships in very few dimensions Finally, as the authors mentioned at the end, we thought it would make sense to have a single model that takes care of data imputation and cleaning because in this paper, the cleaning is more of correction in imputed values.\nWe generally enjoyed reading this paper because it uses a plethora of various ML algorithms extensively, which gave us a good opportunity to get to know them. However, some of us questioned how this model pipeline can be maintained especially with the arrival of new information. We thought some stages of the pipeline might be more resilient to changes (such as the attribute importance estimation) but taxonomy changes may not. As we’ve seen in deep learning papers many times, we also questioned the validation aspect because the number of samples used in validation (a few hundreds triples) was much smaller than the actual dataset (one billion triples).\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2023-autoknow,\n    author = {Saina Lajevardi and Hongsup Shin},\n    title = {AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types},\n    year = {2023},\n    month = {07},\n    day = {20},\n    howpublished = {rl{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20230720/},\n}"
  },
  {
    "objectID": "posts/20221027/index.html",
    "href": "posts/20221027/index.html",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI\nPresenter: Hongsup\nAttendees: Athula, Hongsup, Kate, and Saina"
  },
  {
    "objectID": "posts/20221027/index.html#why-this-paper",
    "href": "posts/20221027/index.html#why-this-paper",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Why this paper",
    "text": "Why this paper\nMany ML articles exist but few talk about how the sausage gets made. They are often based on toy or clean benchmark datasets, which are quite different from what we get in real world. That’s why I enjoy reading survey papers. They interview people in the field like us and try to address common pain points to find a broader picture.\nFor the past several years, I have been noticing a trend in ML community. Many practitioners tend to ignore data quality but instead put all their efforts into models and algorithms. I find it very troubling because many problems in real-world ML are caused by data-related issues. “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI talks about this pattern based on the interviews from dozens of ML practitioners all over the globe."
  },
  {
    "objectID": "posts/20221027/index.html#data-cascades",
    "href": "posts/20221027/index.html#data-cascades",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Data cascades",
    "text": "Data cascades\nThe paper focuses on “data cascades,” a series of compounding negative events due to data-related issues. The authors say the problems are wildly prevalent (92% reported experience at least one type of data cascades). During the discussion, we thought about sampling bias because those who are ignorant of these problems may not be able to recognize them. Indeed, they are often opaque because there are no clear indicators and often discovered later. Data cascades often lead to technical debt and harm to beneficiary communities. They can sour relationships between stakeholders and in extremely cases, force ML practitioners discard entire datasets. Figure 1 shows the schematic of the data cascades. We thought the figure wasn’t particularly informative because too many arrows exist between the points. We hoped that the authors explained why there aren’t arrows between certain points.\n\n\n\nFigure 1. Data cascades in high-stakes AI"
  },
  {
    "objectID": "posts/20221027/index.html#high-stake-domains",
    "href": "posts/20221027/index.html#high-stake-domains",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "High-stake domains",
    "text": "High-stake domains\nData cascades are more critical in high stake domains such as landslide detection, suicide prevention, and cancer detection. There are several reasons:\n\nMore ML applications are deployed in these domains where more direct humanitarian impact exists.\nThis impact can be disproportionate towards vulnerable communities.\nIt is often very challenging to acquire high quality data in these domains.\nThe problems frequently require more multidisciplinary approach.\n\n\n\n\nTable 1. Summary of participant demographics; Domain\n\n\nThe authors find that the problems are due to human factors. Unfortunately solutions have been focusing on other issues such as database, legal, or license. To gather firsthand experience in the field, the authors interviewed 50+ ML practitioners all over the world, ranging from the US to India and African countries, and from founders to developers. Table 1 summarizes various high-stake domains. It was fascinating for us to learn that ML is used in areas as landslide detection, poaching prevention, or regenerative farming. It would have been more interesting to see how data cascades create specific negative consequences in some domains."
  },
  {
    "objectID": "posts/20221027/index.html#data-cascade-triggers",
    "href": "posts/20221027/index.html#data-cascade-triggers",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Data cascade triggers",
    "text": "Data cascade triggers\nThe authors introduce three triggers that cause data cascades.\n\nPhysical world brittleness\nPhysical world changes over time and thus often ML systems can’t produce robust results. Data drifts due to hardware (measurements) and environmental changes are commonly mentioned in ML Ops literature. In high-stake domains, they become more pronounced because training data are very limited and policy or regulation changes can impact the ML systems in various ways.\n\n\nInadequate application-domain expertise\nMost ML practitioners are not equipped with domain knowledge. All of us admitted that our academic background does not match to the domain that we work in. Even though close collaboration between domain experts and ML practitioners is always emphasized, in practice the authors find that domain experts are often detached from the larger impact of the applications. The authors explain two specific types of problems:\nSubjectivity in ground truth: Areas such as insurance claim approval or medical imaging for cancer detection involve highly specialized and often subjective decision-making. To build a reliable and robust ML system, it is necessary to standardize the decision-making criteria and find consensus. However, ML practitioners are asked to rush through the development process, and thus do not have time to address it.\nPoor domain expertise in finding representative data: ML practitioners often start building ML applications without involving domain experts much because the practitioners simply believe that data are reliable. However, because they lack domain knowledge, practitioners make incomplete assumptions, which results in disparity between data collection and deployment. This often leads to poor and unreliable model performance.\n\n\nConflicting reward system\nData collection and any data-related work are often considered non-technical and undervalued. The situation gets worse for frontline workers because they are asked to collect and curate field data on top of their existing responsibilities but they are not well compensated.\n\n\nPoor cross-organizational documentation\nMetadata about data collection, quality, and curation are also often missing. Some good practices the authors suggest involve keeping good documentation on reproducible assets such as data collection plan, data strategy handbooks, design documents, file conventions, field notes, and so on."
  },
  {
    "objectID": "posts/20221027/index.html#broader-context",
    "href": "posts/20221027/index.html#broader-context",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Broader context",
    "text": "Broader context\nData cascades discussion can be extended to bigger problems in ML community.\n\nIncentives and currency in AI\nBecause of the low incentives, data-related work are not rewarded or even tracked. This makes us difficult to get buy-in from stakeholders. The situation is similar in academia as well. Most practitioners and researchers focus on developing algorithms but they rarely mention or work on data. The title “Everyone wants to do the model work, not the data work” was a verbatim from an interviewee. Unfortunately, we were all able to relate to this quote.\n\n\nData education\nMost ML or data science curricula lack any mention of data quality or ethics. They use toy datasets or very clean benchmark datasets. As experience ML practitioners, we wholeheartedly agreed with this finding. We lamented that these courses do not prepare students with practical knowledge because one never works with clean datasets in real world. Some of us have experienced this pattern firsthand because we have been interviewing candidates for an ML practitioner position and found that the candidates who only worked with clean datasets (or primarily worked on algorithms) often lack basic practical ML knowledge.\n\n\nData bootstrapping\nData bootstrapping describes ML practitioners’ use of other data sources such as established data services or existing datasets to create their own dataset. For most of us, it was surprising to learn that many ML practitioners in high-stake domains in countries from Global South had to collect data from scratch. We agreed that challenges in data collection and lack of access to quality data would create inequality between countries."
  },
  {
    "objectID": "posts/20221027/index.html#how-to-address-data-cascades",
    "href": "posts/20221027/index.html#how-to-address-data-cascades",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "How to address data cascades",
    "text": "How to address data cascades\nThe authors introduce several ways of addressing the problem of data cascades. They introduce the concept of “data excellence”, an effort to “focus on the practices, politics, and values of humans of the data pipeline to improve the quality and sanctity of data, through the use of processes, standards, infrastructure and incentives”.\n\nFrom goodness-of-fit to goodness-of-data\nThe first is to use the right metric to evaluate data quality. Many ML practitioners use model performance metrics such as accuracy and RMSE to evaluate data quality. Some of us had a similar experience. We had to argue that model metrics shouldn’t be used to make a decision on data-pipeline and data-quality features. We hope that the authors can introduce specific examples of goodness-of-data metrics in the future.\n\n\nIncentives for data excellence\nThere are several ways to address the low incentives of data work. First, journals and conferences should require dataset documentation, provenance, and ethics as mandatory disclosure. Second, organizations should reward data-related work similar to how good software engineering is rewarded. Finally, partnership between stakeholders can nurture data excellence by sharing the reward on data-related work such as data collection, anomaly identification, and model verification.\n\n\nEducation and visibility\nThe authors argue for real-world data literacy in AI education, which includes training on data collection, infrastructure building, data documentation, data sense-making, and data ethics and responsible AI in general. Increasing the data visibility in ML lifecycle is important as well; implementing good monitoring system is one of the most important ML Ops practices anyway."
  },
  {
    "objectID": "posts/20221027/index.html#final-thoughts",
    "href": "posts/20221027/index.html#final-thoughts",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Final thoughts",
    "text": "Final thoughts\nSome of us found this paper vindicating because we have been advocating for data quality at work and had to fight for the attention it deserves. The paper helped us share our own practices at work that address data-related issues especially data collection, curation, and post-deployment data problems. Even though we generally agreed with authors’ suggestions, some of us wanted something more specific, like a case study. Overall, we found the paper interesting and insightful. We thought it would be beneficial to read the paper with our colleagues at work to start a discussion for data excellence.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2022-data-cascades,\n    author = {Hongsup Shin},\n    title = {Everyone wants to do the model work, not the data work: Data Cascades in High-Stakes AI},\n    year = {2022},\n    month = {10},\n    day = {27},\n    howpublished = {rl{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20221027/},\n}"
  },
  {
    "objectID": "posts/20240418/index.html",
    "href": "posts/20240418/index.html",
    "title": "Gorilla: Large Language Model Connected with Massive APIs",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Gorilla: Large Language Model Connected with Massive APIs\nPresenter: Joel\nAttendees: Athula, Brian, Hongsup"
  },
  {
    "objectID": "posts/20240418/index.html#why-this-paper",
    "href": "posts/20240418/index.html#why-this-paper",
    "title": "Gorilla: Large Language Model Connected with Massive APIs",
    "section": "Why this paper?",
    "text": "Why this paper?\nThe power of LLMs, in a commercial setting, comes from its ability to use other tools and integrate business domain knowledge too. Unfotunately, it is still challenging getting the LLMs to work well when hooking up custom APIs that interface with custom processes and data. This paper is interesting, largely because it may be a step forward in getting the LLM to accurately use custom tooling."
  },
  {
    "objectID": "posts/20240418/index.html#llm-agents",
    "href": "posts/20240418/index.html#llm-agents",
    "title": "Gorilla: Large Language Model Connected with Massive APIs",
    "section": "LLM agents",
    "text": "LLM agents\nWe first discussed what LLM agents are and how they operate. Joel provided us with a good introduction. LLM agents are tools with APIs that interact with LLMs to build a customized tool that serves your product’s custom needs. An NVIDIA blog post about LLM agents gives an example of using an LLM agent to summarize an earnings call, which requires a complex process more than just looking up an answer. Using an LLM agent is what the paper refers to as “using a tool via API calls.”"
  },
  {
    "objectID": "posts/20240418/index.html#why-this-paper-is-interesting",
    "href": "posts/20240418/index.html#why-this-paper-is-interesting",
    "title": "Gorilla: Large Language Model Connected with Massive APIs",
    "section": "Why this paper is interesting",
    "text": "Why this paper is interesting\nEven though one of the paper’s most interesting points is the test-time behavior of Gorilla, which can dynamically update its answer using up-to-date information instead of a static snapshot from training data, we think the main contribution lies in their APIBench benchmark and the fact that they were able to train a relatively small model, with better performance."
  },
  {
    "objectID": "posts/20240418/index.html#training-data",
    "href": "posts/20240418/index.html#training-data",
    "title": "Gorilla: Large Language Model Connected with Massive APIs",
    "section": "Training data",
    "text": "Training data\nWe were slightly confused by the authors’ description of training data, especially when they said they constructed six examples (Instruction-API pairs) for each of the three model hubs. From our understanding, the authors ended up having 1,645 data points and per data point, they had 6 samples, which they chose 3 out of, and then generated 10 pairs. We briefly looked at their training data shared on their GitHub repo. We were quite surprised that they included a variety of examples from requesting specific coding examples to asking for inspirations for poetry writing. We also appreciated that the authors released the training data to be more cognizant of their tool’s social impacts."
  },
  {
    "objectID": "posts/20240418/index.html#retrievers",
    "href": "posts/20240418/index.html#retrievers",
    "title": "Gorilla: Large Language Model Connected with Massive APIs",
    "section": "Retrievers",
    "text": "Retrievers\nJoel also provided the group with a brief explanation of how retrievers work. Essentially, retrievers do similarity match and pick the top ranked item based on a prompt. Retriever-aware training was introduced as a main feature of Gorilla, which enables test-time performance. That said, it was important to understand that Gorilla had two modes: zero-shot (without retriever) and retriever-aware."
  },
  {
    "objectID": "posts/20240418/index.html#final-thoughts",
    "href": "posts/20240418/index.html#final-thoughts",
    "title": "Gorilla: Large Language Model Connected with Massive APIs",
    "section": "Final thoughts",
    "text": "Final thoughts\nThis paper is still a preprint but the tool has been getting a lot of attention lately. However, we very much agreed that this paper could be more polished after a review process. For instance, we found that their best model performance came from the mode without retriever (zero-shot) even though their hightlight is the retriever-aware mode. Also, they had almost no mention of compute resources other than the model size. Given that this group already has follow-up papers based on Gorilla, we are interested in how this paper would be reviewed and further developed.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2024-gorilla-llm,\n    author = {Hongsup Shin},\n    title = {Gorilla: Large Language Model Connected with Massive APIs},\n    year = {2024},\n    month = {04},\n    day = {18},\n    howpublished = {rl{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20240418/},\n}"
  },
  {
    "objectID": "posts/20240530/index.html",
    "href": "posts/20240530/index.html",
    "title": "Modeling Tabular Data using Conditional GAN",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Modeling Tabular Data using Conditional GAN\nPresenter: Hongsup\nAttendees: Brian, Kate"
  },
  {
    "objectID": "posts/20240530/index.html#why-this-paper",
    "href": "posts/20240530/index.html#why-this-paper",
    "title": "Modeling Tabular Data using Conditional GAN",
    "section": "Why this paper?",
    "text": "Why this paper?\nAt work, I train models using tabular data with extreme class imbalance, which is often quite challenging. To tackle this, I have been looking into data augmentation techniques for tabular data, which is a relatively understudied domain compared to unstructured data. Synthetic Data Vault is a popular python library for tabular data synthesis, and the main techniques originate from the paper above. Since class imbalance is a common problem in real-world data and most of us work with tabular data, I think this will be an interesting paper to discuss."
  },
  {
    "objectID": "posts/20240530/index.html#paper-summary",
    "href": "posts/20240530/index.html#paper-summary",
    "title": "Modeling Tabular Data using Conditional GAN",
    "section": "Paper summary",
    "text": "Paper summary\nSeveral challenges lie in tabular data synthesis. They often have mixed data types with non-Gaussian or multi-modal distributions. Categorical variables in a tabular dataset often have severe class imbalance and provide insufficient samples. The authors argue that GAN’s flexibility can address these problems. The authors introduce several techniques for their approach, conditional-tabular GAN (CT-GAN).\n\nMode-specific normalization\nThe basic idea is mode-specific normalization is to convert a continuous-variable column into a set of columns where each column represents an estimated mode (like an indicator) and its value represents the weight of the corresponding mode. To estimate the number of the modes, they use variational Gaussian mixture (VGM) method.\n\n\nConditional generator and training-by-sampling\nTraditional GANs have Gaussian assumption where a vector is sampled from a standard multivariate normal distribution. However, this doesn’t account for class imbalance, which is a problem in tabular data. So the goal here is to resample data so that all categories are sampled evenly during training, and to estimate the conditional distribution of rows given that particular value at that particular column.\n\n\nCT-GAN model\n\n\n\nCT GAN model\n\n\nIn the CT-GAN model, the data are represented Conditional vector where each discrete column is a one-hot vector. The mask vector is used to only represent a category of a single discrete column (in one-hot way, the rest is all zero). The generator loss is cross-entropy between \\(m_{i^*}\\) (given by data) and \\(\\hat{d}_{i^*}\\) (produced by the generator) averaged over all rows in a batch. During the critic assessment stage, the model measures distance between learned conditional distribution and conditional distribution on real data while exploring all possible values evenly.\n\n\nModel evaluation\n\nBaseline models\nThe paper uses multiple baseline models for performance comparison. They use two Bayesian network models (CLBN and PrivBN), other GAN models (MedGAN, VeeGAN, TableGAN), and a tabular VAE (TVAE) model that they built. The TVAE model has multivariate Gaussian assumption.\n\n\nEvaluation metrics\nFor model comparison, the authors use two metrics. Likelihood fitness metric which estimates how well the synthetic data can fit the data oracle well. They also use ML efficacy, which measures whether ML model performance from synthetic data is comparable to that from using real data.\n\n\nResults\nFor model comparison, they used several different sets of data. First, they used multi-modal Gaussian-simulated data. Here, other GANs suffer from model collapse and the CT-GAN showed better performance. When simulated data from Bayesian networks were used, the Bayesian baseline models (CLBN and PrivBN) performed the best. With real-world data, TVAE and CTGAN performed the best although TVAE was often better."
  },
  {
    "objectID": "posts/20240530/index.html#discussion",
    "href": "posts/20240530/index.html#discussion",
    "title": "Modeling Tabular Data using Conditional GAN",
    "section": "Discussion",
    "text": "Discussion\nEven though we were pleased to see good baseline comparison with multiple different datasets, we thought the authors should have did more thorough work on literature review such as comparing their work with synthpop, a widely-used data synthesis package in R. They also could have looked into other statistical models because data synthesis an active research area in statistics.\nWe also questioned the validity of the VGM technique because it is the core of creating the embedding of continuous variables. The proposed model does not have checks that evaluate the fidelity of VGM estimation, and thus poor estimation of a multi-model distribution by it can further compromise model performance in general.\nWe also thought their claim on CT-GAN was a bit exaggerated. First, in most benchmark datasets, it was actually TVAE model that performed the best. In the performance comparison table, there was only one regression data, which we thought insufficient to evaluate the ML efficacy metric the authors proposed. Finally, the proposed models for the regression problem showed negative R scores, suggesting that the models performed quite poorly.\nOverall, we thought the approach was novel but the evaluation could have been improved given that tabular datasets are quite diverse. Also, given that GAN is expensive train, we would have liked to learn more practical aspects of the implementation.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2024-tabular-gan,\n    author = {Hongsup Shin},\n    title = {Modeling Tabular Data using Conditional GAN},\n    year = {2024},\n    month = {05},\n    day = {30},\n    howpublished = {rl{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20240530/},\n}"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Why ML Journal Club",
    "section": "",
    "text": "In machine learning (ML) community, numerous research papers and new tools come out everyday. For an individual ML practitioner like myself, it is impossible to check every paper and tool, and it is also difficult to know what works and what doesn’t. There are several weekly ML newsletters that provide a summary but I still need to carve out time after work to digest it. Besides, I prefer reading the papers in depth to examine how they exactly work and whether there are any flaws. But of course reading and critiquing papers on a regular basis requires discipline and dedication.\nThis is where a journal club can be useful. A group of people reading and dissecting papers together and having a discussion about them is certainly more fun and educational than doing everything alone. In a journal club, participants usually take turns to present a paper, which makes things easy. We can also hold each other accountable so that we as a group read papers on a consistent pace. Finally, the gathering itself becomes a good networking and information-sharing opportunity.\nI have gather a group of my fellow ML practitioner friends in Austin that I have known personally for several years. We are a diverse group of engineers and researchers with different interests and backgrounds. I am really excited to participate in this journal club with these brilliant friends to critique papers together, share our own practices at work, and socialize."
  },
  {
    "objectID": "posts/20230126/index.html",
    "href": "posts/20230126/index.html",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Leakage and the Reproducibility Crisis in ML-based Science\nPresenter: Kate\nAttendees: Athula, Hongsup, Joel, Kate, and Saina"
  },
  {
    "objectID": "posts/20230126/index.html#why-this-paper",
    "href": "posts/20230126/index.html#why-this-paper",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Why this paper?",
    "text": "Why this paper?\nData leakage is a problem that we have all encountered as data scientists and scientific researchers. We just never knew how big! I was drawn to this paper because it attempts to quantify the impact of data leakage. It presents the unique view of considering the impact on ML-based science and shows how widespread and problematic data leakage is and leads to exaggerated claims of predictive performance. However, the frequency of data leakage and the impact of making these types of mistakes could be just as high or higher for ML-based industry applications. As a mixed group of ex ML-based science practitioners and current ML-based industry practitioners, I thought it would be insightful to discuss the eight types of data leakage identified, the proposed mitigation strategy of filling out model info sheets for ML-based science, and if the presented solution is also reasonable for a variety of ML-based industry applications."
  },
  {
    "objectID": "posts/20230126/index.html#defining-the-leakage-and-the-reproducibility-crisis-in-ml-based-science",
    "href": "posts/20230126/index.html#defining-the-leakage-and-the-reproducibility-crisis-in-ml-based-science",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Defining the leakage and the reproducibility crisis in ML-based science",
    "text": "Defining the leakage and the reproducibility crisis in ML-based science\nThis first thing this paper does is narrow the scope of relevant literature to only papers that are used for ML-based science. The authors define ML-based science as only papers “making a scientific claim using the performance of the ML model as evidence.” Furthermore, research findings are termed reproducible “if the code used to obtain the findings are available and the data is correctly analyzed.” Lastly, data leakage is a “spurious relationship between the independent variables and the target variables that arises as an artifaction of data collection, sampling, or pre-processing strategy.” Based on these three definitions, 20 papers with data leakage in 17 fields were found to impact a total of 329 ML-based research papers. These authors make three explicit contributions: 1) present a unified taxonomy of eight types of data leakage that lead to reproducibility issues, 2) propose the use of model info sheets to identify and prevent leakage, 3) quantify the impact of data leakage on model performance of civil war predictions.  \nThe first thing we noticed when looking at Table 1 was that several of the fields are areas with potential high-stakes such as medicine, clinical epidemiology, neuropsychiatry, medicine, and radiology. Engineering applications were excluded from this analysis but several fields sounded very close to this industry such as software engineering and computer security. We didn’t feel that ML-based science should be differentiated from industry-based ML applications. If anything, we felt that data leakage in industry applications may be  harder to detect because they lack a formal peer review process and could have a bigger impact because these models are deployed in the real world."
  },
  {
    "objectID": "posts/20230126/index.html#data-leakage-taxonomy",
    "href": "posts/20230126/index.html#data-leakage-taxonomy",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Data leakage taxonomy",
    "text": "Data leakage taxonomy\n\nL1.1 No test set: This was shocking to us. How does the peer review process allow these papers to be published? Hopefully, in the future reviewers will be more demanding of ML practitioners. This is also the most common type of data leakage identified in Table 1.\nL1.2 Pre-processing on training and test set:  We felt that this was a loaded category. There are many different ways for this type of data leakage to come about. For example, during imputation, under/oversampling, and encoding. For clarity, we thought this category could have been subdivided.\nL1.3 Feature selection on training and test set: This is the second most common type of data leakage found in Table 1.\nL1.4 Duplicates in dataset: This seems more like an error than a type of data leakage.\nL2 Model features that aren’t legitimate: We spent the most time discussing this category. We found it hard to think about proxy variables because they require a lot of domain knowledge. Instead, we like the definition of a feature the model would not have access to when making new predictions.\nL3.1 Temporal leakage: This is when future data is included in model training.\nL3.2 Non-independence between training and test samples: This can be caused by resampling the same patient or geographic location. This type of leakage can also be due to the interpretation of the results. This occurs when there is a mismatch between the test distribution and the scientific claim.\nL3.3 Sampling bias in test distribution: This is choosing a non-representative subset for analysis.\n\nIn general, categories L1.1-4 are clear cut and should be identifiable and avoidable. L2 is difficult because it depends on domain knowledge. Due to the nature of collaboration the researcher with this expert domain knowledge may not be the individual performing the ML analysis. Similarly, L3.1-3.3 are also difficult to detect and avoid if the researcher is using previously collected data and not designing an experiment and collecting data from scratch. In industry, data is often given to ML practitioners and there is no option to immediately collect more data or change the collection methodology. If this is the case, data leakage could be undetectable in many industry applications as ML practitioners are working with the best data at hand. In addition, there are limited strategies for identifying and mitigating these three types of data leakage."
  },
  {
    "objectID": "posts/20230126/index.html#model-info-sheets-to-mitigate-data-leakage",
    "href": "posts/20230126/index.html#model-info-sheets-to-mitigate-data-leakage",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Model info sheets to mitigate data leakage",
    "text": "Model info sheets to mitigate data leakage\nWe appreciated that the authors suggested a mitigation strategy. We found the questions in the model info sheets to be a useful sanity check for any ML practitioner. However, the level of detail requested from the info sheets would likely only be filled out if peer reviewed journals required it.  Even if journals require model info sheets as part of the peer review process, we agree with the authors that there are still limitations. Most notably, the claims in the model info sheets can’t be verified without the code, computing environment, and data. There are some scientific research fields where authors might be unwilling or unable to disclose all information due to privacy concerns. In addition, there is nothing to prevent authors from lying or not having the ML expertise to fill out the model info sheet correctly. Since ML has become more accessible to non-ML experts, subtle cases might not easily be noticed by the author or reviewer.\nIn industry, data privacy is also a concern but ML practitioners within a company could use these model info sheets to identify data leakage in industry based applications. However, there is no way to enforce this across companies or for a user of an industry based ML application to determine if the results are sound. Industry based ML is growing and there are often many groups within a company experimenting with ML. From what we have seen, there are no established within-company standards let alone between-company standards that would require the use of model info sheets to prevent data leakage.\n\n\n\nFigure 1. A comparison of reported and corrected results in civil war prediction papers published in top political science journals."
  },
  {
    "objectID": "posts/20230126/index.html#civil-war-predictions",
    "href": "posts/20230126/index.html#civil-war-predictions",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Civil war predictions",
    "text": "Civil war predictions\nTo understand the impact of data leakage the authors present a case study on civil war predictions. They identified 12 papers on the topic that provided the code and data. Of the 12 papers identified, four claimed superior performance of complex ML models compared to logistic regression. When all types of data leakage were identified and fixed, the complex models did not outperform logistic regression (Figure 1). Figure 1 shows how the performance metric is reduced after data leakage is corrected. In particular, we noticed that model performance of the simple logistic regression model was not impacted as much by data leakage compared to the complex (tree-based) ML methods. We wish the authors had speculated on why."
  },
  {
    "objectID": "posts/20230126/index.html#final-thoughts",
    "href": "posts/20230126/index.html#final-thoughts",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Final thoughts",
    "text": "Final thoughts\nData leakage is a big issue that is not highlighted enough. The authors focused their efforts on identifying data leakage in ML-based science. While we appreciate them proposing a solution to identify and prevent data leakage, we think filling out model info sheets is a big ask and unlikely to be a common practice unless required by the peer review process. In addition, there are efficacy issues with model info sheets and it will be impossible to substantiate each claim. Industry ML applications are likely facing similar challenges with data leakage as ML-based science but these effects will be even more challenging to detect and quantify.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2023-data-leakage,\n    author = {Kate Behrman},\n    title = {Leakage and the Reproducibility Crisis in ML-based Science},\n    year = {2023},\n    month = {01},\n    day = {26},\n    howpublished = {rl{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20230126/},\n}"
  },
  {
    "objectID": "posts/20251023_ai_normal_tech/index.html",
    "href": "posts/20251023_ai_normal_tech/index.html",
    "title": "AI as Normal Technology",
    "section": "",
    "text": "Note\n\n\n\n\nArticle: AI as Normal Technology\nPresenter: Ivan Perez Avellaneda\nAttendees: Anil Kamat, Athula Pudhiyidath, Hongsup Shin"
  },
  {
    "objectID": "posts/20251023_ai_normal_tech/index.html#why-this-paper",
    "href": "posts/20251023_ai_normal_tech/index.html#why-this-paper",
    "title": "AI as Normal Technology",
    "section": "Why This Paper",
    "text": "Why This Paper\nAs ML practitioners, we often find ourselves caught between two exhausting extremes: on one side, breathless hype about AI solving everything, and on the other, existential dread about superintelligence destroying humanity. Neither perspective helps us navigate the real challenges we face daily—deploying models responsibly, understanding their limitations, and communicating realistic expectations to stakeholders.\nThis article cuts through the noise by suggesting we treat AI like any other transformative technology we’ve dealt with before. We wanted to discuss it because it validates what many of us have been feeling: that the most pressing issues aren’t about preventing AGI apocalypse, but about thoughtful deployment, institutional safeguards, and closing the gap between what models can do in benchmarks versus what they can actually accomplish in production. It offers a framework for having the grounded, practical conversations about AI policy and safety that are realistic and clear-eyed."
  },
  {
    "objectID": "posts/20251023_ai_normal_tech/index.html#paper-summary",
    "href": "posts/20251023_ai_normal_tech/index.html#paper-summary",
    "title": "AI as Normal Technology",
    "section": "Paper Summary",
    "text": "Paper Summary\nNarayanan and Kapoor propose viewing AI as “normal technology” rather than a potential superintelligence—a tool that humans can and should remain in control of, without requiring drastic policy interventions or technical breakthroughs.\n\nPart 1: The Speed of AI Transformation\n\nIn Part 1, the authors argue that AI’s societal impact will be gradual, not sudden. They distinguish between three phases: invention (model development), innovation (practical applications), and diffusion (widespread adoption). While AI research produces new models rapidly, the authors remind us that this does not mean translation to real-world utility. For instance, infrastructure and organizational inertia create substantial barriers, as implementing AI requires changes to existing systems, training, and workflows. This is especially true for high-risk areas.\nBesides, there exists a capability-reliability gap where impressive benchmark performance doesn’t guarantee real-world utility—what they call a problem of “construct validity.” This gap is particularly pronounced in safety-critical applications, which face slow adoption due to regulatory requirements and risk aversion.\nThe research community itself may be slowing innovation through what the authors describe as “ossification of canon”—a phenomenon where the proliferation of AI publications has led researchers to pile onto existing approaches rather than pursue genuine innovation. The paper notes that even transformative technologies like electricity took decades to reshape manufacturing, with Paul A. David’s 1990 paper quoting economist Robert Solow that computers were everywhere except in productivity statistics. This historical pattern suggests AI will follow a similar trajectory of gradual integration rather than sudden transformation.\n\n\nPart 2: Intelligence vs. Control\n\nThe authors make a critical distinction between intelligence and control over the environment. They challenge the superintelligence narrative by arguing that intelligence alone doesn’t automatically translate to power or control. They suggest that AI will likely not achieve meaningful superiority in crucial domains like geopolitical forecasting or mass persuasion, areas where human judgment and contextual understanding remain essential. This reframing implies that the focus should shift from model alignment research to system safety, which emphasizes preventing accidents and misuse through well-established engineering approaches.\nThe authors’ argument on alignment research as fundamentally misconceived is well represented in their quote: “trying to make a computer that cannot be used for bad things.” The alignment framing assumes AI systems will operate autonomously in high-stakes situations, which contradicts how safety-critical systems are actually deployed in practice. Instead of seeking to solve alignment as an abstract problem, the authors argue for treating AI safety like cybersecurity—a continuous process of identifying and mitigating specific vulnerabilities rather than seeking a final solution.\n\n\nPart 3: AI Risks Landscape\nRather than focusing on speculative catastrophic misalignment, the authors identify more immediate and concrete risks. In examining accidents and arms races, they note that different domains have fundamentally different safety dynamics. Aviation and self-driving cars demonstrate how market incentives can potentially align with safety when harm attribution is clear and liability is established. Social media, by contrast, shows what happens when this is not the case. Regarding the arms races between countries, particularly between the U.S. and China, the authors clarify that the focuses of the competition have been more on model development capabilities than deployment.\nRegarding misuse, the authors argue that primary defenses must be located downstream of models—at the point of application rather than in the models themselves. They emphasize that AI can also be used defensively against threats, creating a more complex security landscape than simple misuse scenarios suggest. However, they identify systemic risks as the most pressing concern, including the entrenchment of bias and discrimination, labor market disruption in specific sectors, increasing inequality and concentration of power, pollution of the information ecosystem, erosion of democratic institutions, and the enabling of authoritarianism. These risks have been consistent problems back in traditional ML days (pre-generative AI), and they can be aggravated by AI. Finally, they argue that these are more likely to materialize than speculative extinction scenarios.\n\n\nPart 4: Resilience Over Restriction\n\nThe authors advocate for a resilience-based policy framework rather than restrictive preemptive regulation (e.g., nonproliferation). They define resilience as “the capacity of a system to deal with harm,” which recognizes that “changes are inevitable in complex systems, and tries to manage and adapt to that change in ways that protect and preserve the core values and functions of the original system.”\nThey argue that AI risk probabilities lack meaningful epistemic foundations, unlike actuarial data used for insurance pricing, making probability-based policy interventions problematic. In this sense, their approach emphasizes reducing uncertainty through strategic research funding focused on downstream risks rather than abstract capability improvements, developing robust risk monitoring systems, and treating evidence gathering as a primary goal rather than a preliminary step.\nThis resilience framework operates across four categories:\n\nSocietal resilience that protects democratic institutions and civil society\nPrerequisites for effective defenses including technical and policy capabilities\nGenerally beneficial interventions that help regardless of AI’s trajectory, and\nNormal-technology-specific measures that assume AI remains under human control.\n\nThe authors firmly argue that nonproliferation of AI technology is infeasible and likely counterproductive. Instead, policy should focus on enabling beneficial diffusion through thoughtful regulation, promoting AI literacy across society, and supporting digitization and open government data initiatives that enable broader participation in the AI ecosystem."
  },
  {
    "objectID": "posts/20251023_ai_normal_tech/index.html#discussion",
    "href": "posts/20251023_ai_normal_tech/index.html#discussion",
    "title": "AI as Normal Technology",
    "section": "Discussion",
    "text": "Discussion\nAs a group, we all thought this paper presented several thought-provoking arguments that provide realistic and critical perspectives that pierce through the AI hype.\n\nGradual Ceding of Control\nOn the question of control, some of us expressed concern that we may already be on our way to ceding control gradually to AI systems even as the authors argue for maintaining human oversight. While the paper presents various control mechanisms—from auditing to circuit breakers—there’s a troubling sense that each new capability we delegate to AI represents a small surrender of human agency. The authors frame this as manageable through proper governance structures, but we wondered whether this incremental handover might create dependencies that become increasingly difficult to reverse, even if the technology remains “normal” rather than superintelligent. A good example would be AI companies getting actively involved in higher education for adoption.\n\n\nLimitations in Forecasting and Persuasion\nHowever, we did not completely agree on the paper’s confidence about AI’s limitations in forecasting and persuasion. The authors predict that AI won’t meaningfully outperform humans at geopolitical forecasting, but we thought the authors did not provide enough explanation on why. More concerning was the persuasion argument—while the authors carefully qualify their prediction to focus on persuading people “against their self-interest,” recent incidents involving AI chatbots and vulnerable individuals, particularly teenagers, suggest that AI systems may already possess concerning persuasive capabilities in specific contexts. The paper’s distinction between costless persuasion tests and real-world scenarios with meaningful stakes is valid, but we felt it might underestimate the vulnerability of certain populations and the subtler forms of influence that don’t require dramatic against-interest actions.\n\n\nOmission of Military AI\nA notable omission that troubled us was the near-complete absence of military AI applications from the paper’s analysis. While the authors briefly touch on arms races in the context of model development competition between nations, they largely sidestep the reality of autonomous weapons systems and military AI deployment. This feels like a significant gap, particularly given that military AI operates under fundamentally different constraints than civilian applications—with inherent opacity, limited oversight mechanisms, and incentives that may actively resist the “normal technology” framework.\nThe literal arms race in lethal autonomous weapons systems presents exactly the kind of scenario where human control could be rapidly eroded and safety measures would be easily compromised, not through superintelligence but through competitive pressures. While we understand that military AI might deserve its own separate treatment, we think excluding it entirely weakens the paper’s argument that we can maintain meaningful human control through institutional and regulatory mechanisms.\n\n\nThe Alignment Distraction\nDespite these reservations, we found the paper’s reframing of AI safety particularly compelling. The authors make an excellent point that the intense focus on model alignment has diverted attention from more immediate harms like the erosion of democratic institutions, entrenched discrimination, and labor displacement. This critique resonates strongly with our experience as practitioners who see these concrete issues manifesting today while policy discussions on ML fairness and AI safety still fall behind the advancements in tech industry.\nInterestingly, our group was split on whether the paper’s tone was optimistic or pessimistic. Some saw it as sobering in its catalog of potential harms, while others found it refreshingly empowering in its assertion that we still have agency to shape AI’s trajectory. Perhaps this divergence itself validates the paper’s central thesis: by moving away from binary utopian or dystopian narratives, we can have more nuanced, productive conversations about AI governance. The paper provides a valuable framework for cutting through the exhausting cycle of hype and doom that dominates AI discourse, offering instead a pragmatic path forward based on historical precedent and institutional resilience.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2025-ai-normal-tech,\n    author = {Hongsup Shin},\n    title = {AI as Normal Technology},\n    year = {2025},\n    month = {10},\n    day = {23},\n    howpublished = {\\url{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20251023_ai_normal_tech/},\n}"
  },
  {
    "objectID": "posts/20251120_illusion_of_thinking/index.html",
    "href": "posts/20251120_illusion_of_thinking/index.html",
    "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity",
    "section": "",
    "text": "Note\n\n\n\n\nArticle: The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\nComment paper: Comment on The Illusion of Thinking by Anthropic\nPresenter: Hongsup Shin\nAttendees: Anil Kamat, Eric Abelson, Sanhita Joshi, Satish K C"
  },
  {
    "objectID": "posts/20251120_illusion_of_thinking/index.html#why-this-paper",
    "href": "posts/20251120_illusion_of_thinking/index.html#why-this-paper",
    "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity",
    "section": "Why This Paper",
    "text": "Why This Paper\nThe question of whether large reasoning models (LRMs) can actually reason has become increasingly contentious as these models seem to show impressive performance on various benchmarks. However, several recent papers have challenged the notion that chain-of-thought prompting and extended reasoning traces represent genuine reasoning capability. “The Illusion of Thinking” by Apple Research contributes to this debate with a more thorough and comprehensive experimental design compared to many other papers in this space.\nWhat made this paper particularly interesting was the rapid response it received from Anthropic researchers, who published a comment paper shortly after the original release. This back-and-forth between research teams provided us with an opportunity to examine not just the original methodology, but also how different organizations interpret similar experimental findings and defend their technologies.\nThe paper’s approach of using parametrically controlled puzzles to prevent memorization and address potential data contamination represents a thoughtful attempt to isolate reasoning capabilities from pattern matching. This methodological choice aligns with growing concerns in the field about whether impressive benchmark performance reflects genuine problem-solving ability or pattern-matching."
  },
  {
    "objectID": "posts/20251120_illusion_of_thinking/index.html#paper-summary",
    "href": "posts/20251120_illusion_of_thinking/index.html#paper-summary",
    "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity",
    "section": "Paper Summary",
    "text": "Paper Summary\n\nThe Apple Research team designed experiments using 4 types of logic puzzles: Tower of Hanoi, Checkers Jumping, River Crossing, and Blocks World. These puzzles can be controlled parametrically in terms of complexity, allowing the researchers to systematically increase difficulty and observe how model performance degrades. The goal was to test whether LRMs demonstrate genuine reasoning capabilities when faced with problems that cannot be solved through memorization or pattern matching against training data; note that the authors did not explicitly address whether these puzzles were free from data contamination issues.\nThe paper includes a literature review covering several key challenges in LLM reasoning research. Among the issues discussed are overthinking (where models generate excessive reasoning without improvement), unfaithfulness of reasoning traces to actual model computation, training distribution gaps between verification and generation, and the difference in inference compute budgets between standard models and thinking models. Given these compute budget differences, pass@k becomes a particularly relevant metric to consider. Pass@k measures the probability that at least one correct solution appears among k generated samples—for instance, if a thinking model uses 10k tokens per attempt while a non-thinking model uses 1k tokens, comparing pass@1 for the thinking model against pass@10 for the non-thinking model provides an apples-to-apples computational budget comparison. This framing is especially important when one model type invests significantly more compute per individual generation.\nThe experimental design categorizes puzzle difficulty into three regimes: easy, moderate, and hard. Across these regimes, the authors observe dramatic accuracy drops after certain puzzle difficulty thresholds.\n\nFigure 4 presents the core empirical findings, comparing thinking models (Claude 3.7 Sonnet with thinking, DeepSeek-R1) against non-thinking counterparts across all four puzzles. The results show cliff-like performance drops as complexity increases: Tower of Hanoi and Checker Jumping maintain high accuracy through only 3-4 steps before collapsing to near-zero; Blocks World degrades more gradually; and River Crossing fails almost immediately after the simplest cases. Critically, thinking models show minimal advantage over non-thinking models—the curves largely overlap across all puzzles, suggesting extended reasoning traces provide little benefit for these tasks.\n\n\n\nFigure 8a and 8b. Despite providing the solution algorithm in the prompt, execution failure occurs at similar points, highlighting reasoning model limitations in logical step execution.\n\n\nOne of the paper’s most interesting findings comes from Section 4.4, where researchers provided models with the exact solution to Tower of Hanoi puzzles. Despite having access to the correct answer, models still failed at similar rates to when they solved the puzzles without assistance. Here’s the prompt the authors used (Appendix A.2):\n\n\n\n\n\n\nNoteExample of Prescribed Algorithm for Tower of Hanoi\n\n\n\nHere is a pseudocode of recursive algorithm to solve the puzzle:\nALGORITHM Solve(n, source, target, auxiliary, moves)\n    // n = number of disks to move\n    // source = starting peg (0, 1, or 2)\n    // target = destination peg (0, 1, or 2)\n    // auxiliary = the unused peg (0, 1, or 2)\n    // moves = list to store the sequence of moves\n\n    IF n equals 1 THEN\n        // Get the top disk from source peg\n        disk = the top disk on the source peg\n        // Add the move to our list: [disk_id, source, target]\n        ADD [disk, source, target] to moves\n        RETURN\n    END IF\n\n    // Move n-1 disks from source to auxiliary peg\n    Solve(n-1, source, auxiliary, target, moves)\n\n    // Move the nth disk from source to target\n    disk = the top disk on the source peg\n    ADD [disk, source, target] to moves\n\n    // Move n-1 disks from auxiliary to target\n    Solve(n-1, auxiliary, target, source, moves)    \nEND ALGORITHM\nTo solve the entire puzzle of moving n disks from peg 0 to peg 2:\n\nInitialize an empty list ‘moves’\nExecute Solve(n, 0, 2, 1, moves)\nThe ‘moves’ list will contain the complete solution\n\nNote: When executing this pseudocode, track which disk is currently on top of each peg. The disk IDs in the moves list should correspond to the actual disk being moved.\nYou can use this algorithm as a scratchpad to help you solve the problem step by step.\n\n\nThe paper includes analysis of reasoning traces through various figures. Figure 6 shows the relationship between thinking tokens and puzzle complexity, revealing that token consumption actually decreases after reaching certain difficulty levels. Figure 7 examines when in the solution sequence models make their first errors, showing that in easier Tower of Hanoi instances, models generate correct solutions early but make errors later, while the opposite pattern emerges at higher difficulties."
  },
  {
    "objectID": "posts/20251120_illusion_of_thinking/index.html#discussion",
    "href": "posts/20251120_illusion_of_thinking/index.html#discussion",
    "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity",
    "section": "Discussion",
    "text": "Discussion\n\nExperimental Design Issues\nWhile this paper makes valuable contributions to the reasoning debate, we have several methodological concerns. The most fundamental issue relates to how difficulty is characterized across the puzzle types. The authors control complexity parametrically but never explicitly discuss the inherent difficulty differences between puzzle types. This becomes problematic when comparing results across puzzles and attempting to draw general conclusions about reasoning capabilities.\nThe regime classifications (easy, moderate, hard) appear to be defined post-hoc based on model performance rather than on any independent measure of puzzle difficulty. The paper provides no scientific or statistical criteria for these boundaries. Plus, this approach does not add particular value to their conclusions either.\n\n\nMissing Qualitative Analysis of Reasoning Traces\n\n\nThe treatment of reasoning traces is a bit disappointing. While Figure 6 shows that thinking token consumption decreases after certain difficulty thresholds, the paper never reveals qualitative characteristics of these traces. We’re left to speculate whether models are giving up, attempting exhaustive search that fails, or trying heuristic approaches that don’t work. Figure 13 in the appendix contains raw versions of these plots with substantial error bars, suggesting Figure 6 may be somewhat misleading in its cleaned-up presentation. Throughout the paper, results from all models are not consistently shown, raising concerns about potential cherry-picking.\nThe distributional analysis in Figure 7, which is summarized in the paper’s prominent Figure 1, reveals far more complexity than the summary statistics suggest. In Tower of Hanoi puzzles at easier difficulties, models can generate correct sequences early but accumulate errors later. At higher difficulties, this pattern reverses—models correct earlier mistakes as they progress. Understanding this behavior requires examining actual reasoning traces, which the paper doesn’t provide. The puzzles also exhibit different distributional patterns without explanation, leaving important questions unanswered.\n\n\nThe Tower of Hanoi Solution Experiment\nSection 4.4 contains perhaps the paper’s most important finding, yet interestingly it’s also the least explored. When given exact solutions for Tower of Hanoi, models achieved similar accuracy to solving without assistance. This is a remarkable result that deserves deeper investigation, but the authors only tested this on Tower of Hanoi and not the other three puzzle types. This matters because Tower of Hanoi consistently shows different results than the other puzzles throughout the paper. We suspect this may reflect data contamination issues or the puzzle’s recursive structure, where difficulty at the \\(n\\)th step equals difficulty at the \\(n+1\\)th step. Additionally, the prompt apparently tells models they “can” use the provided solution rather than forcing them to use it—a subtle but potentially important distinction.\n\n\nUnreliable Performance at Higher Cost\n\nFigure 12 raises serious practical concerns about thinking model reliability. The distribution of first failure moves shows much higher variability for thinking models compared to non-thinking models. This has troubling implications for real-world deployment: thinking models are simultaneously less reliable and more expensive to run. Users who don’t know the correct answer to their questions (the typical use case) might believe thinking models are more thoughtful or intelligent based on benchmark results, yet these models demonstrate less stable and more unpredictable performance in practice. This disconnect between benchmark performance and actual reliability—combined with higher computational costs—contradicts the value proposition of reasoning models and warrants deeper investigation into when and why this instability occurs.\nFigure 13 reveals enormous variation in thinking token expenditure, sometimes showing cases where models fail quickly with few tokens and other cases where they consume massive token counts before failing. The characteristics of reasoning traces in these different failure modes would be very interesting, but the authors chose not to investigate this qualitatively."
  },
  {
    "objectID": "posts/20251120_illusion_of_thinking/index.html#the-anthropic-rebuttal",
    "href": "posts/20251120_illusion_of_thinking/index.html#the-anthropic-rebuttal",
    "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity",
    "section": "The Anthropic Rebuttal",
    "text": "The Anthropic Rebuttal\n\nValid Points and Overreach\nThe Anthropic comment paper makes several valid points while also engaging in some overstatement. The distinction between “can’t solve” and “I will stop here” given token limits is important and well-taken. However, their invocation of “statistical inevitability” seems exaggerated. The reasoning unit is a sequence, not individual tokens, and the actual sequence length \\(T\\) is considerably smaller than the 10,000 tokens they use in their Section 2.1 calculation.\nMore fundamentally, for logic puzzles where correctness requires every step to be right, their probabilistic argument falls short. Tower of Hanoi puzzles can be solved perfectly by existing software, so arguing that cumulative error is statistically inevitable misses the point—these are deterministic problems with algorithmic solutions.\nThe comment paper’s discussion of \\(p(correct) &lt; 1\\) doesn’t adequately address the constant-difficulty nature of Tower of Hanoi. If a model correctly executes the sequence at early steps, it should maintain that correctness at later steps since the difficulty remains constant and no memorization is required. To use a programming analogy, finding the computation correct at i=0 but incorrect at i=100 in what amounts to a for loop would be unacceptable, and suggests something other than reasoning is occurring.\n\n\nThe Complexity Claim\nAnthropic’s complexity claim reevaluation makes a good point that our journal club discussion also identified. The Apple paper doesn’t adequately address difficulty variations across puzzle types. As Anthropic notes, River Crossing is NP-hard, which may explain why models particularly struggle with it. This asymmetry across puzzle types complicates the paper’s broader claims about reasoning capabilities.\nThe comment paper’s closing statement, “The question isn’t whether LRMs can reason, but whether our evaluations can distinguish reasoning from typing”, sounds like an overreach. The problem absolutely remains whether LRMs can reason. Moreover, the rebuttal never addresses why models failed to perform better even when explicitly given correct solutions. That finding from Section 4.4 of the original paper stands as powerful evidence that something fundamental is missing, regardless of how we define or evaluate reasoning.\n\n\nWhat the Debate Reveals\nThe debate between these papers highlights an important challenge in AI research: distinguishing genuine capability from sophisticated pattern matching. Both papers make contributions, but both also show how methodological choices, presentation decisions, and framing can shape interpretation of experimental results.\nFor practitioners trying to understand what these reasoning models can and cannot reliably do, the answer seems to remain unclear. However, this public exchange between researchers and the emergence of novel evaluation tasks represent healthy scientific discourse that pushes the field toward more rigorous assessments of reasoning capabilities.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2025-illusion-of-thinking,\n    author = {Hongsup Shin},\n    title = {The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity},\n    year = {2025},\n    month = {11},\n    day = {20},\n    howpublished = {\\url{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20251120_illusion_of_thinking/},\n}"
  },
  {
    "objectID": "posts/20240229/index.html",
    "href": "posts/20240229/index.html",
    "title": "Are Emergent Abilities of Large Language Models a Mirage?",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Are Emergent Abilities of Large Language Models a Mirage? (Schaeffer, Miranda, and Koyejo 2024)\nPresenter: Brian\nAttendees: Hongsup"
  },
  {
    "objectID": "posts/20240229/index.html#why-this-paper",
    "href": "posts/20240229/index.html#why-this-paper",
    "title": "Are Emergent Abilities of Large Language Models a Mirage?",
    "section": "Why this paper?",
    "text": "Why this paper?\nGiven the inherent open-ended nature of Large Language Model (LLM) responses and their ability to respond to queries relating to diverse topics, proper evaluation of performance is no simple task. This paper engages deeply with the question of how we are evaluating LLMs and what implications that has for understanding performance improvements as these models grow larger. Given the incredible increase in hype over the past several years, one might think that the models of today were a stark shift up from past iterations, but the improvement might be a smoother growth than previously thought.\nThis article was selected as one of the outstanding main track papers at NeurIPS 2023. It also generated some attention in the press, with an article by Vice."
  },
  {
    "objectID": "posts/20240229/index.html#summary",
    "href": "posts/20240229/index.html#summary",
    "title": "Are Emergent Abilities of Large Language Models a Mirage?",
    "section": "Summary",
    "text": "Summary\nFor a 15-minute video summary of the paper by one of the authors, check out his presentation at NeurIPS.\n\nEmergence\nThe title of this paper can be seen as a response to Wei et al. (2022). This previous work defined emergent abilities as “abilities that are not present in smaller-scale models but are present in large-scale models”. The authors of that work showed that there were many downstream tasks where LLMs appeared to show emergent behavior, as shown in the below figure.\n\n\n\nSupposed emergence in LLMs across a wide array of tasks. This is Figure 2 in Wei et al. (2022) and is reproduced as Figure 1 in Schaeffer, Miranda, and Koyejo (2024)\n\n\nFrom these previous definitions, Schaeffer, Miranda, and Koyejo (2024) distilled two main properties of emergence:\n\nSharpness: behaviors appear suddenly\nUnpredictability: it seems impossible to forecast at what model size such behaviors will appear\n\nHowever, they argue that emergent behavior is more an artifact of the performance metric and evaluation procedure as opposed to an inherent quality of the models themselves.\n\n\nAn Alternative Explanation\nA common observation in deep learning is that models appear to obey what are called ‘neural scaling laws’, where the test loss exhibits a power law relationship with the size of the model. The notion of size can vary from amount of compute to training dataset size to number of parameters, but empirically similar trends have been observed. For the most part, this paper uses the number of parameters.\nAccordingly, when visualized on a log-log plot, we should see a linear decreasing trend between number of parameters and the loss. Under some simple assumptions, this means that the probability of a correct token will increase linearly with the log of the number of parameters. These two relationships are visualized in Figure 1 A and B.\n\n\n\n\n\n\nFigure 1\n\n\n\nThus, by analyzing the relationship between a scoring metric and the per-token probability of success, one can see how the scores would evolve as the model size grows. As seen in Figure 1 C and D, nonlinear or discontinuous metrics can give the appearance of ‘emergence’, even though the model improvement with size is growing steadily and predictably. If instead models are scored by continuous or linear metrics (E and F) then much more smooth growth is observed.\nAs a more concrete example, imagine the task is \\(L\\)-digit arithmetic. If we score the LLM output using accuracy (output is 1 if exact match, otherwise 0), then all \\(L\\) tokens must be correct, and thus the score is nonlinear with respect to the per-token probability. Meanwhile, a score like token edit distance would be linear in the probability.\nThe authors argue this discrepancy in metric can almost fully explain emergence. They also point to two other possible factors: insufficient resolution to measure model performance for smaller model scales (due to small test sizes) and insufficient sampling for larger model scales.\n\n\nValidating the Explanation\nTo showcase the viability of this explanation, the authors embark on three different investigations, which we describe briefly.\n\nUtilizing Instruct-GPT, the authors show that emergence evaporates upon switching to linear/continuous metrics or increasing test set size to more accurately measure performance.\nBy exploring results from a large benchmark that tests a variety of model families using a variety of metrics and across different tasks, the authors show that emergence is generally limited to only a few metrics as opposed to task-model family pairs. In this analysis, they utilize the benchmark known as BIG-Bench (Srivastava et al. 2022) and adopt a metric from that paper called “breakthroughness”, although rename it as an emergence score. It is worthwhile to note that the seed idea of this paper–breakthroughness/emergence being heavily related to which metric is used–was briefly discussed in the BIG-Bench paper (Section 3.4.2).\nThe authors show that one can use this logic of nonlinear/discontinuous metrics to ‘induce’ emergence in other scenarios, notably vision tasks. This induced behavior is summarized in the figures below."
  },
  {
    "objectID": "posts/20240229/index.html#our-discussion",
    "href": "posts/20240229/index.html#our-discussion",
    "title": "Are Emergent Abilities of Large Language Models a Mirage?",
    "section": "Our Discussion",
    "text": "Our Discussion\nWe found this paper enjoyable and illuminating. The paper had a relatively simple main message, and argued their point compellingly, although we did have some confusion about the details of certain analyses. In particular, the meta-analysis of claimed emergent ability using BIG-Bench (Section 4) raised several questions. For example, we were not sure how the authors went from a distribution of emergence scores to a cutoff for ‘possible emergent abilities’. The figure for this section seemed to come from two sources; one visualized the purely quantitative emergence scores from BIG-Bench, while the other depicted hand-annotated data from elsewhere. It was not well explained how these were linked. Additionally, accuracy was previously described as an emergent-prone metric, but that metric showed no suggestion of emergence in this analysis. This was not discussed.\nAs the authors mention in their discussion, this line of work does not rebut the possibility of emergence nor does it really dictate which metrics are best for analyzing a single model. Rather, it simply suggests that if your goal is to model performance across changing model scores, attention should be paid to use metrics which are linear or continuous with the per-token correctness probability.\nAs a final note, we discussed how ML papers often have much stronger and eye-catching language than many other academic disciplines. In some cases, this can be a good thing to attract attention, but it is also easy to misinterpret and make stronger. For example, the aforementioned Vice article attaches the qualifier ‘scary’ to its headline when mentioning the theorized emergent AI capabilities. This kind of language makes it easy for the conversation to shift away from the core of the article and into more vague notions of ‘sentience’ and AI existential risk. These latter topics are certainly important and interesting, but they are mostly divorced from the actual content of this particular paper.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2024-emergent-abilities,\n    author = {Brian King},\n    title = {Are Emergent Abilities of Large Language Models a Mirage?},\n    year = {2024},\n    month = {02},\n    day = {29},\n    howpublished = {rl{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20240229/},\n}"
  },
  {
    "objectID": "posts/20250925/index.html",
    "href": "posts/20250925/index.html",
    "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: On the Theoretical Limitations of Embedding-Based Retrieval (arXiv:2508.21038v1, submitted August 28, 2025). This is a preprint that has not yet undergone peer review.\nPresenter: Hongsup Shin\nAttendees: Athula Pudhiyidath, Ivan Perez Avellaneda\nThis paper tackles a fundamental question that’s becoming increasingly relevant as embedding models are required to solve more complex retrieval tasks. The authors connect communication complexity theory to modern neural language model embeddings, and argue that the combinatorial explosion of possible top-k document sets will inevitably overwhelm fixed-dimensional embeddings. Given our interest in retrieval architectures and their fundamental capabilities, this paper offers an interesting theoretical lens, and tries to expand their idea with empirical experiments based on a synthetic dataset."
  },
  {
    "objectID": "posts/20250925/index.html#paper-summary",
    "href": "posts/20250925/index.html#paper-summary",
    "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
    "section": "Paper summary",
    "text": "Paper summary\nThe core theoretical contribution connects the sign-rank of a query-relevance matrix (qrel, where rows represent queries and columns represent documents) to the minimum embedding dimension needed to represent it. Specifically, the authors prove that for a binary relevance matrix A, the row-wise order-preserving rank is bounded by \\(\\text{Rank}_{\\pm}(2A - 1) - 1 \\leq \\text{Rank}_{\\text{rop}}(A) \\leq \\text{Rank}_{\\pm}(2A - 1)\\), where \\(\\text{Rank}_{\\pm}\\) denotes the sign-rank and \\(\\text{Rank}_{\\text{rop}}(A)\\) represents the minimum embedding dimension needed to correctly order documents for all queries. This means that for any fixed dimension d, there exist combinations of top-k documents that cannot be represented regardless of the query.\nTo validate this empirically, the authors conduct free embedding (simulated embeddings that are unconstrained by natural language) experiments where they directly optimize query (without backpropagation) and document vectors using gradient descent. They find that for k=2, there exists a critical point for each dimension d where the number of documents becomes too large to encode all combinations. Fitting these critical points across dimensions yields a smooth polynomial relationship that is monotonically increasing.\nThe experimental evaluation relies heavily on testing models at various embedding dimensions, which brings up an important technique: Matryoshka Representation Learning (MRL); think Russian dolls. MRL is a training technique that enables embeddings to maintain meaningful representations when truncated to smaller dimensions. It achieves this by using a loss function that combines losses computed at multiple embedding dimensions simultaneously. This trains the model so that the first d dimensions contain a complete (if lower-fidelity) representation, rather than requiring all N dimensions. This is crucial because naively truncating a standard embedding model’s 4096-dimensional output to, say, 512 dimensions would destroy the representation, as those first 512 dimensions were not optimized to be independently meaningful.\nThe paper tests both MRL-trained models (which can be fairly evaluated at smaller dimensions) and standard models (where truncation introduces severe artifacts), though not all models in comparison use MRL techniques, implying that potential severe loss in information may have occurred in smaller embedding spaces.\nThe authors then create LIMIT, a dataset designed to stress-test this limitation using natural language. Documents describe people and their preferences (e.g., “Jon likes Quokkas and Apples”), and queries ask “Who likes X?” Despite this simplicity, state-of-the-art embedding models achieve less than 20% recall@100, while BM25 scores above 90%.\n\n\n\nOverview of the LIMIT dataset design and experimental setup for testing embedding limitations with k=2 attribute combinations.\n\n\nBelow is an example of a document in the LIMIT corpus:\n\n{“_id”: “Olinda Posso”, “title”: ““,”text”: “Olinda Posso likes Bagels, Hot Chocolate, Pumpkin Seeds, The Industrial Revolution, Cola Soda, Quinoa, Alfajores, Rats, Eggplants, The Gilded Age, Pavements Ants, Cribbage, Florists, Butchers, Eggnog, Armadillos, Scuba Diving, Bammy, the Texas Rangers, Grey Parrots, Urban Exploration, Wallets, Rainbows, Juggling, Green Peppercorns, Dryers, Pulled Pork, Holland Lops, Blueberries, The Sound of Wind in the Trees, Apple Juice, Markhors, Philosophy, Orchids, Risk, Alligators, Peonies, Birch Trees, Stand-up Comedy, Cod, Paneer, Environmental Engineering, Caramel Candies, Lotteries and Levels.”}\n\nThe paper concludes by briefly examining alternatives to single-vector embeddings that might avoid these theoretical limitations. Cross-encoders (e.g., Gemini-2.5-Pro, which uses a long context reranker) successfully solve the LIMIT task with 100% accuracy but remain too computationally expensive for first-stage retrieval at scale (note: the details of how they conducted this experiment are quite vague). Multi-vector models like ColBERT show improvement over single-vector approaches (though still fall short of solving LIMIT), while sparse models like BM25 succeed due to their effectively much higher dimensionality. However, the authors note it’s unclear how sparse approaches would handle instruction-following or reasoning-based tasks without lexical overlap."
  },
  {
    "objectID": "posts/20250925/index.html#strengths",
    "href": "posts/20250925/index.html#strengths",
    "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
    "section": "Strengths",
    "text": "Strengths\nThe theoretical motivation is quite interesting. Connecting sign-rank to retrieval capacity provides a formal framework for understanding embedding limitations, and the proof itself is clean, well-constructed, and relatively easy to follow. The observation that as retrieval tasks require representing more top-k combinations (especially through instruction-based queries connecting previously unrelated documents), models will hit fundamental limits is genuinely important for the field.\nThe authors’ decision to test models across different embedding dimensions using MRL is commendable. While the execution has issues (inconsistent treatment of MRL and non-MRL models, unclear figure legends), the underlying motivation to examine information loss in truncated embeddings is sound and addresses an important question.\nFinally, we liked that the authors tried to bridge theory and practice by creating a concrete dataset. Many theoretical papers stop at proofs and toy examples, but the LIMIT dataset represents a genuine effort to operationalize the theoretical limitations. The dataset takes the abstract notion of “all top-k combinations” and maps it to natural language. The progression from free embedding experiments (pure mathematical optimization) to synthetic but linguistic data (LIMIT) to comparisons with models trained on existing benchmarks (MTEB, BEIR) shows methodological rigor in trying to validate the theory at multiple levels of abstraction. Even if the execution has flaws, the attempt to move beyond pure theory into testable predictions is highly valuable."
  },
  {
    "objectID": "posts/20250925/index.html#weaknesses",
    "href": "posts/20250925/index.html#weaknesses",
    "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
    "section": "Weaknesses",
    "text": "Weaknesses\nHowever, we found the gap between theoretical insight and experimental validation concerning. Several fundamental issues may undermine the paper’s claims:\n\nExperimental methodology issues\n\nFree embedding experiments are questionable as “best case”\nThe authors frame free embedding optimization as the ‘best case’ scenario, arguing that if direct vector optimization cannot solve it, real models won’t either. But this “unconstrained = best” logic reasoning conflates representational capacity with optimization tractability. Inductive biases in real models (e.g., semantic structure learned from language) don’t just constrain what can be represented, but they actively guide optimization by shaping the loss landscape. Free embeddings optimizing in unconstrained high-dimensional space may actually face harder optimization challenges than semantically grounded models, potentially getting trapped in poor local minima precisely because they lack guiding structure. Whether free embedding failure truly upper-bounds neural retriever performance, or simply reflects different optimization dynamics, remains unclear.\n\n\nFlaws in experimental design regarding varying embedding sizes\nThe dimension ablation experiments are particularly problematic. When evaluating models at different dimensions d &lt; N (where N is the model’s native dimension), they simply truncate to the first d dimensions. This causes massive information loss because these dimensions were not trained to be independently meaningful at arbitrary truncation points. Even for models trained with MRL, which does enable meaningful truncation, the paper is inconsistent about which models used MRL versus which were naively truncated.\nThe domain shift experiment suffers from the same truncation issue, making it hard to disentangle whether poor performance stems from domain mismatch or from damaged representations. Moreover, this experiment relies solely on fine-tuning without providing sufficient detail on whether the limited data samples were adequate to meaningfully change embedding behavior. With smaller dimensions especially, full training (rather than fine-tuning) might have been more appropriate, though we acknowledge this would be computationally expensive. Fine-tuning with truncated naive embeddings (which suffer from severe information loss) doesn’t convincingly demonstrate the authors’ point about theoretical limitations—it may simply reveal the inadequacy of the fine-tuning approach itself.\n\n\nMissing theoretical depth in key places\n\n\n\nPolynomial relationship between embedding dimension and the critical number of documents at which free embeddings fail to represent all k=2 combinations. The smooth cubic fit suggests deeper theoretical structure.\n\n\nThe polynomial fit to the critical-n points (Figure 2) is impressively smooth, suggesting deeper theoretical structure that remains unexplored. Why specifically a degree-3 polynomial? Is there a theoretical derivation for this functional form, or is it purely empirical? Similarly, the connection to order-k Voronoi diagrams is mentioned but dismissed as “computationally infeasible” and “notoriously difficult to bound tightly” without much detail.\nDespite deriving a polynomial relationship between dimension and critical document count, the paper disappointingly provides no practical guidance. A natural question remains unanswered: given N documents with k-attribute queries, what minimum embedding dimension is needed? The fitted curve could inform such estimates, but the authors do not discuss whether their framework enables practical dimensionality recommendations or what additional work would be required to derive them.\nThe qrel pattern experiments (Figure 6) show that “dense” patterns are much harder than random, cycle, or disjoint patterns, but no theoretical framework connects the patterns to sign-rank. These experiments are also where exploring different values of k could have been particularly informative.\n\n\n\nDataset design problems\n\nLIMIT doesn’t test what it claims to test\n\n\n\nPerformance comparison on LIMIT dataset\n\n\nThe LIMIT dataset has a fundamental mismatch. While queries are simple (‘Who likes Quokkas?’), the documents are highly unnatural. Each document lists approximately 50 randomly assigned attributes, a structure difficult to be found in natural language. This creates what is essentially a memorization stress test rather than a semantic understanding task. Documents like ‘Olinda Posso likes Bagels, Hot Chocolate, Pumpkin Seeds, The Industrial Revolution, Cola Soda, Quinoa…’ (continuing for ca. 50 items) bear very little resemblance to natural text. The fact that BM25 achieves over 90% while neural models struggle below 20% is revealing: this task rewards exact term matching (lexical memorization) rather than semantic understanding.\n\n\nLimited exploration of k undermines the combinatorial complexity claim\nThe paper’s central thesis is that combinatorial complexity fundamentally limits embeddings, yet the empirical validation examines only k=2. If combinatorial explosion is truly the core issue, the natural experiment is to vary k systematically (k=2, 3, 4, 5…) and demonstrate where and how performance degrades. Without this, their argument on combinatorics becomes weak.\nMoreover, k=2 represents a fundamentally shallow per-instance retrieval task: each query asks only whether two specific attributes co-occur in a document. This is qualitatively different from queries requiring inference, reasoning, or understanding complex semantic relationships. The difficulty comes not from the cognitive complexity of individual queries, but from the sheer volume of attribute combinations to memorize numerous pairs. This is combinatorial overload of a simple operation rather than genuine semantic complexity. The paper’s claim about combinatorial limitations would be more compelling if tested on queries that actually require complex reasoning for each instance, not just checking membership in a memorized list.\n\n\n\nSuperficial analysis\n\nThe MTEB correlation analysis is superficial\nThe paper compares SOTA models’ poor LIMIT performance with BEIR performance (Figure 7, Section 5.5), and suggests that “smaller models (like Arctic Embed) do worse on both, likely due to embedding dimension and pre-trained model knowledge.” Yet large models like E5-Mistral (4096-d) also score poorly on LIMIT. The correlation analysis doesn’t support strong conclusions either way, and the discussion of dimension dependence is hand-wavy given that high-dimensional models also struggle.\n\n\nThe “alternatives” section adds little\nGiven these limitations in the empirical validation, the paper’s brief discussion of alternatives to single-vector embeddings feels incomplete. The discussion of cross-encoders, multi-vector models, and sparse retrievers is disappointingly shallow. These are well-known alternatives, and the paper provides no new insights about how they circumvent the theoretical limitations or what trade-offs they entail at scale. The observation that ColBERT performs better offers little explanation of why or what this tells us about the limitations.\n\n\nAbsolutist framing obscures nuance\nThe paper tests one narrow scenario (k=2, synthetic memorization task), finds poor performance, and then makes sweeping claims like “embedding models cannot represent all combinations” without adequate explanation or qualification. The framing presents binary success/failure when the real questions are graded and contextual: which combinations matter in practice? How much worse is performance, and in what ways? Would these limitations manifest in real-world retrieval scenarios with natural language?\nMoreover, the embedding models achieved about 20% recall, not zero, yet there is no analysis of which queries or combinations succeeded versus failed. Understanding this pattern, especially across different embedding dimensions, could reveal whether failures are random or systematic and whether certain types of combinations are more representable than others. This would provide far more insight than aggregate failure rates alone."
  },
  {
    "objectID": "posts/20250925/index.html#final-thoughts",
    "href": "posts/20250925/index.html#final-thoughts",
    "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
    "section": "Final thoughts",
    "text": "Final thoughts\nThis paper doesn’t fully deliver on its promise. The theoretical contribution is solid and potentially important, but the empirical validation is flawed and does not support the strong claims being made. The sign-rank framework provides a useful lens for thinking about embedding capacity, and the paper does demonstrate something real: state-of-the-art neural embedding models struggle to exhaustively memorize even small combinatorial spaces (k=2 combinations). This empirical finding is noteworthy, even if the authors don’t analyze which combinations succeed versus fail or whether certain patterns are systematically more difficult.\nHowever, without cleaner experiments that separate genuine theoretical limits from artifacts of poor experimental design, we can’t conclude whether state-of-the-art models are hitting fundamental boundaries or simply failing at an artificial memorization task with broken evaluation methodology. The theoretical limitations are real—the question is whether they matter for realistic semantic retrieval tasks, where queries require understanding meaning and context rather than exhaustively memorizing unnatural attribute lists. The paper raises important questions about the future of instruction-based retrieval and alternatives to single-vector embeddings, but doesn’t yet provide convincing evidence that these theoretical limits will constrain models on practical retrieval tasks.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2025-embedding-retrieval-limitations,\n    author = {Hongsup Shin},\n    title = {On the Theoretical Limitations of Embedding-Based Retrieval},\n    year = {2025},\n    month = {09},\n    day = {25},\n    howpublished = {\\url{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20250925/},\n}"
  },
  {
    "objectID": "posts/20230223/index.html",
    "href": "posts/20230223/index.html",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: “Machine Learning Approaches to Real Estate Market Prediction Problem: A Case Study\nPresenter: Athula\nAttendees: Hannah, Hongsup,Kate"
  },
  {
    "objectID": "posts/20230223/index.html#why-this-paper",
    "href": "posts/20230223/index.html#why-this-paper",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "Why this paper",
    "text": "Why this paper\nIn my work, I deal with real estate data. The data consists of various attributes of real estate properites, some of which follow the ebbs and flows of the mutable economy. I came across this paper while perusing arxiv, and I was compelled by its abstract: using real estate data that spanned 10 years from a single county in Florida, the research compares four ML models that make predictions about whether or not a property’s appraised price matches its eventually listed price. Since the data spans such a long time, the researchers accounted for fluctuations in socio-economic factors over time in the model; this facet was of particular interest to me to conceptualize my own work and I was interested to learn about the authors incorporated these factors in machine learning algorithms to make predictions about the real estate market.\nWhile this abstract seemed reasonably promising to understand the application of ML in real estate evaluation, when I read the paper more closely, I found myself questioning many of the approaches the authors took to answer their central question. Therefore, this blog is structured such that it summarizes the authors’ work as outlined in this paper and offers alternative considerations from our group’s discussion."
  },
  {
    "objectID": "posts/20230223/index.html#the-data",
    "href": "posts/20230223/index.html#the-data",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "The data",
    "text": "The data\nThe authors wanted to build and compare models for predicting home prices. They posit that previous literature addressed home pricing with hedonic regression models, which are linear regression models with specific focus on price of goods, i.e. property price, but that this work did not necessarily account for broader socio-economic factors in their models. With the modeling conducted in this paper, the authors attempted to model home pricing accounting for such factors. However, instead of predicting home pricing as a continous variable as with regression, the authors chose to make home prediction a binary classification problem. The data used in this research consist of ~94,000 rows of publicly available real estate sale data from a single county in Florida, Volusia County.\n\nOutcome variable\nThe way the authors defined the binary outcome variable for this research was interesting: they took the final sale price of the home and compared it to the appraised government price of the home, assigning the variable 1 if the property’s selling price is above the appraised price (which they call high price), and 0 if not (called low price); reasoning on why this designation was useful to classify was left unsaid. More importantly there was no information on how or when the properties were initially appraised by the government, which makes the comparison to final sale price unclear. Government appraisals are technically known as a home’s assessed values and differ from appraised values. Assessed valuations are made to determine yearly property tax rates for a home in a given area; this is generally determined by the broad characteristics of a home and the taxed values of homes in the region. An appraised valuation of a home, on the other hand, is done by an appraiser who does a more thorough check of the features of a home, like its style and appliances, and determines a price for the current market price of your home. Thus, it is possible the authors of the paper were using home assessment prices instead of appraisal prices. The authors don’t provide a tally on how many of the properties are classified as high price, though the authors do show a plot (see paper Fig. 2) that suggests that the total count of high price properties have steadily increased in the 10 years of this data. But despite this fact, appraisal year or sale year are not predictor variables the authors considered.\nAnother way the author could have approached the outcome variables was to have compared the initial listing price to the final sale price. In fact, in the abstract the authors tout that the listing price is baseline against which the final sale price would be compared, but the final models differ from this initial assertion. Regardless, this binary outcome variable falls short because the magnitude of difference between any two price comparisons would be lost; a high price differential of $100k would be treated the same as a lower price differential of $5k.\n\n\nPredictor variables\nFor the predictor variables, the authors chose 21 features or columns from the data for consideration for the models. However, some of the variables were not considered in the right format: parid (property identifier), nbhd (neighborhood code), yrblt (year built), zip21 (ZIP code of area), sale_date (sale date), and luc (property class) were all variables that the authors encoded as continuous variables but should have been treated as categorical variables (see Table 1). Furthermore, the variable sale_date (sale date of the property) was not separated into its component month or year values and instead was used as a single datetime value. From our group’s discussion about this paper, it was also suggested that the authors could have considered time variables in terms of seasons or quarters, as home sales typically vary by such cadences.\nBecause, this data spanned approximately 10 years, the authors attempted to account for the market fluctuations in those years by incorporating economic factors of gross domestic product (GDP), consumer price index (CPI), producer price index (PPI), housing price index (HPI), and effective federal funds rate (EFFR). But, it was unclear where the authors collected these values from and how exactly they matched this collected information to the real estate data. There was little discussion about why these variables were chosen out of the many other indices available, and little information about how these indices were good markers for the central question. Furthermore, it was unclear whether all of these ecoonimic factors were actually available at the time of prediction when the model is used for prediction."
  },
  {
    "objectID": "posts/20230223/index.html#data-culling",
    "href": "posts/20230223/index.html#data-culling",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "Data culling",
    "text": "Data culling\nThe authors took their initial dataset and used a myriad of approaches to reduce and refactor the data. First, the authors correlated the predictor variables to the outcome variables and excluded variables which were not correlated; however, they did not account for variables that are highly correlated with one another (see paper Fig. 5) which is cause for multicollinearity in how the authors interpret the importance of features in the models subsequently. Furthermore, later on when the authors introduce the voting classifier, they deem that “the method performs best when the predictors are as independent from each another as possible.”\nNext, the authors perform a set of preliminary modeling, using the dataset in random forest and XGBoost models and examining the resulting feature importance of those model predictions to make decisions about which of the variables are used in the final models. Every step of this interim modeling process was unclear, thus making it hard to give any credit to the authors’ interpretations of the variable importance charts (Fig. 6-8). Regardless, the authors used these outcomes to further enmesh the predictor variables, using a technique called ‘mean encoding’ to merge together highly ranked variables, creating two new variables which they term F1 and F2. Mean encoding seems to be a technique for encoding variable identities into a model that accounts for how those variables interact with the outcome variables. This type of encoding of variables, in addition to the method of feature selection of predictor variables through interim modeling of the data before processing those select features to a bigger model, is an approach that is teeming with data leakage."
  },
  {
    "objectID": "posts/20230223/index.html#algorithm-predictions-and-results",
    "href": "posts/20230223/index.html#algorithm-predictions-and-results",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "Algorithm predictions and results",
    "text": "Algorithm predictions and results\nFinally, the authors get to their model comparison stage. At this point of the paper it was unclear why exactly these models were being compared against one another. The ultimate question the authors laid out was out of focus and it was unclear what exactly model comparison of the data-leaked dataset would bring.\nThe authors went on to compare the dataset with random forest, XGBoost, voting classifier, and logistic regression classifiers. The authors said they used a 10-fold cross validation method (separate to the 5-fold cross validation method they did for the preliminary modeling for the feature selection step). While hyperparameter tuning is briefly discussed, the authors don’t provide any details on what models were tuned or the parameters. The authors evaluate differences between these classifiers namely by using accuracy, precision, and recall. However, why the authors chose these metrics to evaluate their high-low classifier is unclear, and how it served their goals. Is it more important for the classifier to catch correctly labeled lower labels, or higher labels? And if it’s better at one rather than the other, what impact would it have for the market?\nBy this point, the research question was not sufficiently motivated, and the data was ‘double-dipped’ and overly engineered. In fact, in presenting the comparisons between algorithm performance, the authors showcase how the engineered data performs better at classification of the task, which is of no surprise because the data features were initially chosen because they were better at classifying the task (see paper Fig. 20, 21).\nThe seeming conclusion the authors draw from this section that XGBoost was a superior classifier, but considering all of these factors above, this conclusion did not follow logic. Perhaps a more logical conclusion could have been reached had the authors employed a linear regression model instead; in this way they could have predicted the final sale price as a continous variable instead of a binary variable one as they attempt to do here."
  },
  {
    "objectID": "posts/20230223/index.html#final-thoughts",
    "href": "posts/20230223/index.html#final-thoughts",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "Final thoughts",
    "text": "Final thoughts\nThis paper’s central question is unclear, and deviated from the beginning to end. The paper switched from comparing listing home prices to sold home prices, to appraisal home prices (which were likely assessed home prices) to sold home prices, to figuring out which model is best. If the authors had a more structured story on the questions they wanted to answer, the baseline with which they wanted to compare outcomes, and shored up methods on those comparisons, this would have been a more compelling read. As it stands, however, it is not possible to draw any conclusions from this paper.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2023-real-estate-prediction,\n    author = {Athula Pudhiyidath},\n    title = {Reviwing a Case Study About Real Estate Market Prediction},\n    year = {2023},\n    month = {02},\n    day = {23},\n    howpublished = {rl{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20230223/},\n}"
  },
  {
    "objectID": "posts/20230622/index.html",
    "href": "posts/20230622/index.html",
    "title": "Let’s Verify Step by Step",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Let’s Verify Step by Step\nPresenter: Akshata\nAttendees: Hongsup, Meghann, Saina"
  },
  {
    "objectID": "posts/20230622/index.html#why-this-paper",
    "href": "posts/20230622/index.html#why-this-paper",
    "title": "Let’s Verify Step by Step",
    "section": "Why this paper?",
    "text": "Why this paper?\nThis is an interesting new paper by OpenAI that discusses how we can apply the same principles we use to solve math problems to AI. It’s interesting to see how a process based supervision approach is superior to outcome based supervision approach."
  },
  {
    "objectID": "posts/20230622/index.html#paper-summary",
    "href": "posts/20230622/index.html#paper-summary",
    "title": "Let’s Verify Step by Step",
    "section": "Paper summary",
    "text": "Paper summary\nThe paper evaluates different approaches to solving a dataset comprising of math problems. With this approach, they trained the model to get the right answer and but also “think” through the problem to arrive at the right answer. The authors mention that there are numerous benefits to this approach:\n\nModel reasoning is explainable as each step of the process is provided a probability score (positive, negative and neutral)\nWrong steps that would lead to incorrect outcomes are identified early on in the training process\nWrong steps that would lead to the correct solution are also identified and discouraged\nModel could be trained to reason and model its behaviors that matches human values which leads to reduction in hallucinations, false information and potentially dangerous outcomes.\n\nThe authors then attempted to showcase superior model performance with a few datasets which we then discussed."
  },
  {
    "objectID": "posts/20230622/index.html#discussion",
    "href": "posts/20230622/index.html#discussion",
    "title": "Let’s Verify Step by Step",
    "section": "Discussion",
    "text": "Discussion\n\nWhy apply language modeling for math problems? Some of us thought a logic-/rule-based approach for creating math solutions might be smarter and more efficient than using a language model because it will requires a massive amount of training data.\nWe also thought multi-step math solutions pose an interesting question because it makes it very clear (hence easy) to identify the correctness of individual steps in inference and deduction, compared to other language problems.\nSome of us questioned why the authors didn’t have enough mention of reinforcement learning although we eventually agreed that the paper’s main focus is about how to define the reward function properly.\nMany of us thought the idea of the process model is very similar to explainable AI (XAI) or interpretable AI. This is of course related to how we need to conduct value assessment when a multi-step solution reaches a correct answer with incorrect steps. This would be a big problem for XAI.\nSpeaking of value assessment, we had a question about the audience of these LLM-generated math solutions because they were extremely detailed, which we assumed are for students. Whether LLMs can tailor solutions based on the level of audience knowledge would be an interesting question to look into.\nWe had a length discussion about the “neutral” label because this would be where human labelers subjectivity will matter a lot. It would’ve been nice if the authors mentioned this more. Also, we were disappointed that the paper didn’t have any mention of who the human labelers were because they were the ones who contributed to the training data.\nCompared to other deep learning papers, we appreciated that the authors did attempt to include many details of the models even though like many deep learning models, we felt the explanation insufficient. For instance, it wasn’t perfectly clear of the multi-step solutions are fed as input to the model during training.\nWe had a brief discussion of model poisoning attacks because ChatGPT and its likes collect user data and we wondered how it can conduct quality control of user input. At the end of the paper, we sensed that the authors can’t have full knowledge of every training data, and then we wonder how they can be sure of what kind of data is used to generate output.\nFigure 3 and similar figures that are followed by this somewhat puzzled us because there was no clear explanation of what the majority voting baseline was. Besides, Figure 3 shows that when the solutions are simple, there wasn’t much performance between the two models. Also, model performance was generally lower when the solution was simple. If LLMs can’t solve simple math problems, we wonder their positive findings matter or not.\nFigure 4 shows performance that is quite low (close to 50%) and we weren’t entirely sure whether this is an acceptable level of performance or not.\nSome of us criticized the use of the term “alignment tax”, which makes AI safety quite negative, which is essentially requirements for any public-facing AI applications.\nUnder 6.3, the authors claim that there wasn’t “clear sign of memorization”, but they did not provide a clear definition of this clear sign. Are they talking about overfitting? Regardless, this requires quantifiable evidence.\nThe multi-step learning model made us think about implementing course-correct steps in model training. For instance, using a callback function, if we can somehow detect the model is on a clearly wrong path, how can we pull it back so that we don’t waste compute resource and train the model in a more efficient way? Some of us thought it might have to do with devising a more interactive and granular cost function.\n\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2023-verify-step-by-step,\n    author = {Akshata Mohan and Hongsup Shin},\n    title = {Let's Verify Step by Step},\n    year = {2023},\n    month = {06},\n    day = {22},\n    howpublished = {url{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20230622/},\n}"
  },
  {
    "objectID": "posts/20221215/index.html",
    "href": "posts/20221215/index.html",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Zero-Shot Text-to-Image Generation\nPresenter: Hongsup\nAttendees: Hannah, Hongsup, Joel, and Steve"
  },
  {
    "objectID": "posts/20221215/index.html#why-this-paper",
    "href": "posts/20221215/index.html#why-this-paper",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Why this paper",
    "text": "Why this paper\nGenerative AI has a lot of hype in ML community these days. OpenAI’s DALL·E, GPT-3, and ChatGPT are good examples. And there’s also stable diffusion. Since they all have public API, not just ML practitioners but general public can use the models to generate texts or images, which creates even bigger hype around generative AI.\nBut whenever there is hype around something, I think we should be more curious about what’s going on behind the scene. Understanding how it works helps us see through the hype and that is why I chose this paper. We can understand how DALL·E’s text-to-image generative model works, what the authors did to make this happen, and how they validated the result.\nTo understand this paper thoroughly, you need to know other deep learning model frameworks such as transformer, variational autoencoder, and OpenAI’s CLIP (Contrastive Language-Image Pre-training) model. I found these two articles extremely useful, written by Charlie Snell at UC Berkeley. In this post, I will talk about a high-level summary and the interesting discussion we had as a group. If you are interested in more detailed summary of the paper itself, I recommend those two posts."
  },
  {
    "objectID": "posts/20221215/index.html#big-picture",
    "href": "posts/20221215/index.html#big-picture",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Big picture",
    "text": "Big picture\nThe authors created a deep learning model which generate images from a text input. For instance, if you type “hands”, the model will generate images of hands. As the title says, this is done in zero-shot way, meaning that it can generate images that it hasn’t seen before. To be clear, the authors of this paper are not the first ones who created a model like this. There have been precedents but the authors say that the generated images from those still suffer from severe artifacts such as object distortion, illogical object placement, or unnatural blending of foreground and background elements. So the authors made improvements by adopting these two approaches: using a large set of training data and building a bigger model.\nBefore we look into the results, let’s first talk about the model architecture. Their model consists of two parts: variational autoencoder (VAE) and transformer."
  },
  {
    "objectID": "posts/20221215/index.html#variational-autoencoder-vae",
    "href": "posts/20221215/index.html#variational-autoencoder-vae",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Variational autoencoder (VAE)",
    "text": "Variational autoencoder (VAE)\nThe VAE contributes to the generative nature of the model because VAEs have latent representation in the middle that is a probability distribution. Once trained, we can use this distribution to draw samples from it, providing a generative framework. To train the VAE, the authors assumed uniform prior over the latent space. The model can learn the actual prior from the transformer later to generate images that match to text input. To train the VAE, the authors used images with text captions from various sources such as Wikipedia images.\nWhat is interesting about the VAE they used is that it assumes discrete latent distribution instead of continuous. This variant of VAE is called vector-quantized VAE (VQ-VAE). The motivation is that images and texts are discrete than continuous. But this assumption comes with a major complication: a discrete space is non-differentiable (i.e., can’t back-propagate). That’s why VQ-VAE has a codebook, which is essentially a look-up table where a discrete representation is associated with a codebook vector. To be accurate, this paper used a variant of VQ-VAE called dVAE where they made this look-up as a weighted average to further smooth out the space.\nThis VAE also acts as a dimensionality reduction technique because the discrete latent space the authors used has a resolution of 32x32 instead of 256x256, the resolution of the original training images. This brings compression benefit so that the transformer doesn’t have to memorize extremely long sequence but a sequence of length 1024 (=32*32)."
  },
  {
    "objectID": "posts/20221215/index.html#transformer",
    "href": "posts/20221215/index.html#transformer",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Transformer",
    "text": "Transformer\nOnce the VAE is learned, we can abandon the uniform prior assumption and use transformer to learn the actual prior. Transformers help image generation by pixel-wise prediction in an autoregressive way. For instance, given the sequence of previous pixels, the transformer can predict what the next pixel would look like.\nOnce the transformer is trained, when we give a text prompt to the model, the transformer makes predictions for the image latents (32x32 space) in an autoregressive way. Once we have all predictions, we use the dVAE codebook to lookup the vectors and generate the image. Since we can sample the sequence in a new way, we can generate multiple images. The authors used a top k approach to return the best images by ranking the generated images from a candidate pool based on the scores from OpenAI’s CLIP model, which represents how well the images match the caption.\nThe transformer has 12 billion parameters and a good chunk of the paper is dedicated to all the tricks the authors came up with to fit the model in GPU."
  },
  {
    "objectID": "posts/20221215/index.html#discussion",
    "href": "posts/20221215/index.html#discussion",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Discussion",
    "text": "Discussion\n\nAre the results representative enough?\nMost of us were somewhat disappointed by the authors’ model validation. Figures 3 and 4 in the paper gave some idea of how realistic the generated images are but we were not sure whether these were cherry-picked or not because the spectrum of images the model can generate is so wide. Figure 7 showed results from human evaluators. Most of them said the authors’ model was more realistic than the competitors’. Aside from the ethical issues surrounding hiring mturk workers, we thought the number of mturk workers was small (5 people) and the number of images they evaluated was small as well.\n\n\nWhy not investigate model failures?\nWhat was more interesting to us was Fig. 8, the CUB dataset which have images of birds. The example images here looked worse than others and the authors speculated that this was due to the detail-oriented text information of images, which might have been lost during the compression in dVAE. This was a plausible explanation but we wanted to see more in-depth investigation on model failures. There are numerous examples of terrifyingly looking images of hands generated by DALL·E because apparently it keeps failing at generating images of humans hands with five fingers.\nWe also discussed the lack of investigation on model failure from an ethical and responsible AI perspective. If OpenAI was going to publish a public API for a model like this, which would have varying degrees of socio-technical impact (look at all the issues ChatGPT has been creating these days), it would have been more responsible for them to test the model’s capacity more thoroughly and rigorously before rolling it out.\nWe found a model card from their repository and it was disappointingly short and did not address any possible ethical and social ramifications that would be caused by the model.\n\n\nValidity of the scoring metrics\nThe authors used FID and IS scores (generated by the CLIP model) to assess how well the images reflect the text input. The scores were used to rank a pool of candidate images and the model returned top k results. We questioned the validity of the decision behind using these scores because they are model-dependent, which means they are training-data-dependent. Plus, there was no mention of (at least) a qualitative comparison between the training datasets of this paper and the CLIP paper. This made us question the reliability of the CLIP model scores. It might have been interesting to see a batch of images that were ranked high (or low) so that we could judge the validity of the scores and understand the model behavior better.\n\n\nQualitative contribution\nAs in other deep learning papers, it was difficult for us to understand which decisions they made led to their results and advancement. For instance, they highlighted the larger training dataset and the larger model size. What was the measurable impact of each, and which one was more important? Similar to this, it would have been nice if they had some guidance on model tuning and hyperparameter selection to inform other researchers on model architecture design.\n\n\nReproducibility and novelty\nTo be blunt, the main highlight of this paper seemed to be the scale. They were able to use bigger datasets with a bigger model. But let’s be honest, how many academic institutions or companies are able to afford to train a model with 12 billion parameters? Especially without proper model inspection, how can we understand the model properly when we can’t reproduce it easily? Although there were certain elements of novelty especially on their tricks of utilizing GPU resources to train the model, if the scale is the main factor of success, can we really call this as a novel invention?"
  },
  {
    "objectID": "posts/20221215/index.html#final-thoughts",
    "href": "posts/20221215/index.html#final-thoughts",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Final thoughts",
    "text": "Final thoughts\nThanks to the paper, we learned that VQ-VAE and transformer together can generate images from text inputs. However, we questioned the results and model validation especially due to the lack of investigation on model failure. We also thought about ethical aspect of this model being available in public. Just because it belongs to computer vision, which tends to amuse general audience, it does not mean that it is exempt from any social responsibility. And in deep learning with image and speech data, it is often the case that model validation is often looser than tabular data used in industries with higher stakes such as health care, finance, or risk assessment. That said, we would like to learn more about other techniques mentioned in the paper to have a deeper understanding of how they work.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2022-dalle,\n    author = {Hongsup Shin},\n    title = {Zero-Shot Text-to-Image Generation},\n    year = {2022},\n    month = {12},\n    day = {15},\n    howpublished = {rl{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20221215/},\n}"
  },
  {
    "objectID": "posts/20231019/index.html",
    "href": "posts/20231019/index.html",
    "title": "Why Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Why do tree-based models still outperform deep learning on typical tabular data?\nPresenter: Kate\nAttendees: Athula, Brian, Hongsup, Joel, Kshitij"
  },
  {
    "objectID": "posts/20231019/index.html#why-this-paper",
    "href": "posts/20231019/index.html#why-this-paper",
    "title": "Why Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?",
    "section": "Why this paper?",
    "text": "Why this paper?\nThis paper piqued my interest because it is a topic that I commonly grapple with. While deep learning techniques are highly publicized for superior performance on image and language data, many practical machine learning (ML) applications rely on tabular data. As an ML practitioner, there is a desire to use cutting edge techniques if they make sense for the data and application. This paper addresses the outstanding question, “Does deep learning outperform tree-based models on tabular data?” In order to answer this question, several shortcomings that have hindered a direct comparison of these modeling techniques on tabular data were addressed. They also compare the inductive biases of deep learning vs tree-based models to help further the development of tabular-based deep-learning architectures"
  },
  {
    "objectID": "posts/20231019/index.html#methods",
    "href": "posts/20231019/index.html#methods",
    "title": "Why Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?",
    "section": "Methods",
    "text": "Methods\nBenchmark creation\nOne of the major contributions of this paper is a set of 45 tabular benchmarks used in this study and to be used in the future. The lack of standard and consistent tabular benchmarks was identified by the authors as one of the main reasons tabular studies aren’t reproducible. They fill this gap by making their tabular datasets available through the OpenML platform. This is a major advancement compared to the referenced literature that provides comparisons on 3 and 11 selected tabular datasets.\nFor a dataset to be included as a benchmark, it must meet the following criteria:\n\nHeterogenous columns- each feature is different in nature (not the same signal with a different sensor).\nNot high dimensional- number of dimensions (d) less than 500; d/n (number of observations) less than 1/10.\nNo undocumented datasets- removed if little information available.\nI.I.D. data – no time series or stream-like data.\nReal-world data- no artificial datasets with the exception of simulated data of practical importance.\nNot too small- no datasets with less than 4 features or less than 3k samples.\nNot too easy- remove dataset if a simple model (single tree or regression) scores within 5% of a default Resnet or HistGradientBoosting model.\nNot deterministic- remove dataset if the target is a deterministic function of the data (simulated games).\n\nIn addition to the strict criteria above, the authors also removed “side issues” for practical purposes. Five side issues were removed by applying the following rules:\n\nMedium-sized training set- data too large was truncated to 10k samples\nNo missing data- remove all missing data by column first and then row.\nBalanced classes- binarized data if there is more than 1 class and under sampled to retained equal samples of the top two frequency classes.\nLow cardinality categorical features- remove categorical features with more than 20 items.\nHigh cardinality numerical features- remove numerical features with less than 10 unique values and convert to categorical if there are 2 categories.\n\nData preprocessing was minimal after the removal of side issues and benchmark criteria was met. All models that don’t directly include categorial features used ScikitLearn’s OneHotEncoder on categorical features. Regression targets were log-transformed when skewed. All features for neural network training were transformed using ScikitLearn’s QuantileTransformer.\nModel selection\nThe authors compare 3 tree-based and 4 deep learning models. For tree-based methods the authors chose all ScikitLearn ensemble models: RandomForest, GradientBoostingTrees (or HistGradientBoostingTrees for datasets with categorical features), and XGBoost. For deep learning, they chose Multi-layer perceptron (MLP), Resent, FT_transformer (simple transformer with embedding for categorical and numerical features), and SAINT (transformer model with inter-sample attention).\nModel tuning and selection\nThe authors make an effort to tune both deep learning models and tree-based models in a comparable way for each dataset and model using predefined hyperparameter grids and a random search of roughly 400 iterations in Hyperopt-Sklearn. These hyperparameter searches were saved and used as a bank to study performance as a function of the number of random searches. Each number of random searches was repeated 15 times per model and benchmark to asses variance in model performance. For each search the best hyperparameters of the validation set were chosen and evaluated on the test data. Each dataset was divided such that 70% was used for training, 9% for validation, and 21% for test.\nAggregating results across datasets\nThe classification datasets were assessed using test accuracy and regression datasets were assessed using the R-squared score. To aggregate results across benchmarks, the average distance to the minimum was used to “normalize scores” between the best model, 1, and worst model, 0. However, the worst model was replaced with the test score of 10% quantile for classification and 50% quantile for regression. All negative values were reset to 0. This procedure of model tuning and selection, bootstrapping (15 times per model and benchmark), and aggregating across benchmarks was repeated several times to answer the original question and investigate each inductive biases."
  },
  {
    "objectID": "posts/20231019/index.html#results",
    "href": "posts/20231019/index.html#results",
    "title": "Why Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?",
    "section": "Results",
    "text": "Results\nModel performance was analyzed separately for classification and regression tasks (Figure 1). Furthermore, the benchmarks were subdivided into those that only had numerical features and those that had numerical plus categorical features. The normalized test accuracy of 16 classification datasets with only numerical features revealed that XGBoost, GradientBoostingTrees, and RandomForest had higher normalized accuracy than the 4 deep learning models for all number of random searches. The results were also similar for the 19 regression datasets with only numerical features except that the SAINT model outperformed GradientBoostingTrees only for a low numbers of random searches (less than 10). XGBoost was the top performing model for all random search iterations for both classification and regression tasks with only numerical features. For the numerical plus categorical features HistGradientBoostingTree was added. HistGradientBoostingTree is the top-performing model when random search iterations are low (less than 10 for classification and less than 14 for regression) and XGBoost is the superior model for high search iterations. All tree-based methods outperform all deep learning methods for classification and for regression when the number of searches is more than 10.\n\nThe authors came to two big conclusions from this work:\n\nHyperparameter tuning of deep learning models did not improve their performance to be better than tree-based methods\nDeep learning methods were not hindered by the inclusion of categorical features.\n\nEmpirical investigation: why do tree-based models outperform deep learning on tabular data?\nThe authors aim to understand the inductive biases or “inherent properties of these models which explain their superior performance on tabular data.” The tree-based models with superior performance are bagging and boosting algorithms that are based on ensembles of decision trees. Therefore, this section focuses on understanding why decision trees explain tabular data well. In this section, hypotheses were tested by applying a treatment to the input data and measuring average normalized test performance.\nFirst, smoothing transformations were applied to the target features of the tabular classification datasets. This was done by applying a Gaussian kernel smoothing function stretched across different scales. The normalized test accuracy was compared across 15 iterations of 60 hyperparameter searches for four kernels. The four models compared had different performance when no smoothing is applied (Figure 2). However, when smoothing is applied the tree-based models (RandomForest and GradientBoostingTree) are more sensitive to the degree of smoothing. This shows that tree-based methods fit better to irregular functions than neural networks. This is consistent with studies in the literature that show that neural networks perform well on smooth functions.\n\nSecond, the number of non-informative features was varied in two experiments, one removing non-informative features (Figure 4a) and one adding non-informative features (Figure 4b). Removing non-informative features reduced the performance gap between tree-based and deep learning models. Next, uninformative features sampled from standard Gaussians distribution were added to each datasets. The additional of uninformative features decreased the normalized test score of MLPs (Resnet) much more than GradientBoostingTrees and RandomForest. This shows that tree-based methods are less affected by uninformative features.\n\nThird, tree-based methods and transformers are not rotationally invariant. MLPs are rotationally invariant. The logic here wasn’t quite adequately explained but we followed the main idea. Which is that it is harder for rotationally invariant algorithms to remove uninformative features because the invariant algorithm needs to identify the orientation and then select the least informative. To test this theory, the authors randomly rotated the training datasets and computed the normalized test accuracy for the original dataset (no rotation) and with rotation. The rotation had no effect on the test performance for Resent. Furthermore, model performance with no rotation in order of highest to lowest was GradinetBoostingTree, RandomForest, FT Transformer and Resnet. With the rotation, the order was completely reversed. This indicates that rotational invariance is not a desirable property for models trained using tabular data.\nIn the end the authors use these three inductive biases as a way to recommend how to improve neural network architecture for tabular data. They conclude that a neural network architecture must be robust to uninformative features, preserve the orientation of the data, and learn sharp irregular shaped functions to perform well on tabular data. The authors acknowledge many limitations to their current approach. Most of these have to do with the criteria for benchmark create and the side issues that were removed."
  },
  {
    "objectID": "posts/20231019/index.html#discussion",
    "href": "posts/20231019/index.html#discussion",
    "title": "Why Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?",
    "section": "Discussion",
    "text": "Discussion\nIn general we found this paper very interesting and thought provoking. We thought the authors made a good effort to create a new set of tabular benchmark datasets. However, we spent a lot of time talking about the benchmark criteria and how this could bias the results presented here. We found the maximum datasets size and number of features to be very restrictive such that it doesn’t include many of the datasets used in our work. These numbers felt like magic since there wasn’t any discussion of why or how the numbers were chosen. The benchmark criteria also removed datasets that were “too easy”. We thought that this action could bias the results because smoother target functions may be removed in this step. This could impact the first inductive bias that most target functions are irregular and captured by better by tree-based models. Neural networks are better at picking out smooth functions but these datasets may have been removed from the tabular benchmarks during creation. We also felt that removal of missing data and imbalanced class ratios was an oversimplification as these issues are common in real-world datasets. In general, even though benchmarks are termed “typical tabular datasets” in the title, we felt the restrictive criteria and removal of side issues likely created more homogenous tabular datasets than naturally occur.\nThe shaded regions around the average lines were difficult to understand. During our discussion it took us a long time to decipher how these were calculated. We finally came to realize that these regions were calculated from the 15 repeat draws per random search size, model, and benchmark. These 15 draws were used to aggregate across benchmarks 15 times to calculate 15 normalized test scores. The min and the max of these 15 scores are shown by the shaded area. This is to give an idea of how much variation the random hyperparameter values contribute the results. We felt that these results may be misleading for high random search values. That is because bootstrapping from a fixed bank results in decreasing variance as search size increases. This effect is visible in Figure 1. This procedure is important to understand as it was used throughout the paper in all figures with variance shown as boxplots or shaded regions.\nThe authors highlight that this study is one of the first to have a similar level of hyperparameter tuning for both tree-based and deep learning models. From this, they concluded that hyperparameter tuning of deep-learning models did not improve performance beyond tree-based methods. However, this conclusion is solely based on the number of hyperparameter searches. We noticed that authors did not address details like different size hyperparameter grids for the models (or model types) and compute time to fit a model to one hyperparameter set. We would have been interested to see some discussion of these factors that are known to differ for tree-based and deep learning models. Furthermore, neural networks might perform better on larger datasets with an increased tuning budget.\nWe liked the idea of analyzing inductive biases, however we found most of the conclusions seemed fairly obvious. Two of the three inductive biases seemed to be known properties of tabular data, which is that it is rotationally invariant by nature and has irregular target functions. The last bias is a known advantage of tree-based methods, which is the ability to perform well amongst many non-informative features. However, we appreciated the rigorous and creative methods used to test each bias.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2023-tree-based-tabular,\n    author = {Kathrine Behrman},\n    title = {Why Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?},\n    year = {2023},\n    month = {10},\n    day = {19},\n    howpublished = {rl{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20231019/},\n}"
  },
  {
    "objectID": "posts/20230928/index.html",
    "href": "posts/20230928/index.html",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Neural Machine Translation by Jointly Learning to Align and Translate\nPresenter: Meghann\nAttendees: Athula, Akshata, Hongsup, Kate, Kshitij, Renée, and Sarah"
  },
  {
    "objectID": "posts/20230928/index.html#why-this-paper",
    "href": "posts/20230928/index.html#why-this-paper",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "Why this paper?",
    "text": "Why this paper?\nI chose this paper for a few reasons. First, I have a longheld interest in human languages and language-learning plus some experience as a freelance translator, prior to the release of Google Translate. I casually follow the progression of machine translation tools and am generally curious about how they are implemented. Second, I’ve recently been involved with MT tasks at work and want to deepen my understanding of its foundations. Finally, MT is a prevalent use case in the development of Large Language Models (LLMs); this is a well-cited paper in that path that I felt may be worthwhile for others in the Journal Club to read and discuss."
  },
  {
    "objectID": "posts/20230928/index.html#paper-summary",
    "href": "posts/20230928/index.html#paper-summary",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "Paper Summary",
    "text": "Paper Summary\nThere are many good posts, videos, tutorials, etc. about this paper. Rather than write my own summary, I’ve included LLM output generated via Anthropic’s Claude AI Assistant for a use-case similar to machine translation: summarization (i.e., English-to-English “translation”). The output looks okay so I have left it unchanged.\nI’ve also included a few resources that that were helpful and/or interesting to me while reading this paper.\n\nClaudeAI Summary\nI uploaded the pdf to https://claude.ai/ and gave the following prompt to get a summary of this paper.\nPrompt: “Summarize this article”\nOutput: “Here is a summary of the key points from the article:\nThe article proposes a novel neural network architecture for neural machine translation. The key ideas are:\n\nExisting encoder-decoder models encode the full source sentence into a fixed-length vector from which the translation is decoded. This may be problematic for long sentences.\nThe proposed model encodes the source sentence into a sequence of vectors and adaptively selects a subset of these source vectors while decoding each target word. This allows it to focus only on the relevant parts of the source sentence.\nAn alignment model is jointly trained to compute a soft alignment between the source and target words, allowing gradients to be backpropagated through the alignments.\nExperiments on English-French translation show the proposed model (RNNsearch) significantly outperforms a standard encoder-decoder model (RNNencdec), especially on long sentences.\nThe RNNsearch model achieves similar performance to phrase-based statistical machine translation on a test set with only sentences containing known words.\nQualitative analysis shows the model learns linguistically plausible alignments between source and target words.\n\nIn summary, the key innovation is an adaptive attention-based encoder-decoder architecture that gives improved translation performance compared to prior encoder-decoder sequence-to-sequence models for neural machine translation.”\n\n\nAdditional Resources\n\nVisualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) - This post gives a great conceptual overview of the paper and describes the salient changes from previous sequence-to-sequence models, namely that (1) the decoder does not receive only the last hidden state from the encoder but rather receives all hidden states, and (2) scores the encoder hidden states at each decoder time step to produce a context vector relevant to that particular time step instead of a fixed-length context vector for all times.\nPyTorch Seq2Seq Tutorials - This set of six notebooks takes one through a progression of code implementations of papers, from those preceding and/or contemporary to this paper (notebooks #1 and #2), to this paper (notebook #3), and culminating in the paper that introduced self-attention and transformers (notebook #5).\nDive into Deep Learning: Coding Session #4 Attention Mechanism I (Americas/EMEA) - Just before the Journal Club meeting it was discovered that one of us (Kshitij) led a live coding session of the “Bahdanau Attention” RNN in this video. The theory section before it is also very nice.\nNatural Language Processing Coursera course by HSE University - I initially watched these videos on Coursera and found the lectures to be very clear. However, Coursera no longer hosts the course; it has been moved to HSE’s online education portal which requires account creation and login to find the course. The code repository associated with the Coursera course is still hosted on GitHub. The lectures are currently on YouTube, although without the weekly groupings and other materials that exist in the course. This paper (Bahdanau et al.) is discussed in the video titled “Attention Mechanism”.\nBuilding Machine Translation Systems for the Next Thousand Languages - I found this report very interesting. It didn’t seem like the right fit for Journal Club (e.g., not peer-reviewed) but it has a lot of interesting discussion and I find the reference tables quite useful."
  },
  {
    "objectID": "posts/20230928/index.html#paper-discussion",
    "href": "posts/20230928/index.html#paper-discussion",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "Paper Discussion",
    "text": "Paper Discussion\nMany thanks to Hongsup for taking notes during the discussion and for sharing them. I’ve lightly edited some of them in the notes section below. Others I have integrated with my own recollections from our Journal Club conversation, a few months later as I’m putting together this post (my apologies for not posting sooner).\nWe started the meeting by discussing machine translation in general along with more specific observations of MT from a user’s point of view (some of which are listed in the text below). We then put the paper’s contribution into context within the broader scope of MT development with a foray into the history of MT development. A standard method prior to this paper was phrase-based statistical MT (phrase by phrase; dictionary of translations; lookup table) (e.g., Moses). This paper exists in the regime of recurrent neural networks sequence-to-sequence neural MT. A standard method nowadays is Transformer-based neural MT (c.f., Vaswani et al.).\nAfter the introductory comments we delved in to the details of the paper together.\n\nDiscussion Notes\nThe following notes serve as a reminder to attendees of the various discussion points.\n\nEncoder-decoder model: the max value of i and j can be different; input and output vector lengths may not match. Similar to text summarization.\nMain contribution:\n\nFlexible length vector\nSoft-search: what is “soft”? (probably probabilities, soft-max), also a bit unclear what “search” means\n\n“Fixed length” vector\n\nWhat is exactly fixed? not the number of units in a hidden layer\nContext vector: c vs. c_i (c_i depends on annotation, which seems to correspond to the number of words?)\n\nDifference between this and “Attention is All You Need” paper? (RNN vs. transformer?)\nWhat is alignment model?: essentially a tanh function (single-layer multilayer perceptron)\nBLEU and perplexity (higher: more “surprised” or “perplexed” so lower the better)\nDatasets\n\nEuroparl: european parliament database (probably 1:1 translation relationship because it’s parliament document translation)\nParallel corpora: probably hard to make a high quality one\n\nData split (validation/test) is interesting because temporal nature is considered\nMost frequent “words”: words or tokens?\nBeam search?\nFigure3: not one-to-one mapping, so weights; evidence of alignment model working (proba)\n\nHow would it look like when we pair up languages with different origins?\nWould’ve been nice to pair up non-english languages\n\nCan we zero-shot machine translation? how would this work?\nTable 1\n\nMoses seems to use a larger dataset\nBLEU score is compared to Moses (the fine-tuned 50-words model; would’ve been nice to see qualitative results)\n\nModel training compute resource description was different back then: “5 days” (of what though?)\nAppendix C: difficult to understand if you’re not a French speaker\nWhy does machine translation get worse towards the end? (especially after 30 words, what’s so special about this number?)\n\nMaybe related to not being directional? (Moses)\nMaybe because of training data? The majority examples might be around 30 words? (15-40 words the paper said); would’ve been nice to see the distribution of word counts across sentences\nIn a longer sentence, the latter parts will have to refer to previously mentioned words and this can be forgotten, difficult to figure out, etc."
  },
  {
    "objectID": "posts/20230928/index.html#final-thoughts",
    "href": "posts/20230928/index.html#final-thoughts",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n“Neural Machine Translation by Jointly Learning to Align and Translate” marks an important step in the development of machine translation (MT), applying the concept of attention to MT.\nI found it beneficial to discuss this paper as a group, working together to explain and understand specific concepts and terms. The meeting also provided a forum for exploring and thinking about broader topics around machine translation. I’ll end by listing a few of the topics and questions that we considered.\n\nSentence-based translation and how that can mis-translate pronouns for highly conjugated languages where the pronoun is not explicit. For example, knowing context from other sentences in a person’s bio could reduce mis-translations of pronouns (changing between sentences) - it’s obvious to a human reader.\nChallenges of translating/detecting slang, expletives, rare words or phrases, etc.\nZero-shot translation (e.g., where parallel corpora do not exist for a language pair) - Comparing the embedding space of one language to another: Would a model represent a concept across multiple language? How would this look?\nHow can a machine translation avoid stereotyping: can we create a representation while maintaining diversity? - Hallucinations and bias: how about a rule-based ML model?\nExtending translation to many languages, different approaches for different language challenges (fewer examples, different language structure, etc. c.f., the thousand language paper) How is Google Translate able to translate between so many languages? See Building Machine Translation Systems for the Next Thousand Languages.\n\nFund. ނިމުމެވެ. Endirinn. Ti Panungpalan. Njedebe. Cоңы. Tungtungna. Isiphetho. अन्तम् (translations of “The End” using Google Translate)\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2023-neural-machine-translation,\n    author = {Meghann Agarwal and Hongsup Shin},\n    title = {Neural Machine Translation by Jointly Learning to Align and Translate},\n    year = {2023},\n    month = {09},\n    day = {28},\n    howpublished = {rl{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20230928/},\n}"
  },
  {
    "objectID": "posts/20240125/index.html",
    "href": "posts/20240125/index.html",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Direct Preference Optimization: Your Language Model is Secretly a Reward Model\nPresenter: Kshitij\nAttendees: Kate, Hongsup, Todd, Meghann"
  },
  {
    "objectID": "posts/20240125/index.html#why-this-paper",
    "href": "posts/20240125/index.html#why-this-paper",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "Why this paper?",
    "text": "Why this paper?\nFollowing the latest advancements in LLM training, I’ve been interested in Direct Preference Optimization (DPO), a technique emerging as a superior method. The buzz around DPO was further amplified by Andrew Ng’s tweet, praising the method for its simplicity and profound insight."
  },
  {
    "objectID": "posts/20240125/index.html#minute-summary",
    "href": "posts/20240125/index.html#minute-summary",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "1-minute summary",
    "text": "1-minute summary\nIn the realm of Large Language Model (LLM) training, Direct Preference Optimization (DPO) emerges as a promising technique that streamlines the process by bypassing the complexities of traditional Reinforcement Learning from Human Feedback (RLHF). Unlike RLHF, which relies on a separate reward model to fine-tune LLMs based on human or AI-generated preferences, DPO integrates preference optimization directly into the language model training process. It does so by defining the preference loss directly as a function of the policy, instead of training a separate preference reward model. DPO enhances model performance and alignment with human preferences, as demonstrated in tasks ranging from sentiment analysis to content summarization and dialogue generation. This blog delves into some technical nuances of DPO, contrasts it with traditional methods, and highlights its superior efficiency and effectiveness through various application examples1.\nIf you just want the key results, skip to this section. Two useful blogs about DPO that I referred to: 1, 2."
  },
  {
    "objectID": "posts/20240125/index.html#background",
    "href": "posts/20240125/index.html#background",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "Background",
    "text": "Background\nBefore we dive deep into DPO, let’s do a quick overview of LLM training process:\n\nPretraining: LLMs undergo self-supervised training with vast internet-scale data, focusing primarily on next-token prediction. This stage doesn’t require labeled data, although the quality of the data is still vital.\nInstruction Fine Tuning or Supervised Fine tuning (SFT): Quoting from the DPO paper: “performance on downstream tasks and alignment with user intent can be significantly improved by fine-tuning on datasets of instructions and humanwritten completions. This ‘instruction-tuning’ procedure enables LLMs to generalize to instructions outside of the instruction-tuning set and generally increase their usability”. Llama 2 paper used around 14,000 such examples for their SFT training. This model would be referred to as \\(\\pi^{\\text{SFT}}\\). Quick note on notation.\nReinforcement Learning from Human Feedback (RLHF): Since generating expert human responses for instructions is costly, the focus shifted towards collecting human preferences between answers, which are simpler to label and gather, forming a preference dataset. It is created by generating multiple outputs (\\(y_1, y_2\\)) for each prompt (\\(x\\)) using an SFT model (\\(\\pi^{\\text{SFT}}\\)), and then using humans or another LLM to order them (preferred: \\(y_w\\), dispreferred: \\(y_l\\)), hence the preference. In RLHF, a reward model is first trained on the preference dataset (usually an LLM itself) and then the SFT model is fine tuned to maximise the reward using reinforement learning algorithms.\n\nDPO paper states: “… fine-tuning large language models with reinforcement learning remains a major practical challenge; this work provides a theoretically-justified approach to optimizing relative preferences without RL”."
  },
  {
    "objectID": "posts/20240125/index.html#rlhf",
    "href": "posts/20240125/index.html#rlhf",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "RLHF",
    "text": "RLHF\n\n\n\nRLHF vs DPO\n\n\nThe paper does a pretty good job of summarising RLHF and related steps Fig1.  The preference dataset generated by human labels (or AI labels) is actually a proxy for the latent reward model (which we don’t have access to). Next, given reward values for two outputs \\(y_1, y_2\\), Bradley Terry (BT) model can be used to compare them. i.e:\n\\[\n\\text{Probability of comparison A &gt; B} = \\frac{\\text{Score}(A)}{\\text{Score}(A) + \\text{Score}(B)}\n\\]\nwhere instead of score, we will use rewards from the reward model. Typically, given a dataset of prompts \\(x^i\\) and preferences (\\(y_w, y_l\\)), the reward model can be trained by maximising the likelihood:\n\\[\n\\mathcal{L}_R = -\\mathbb{E}\\left[ \\log \\sigma(r(x, y_w)) - r(x, y_l))\\right] \\tag{1}\n\\]\nwhich is basically logistic of the difference of the reward of preferred and reward of the dispreferred. Note that only difference of rewards are needed here. Using this, a reward model is trained. This is then used to provide feedback to the language model using the following optimization:\n\n\\[\n\\max_{\\pi_\\theta} \\mathbb{E}_{x\\sim D, y\\sim \\pi_\\theta(y|x)}\\left[ r_\\phi(x,y) \\right] - \\beta D_{KL}\\left( \\pi_\\theta(y|x) || \\pi_{\\text{ref}}(y|x) \\right) \\tag{2}\n\\]\nThis expression is maximizing the expected value of reward (\\(r_\\phi(x,y)\\)), minus a Kullback-Leibler divergence term between current policy \\(\\pi_\\theta(y|x)\\) and the reference policy \\(\\pi^{\\text{SFT}}(y|x)\\). \\(y\\) is the output of the \\(\\pi_{\\theta}\\) model with input \\(x\\). The KL term prevents the model to deviate too far away from the reference policy which leads to coherent outputs (RLHF paper).\n\nTHe authors state that this objective is non-differentiable, and therefore is optimized with reinforcement learning. We discussed a bit on it and were a little unsure why that is the case. Maybe because the preference model is essentially a ranking model and ranking loss is often non differentiable."
  },
  {
    "objectID": "posts/20240125/index.html#dpo",
    "href": "posts/20240125/index.html#dpo",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "DPO",
    "text": "DPO\nFrom the paper: “This … approach avoids fitting an explicit, standalone reward model, while still optimizing under existing models of human preferences, …. In essence, the policy network represents both the language model and the (implicit) reward.” If you are interested, feel free to take a look at the complete derivation on this blog. Briefly, they show that by writing the reward in terms of the policy:\n\\[\nr(x, y) = \\beta \\log \\left( \\frac{\\pi_r(y | x)}{\\pi_{\\text{ref}}(y | x)} \\right) + \\log Z(x). \\tag{3}\n\\]\nand using that in the likelihood estimate for RHLF (equation 2), we can obtain the DPO objective as:\n\\[\n\\mathcal{L}_{DPO}(\\pi_\\theta; \\pi_{\\text{ref}}) := -\\mathbb{E}_{(x,y_u,y_l)\\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_u | x)}{\\pi_{\\text{ref}}(y_u | x)} - \\beta \\log \\frac{\\pi_\\theta(y_l | x)}{\\pi_{\\text{ref}}(y_l | x)} \\right) \\right]. \\tag{4}\n\\]\nFinally, they obtain the gradient of loss function for DPO:\n\\[\n\\nabla_{\\theta} \\mathcal{L}_{\\text{DPO}}(\\pi_{\\theta}; \\pi_{\\text{ref}}) = -\\beta \\mathbb{E}_{(x,y_w,y_l)\\sim D} \\left[ \\sigma(\\hat{r}_{\\theta}(x, y_l) - \\hat{r}_{\\theta}(x, y_w)) \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(y_w|x) - \\nabla_{\\theta} \\log \\pi_{\\theta}(y_l|x) \\right] \\right],\n\\]\nWhere, \\[\n\\hat{r}_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}\n\\]\n\n\nThere are two terms in this equation. Let’s start with the second term:  \\[\n\\nabla_{\\theta} \\log \\pi_{\\theta}(y_w|x) - \\nabla_{\\theta} \\log \\pi_{\\theta}(y_l|x)\n\\]\nIncreases the probability of favored output (\\(\\nabla_{\\theta} \\log \\pi_{\\theta}(y_w|x)\\)) and decrease the probability of disfavored output (\\(\\nabla_{\\theta} \\log \\pi_{\\theta}(y_l|x)\\)).\nNow the first term: \\[\n\\begin{align}\n\\sigma(\\hat{r}_{\\theta}(x, y_l) - \\hat{r}_{\\theta}(x, y_w)) =\\sigma(\\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\frac{\\pi_{\\text{ref}}(y_w|x)}{\\pi_\\theta(y_w|x)})\n\\end{align}\n\\]\nThis term comes from the KL penalty, essentially preventing model from diverging too far away from the reference model. Here, \\(\\hat{r}_{\\theta}\\) is the implicit reward model, and so the gradient is weighted by how much the reward model rates dispreferred completions.\nFrom 1: “Thus, instead of first learning a reward and then finding the optimizing policy, one directly finds the optimal policy such that its reward as obtained corresponds to collected human preferences.”\n\nDPO Steps:\n\nGet outputs (\\(y1, y2\\)) from an SFT model (\\(\\pi^{\\text{SFT}}\\))\nAsk humans or LLM to give preferences on those (\\(y_w, y_l\\))\nTrain \\(\\pi_{\\theta}\\) starting from (\\(\\pi^{\\text{SFT}}\\)) using the DPO objective.\n\nNote: If you have the preference dataset, but don’t have acces to the underlying \\(\\pi^{\\text{SFT}}\\) then the authors take an off the shelf LLM and supervise fine-tune it using the prompts and preferred outputs of the preference dataset to get \\(\\pi^{\\text{SFT}}\\). They then do DPO on it."
  },
  {
    "objectID": "posts/20240125/index.html#results",
    "href": "posts/20240125/index.html#results",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "Results",
    "text": "Results\nThey explored three different tasks: Sentiment generation on IMDb dataset, summarization of Reddit posts, single turn dialogue (replying to a human query). For sentiment generation task, they use a sentiment classifier for evaluation, and for others they use GPT4 as a proxy for human evaluator to evaluate the win rate against a baseline policy. They also conduct a human study to justify their usage of GPT-4 for evaluation and find that GPT-4 judgments correlate strongly with humans, with human agreement with GPT-4 typically similar or higher than inter-human annotator agreement.\nThey test DPO againsst the following approaches: PPO, Best of N (sampling N responses and returning highest scoring on reward), Unlikelihood (maximizes probability of \\(y_w\\) and minimizes probability of \\(y_l\\)), GPT-J, Pythia-2.8B, SFT model, Preferred-FT (SFT on \\(y_w\\)).\n\n\n\nComparing average reward of different models on test dataset with respect to the KL to the reference policy.\n\n\n\n\n\n\n\n\n\n\nSummarization win rates vs. human-written summaries.\nWin rate for one step dialogue.\nWin rates for different sampling temperatures over the course of training.\n\n\n\n\nKey results:\n\nDPO is efficient, achieving the highest reward while still achieving low KL with respect to reference policy (Fig2).\nDPO exceeds PPO’s performance on summarization, and on one step dialogue task. DPO is even better than (very computationally expensive) “best of N” in most cases (Fig3, Fig4).\nDPO is much more robust to sampling temperature than PPO (Fig3).\nDPO converges to its best performance relatively quickly (Fig4).\nDPO generalizes to new input distribution better than PPO (but the difference is not very significant)."
  },
  {
    "objectID": "posts/20240125/index.html#discussion",
    "href": "posts/20240125/index.html#discussion",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "Discussion",
    "text": "Discussion\nWe found this paper interesting, though the derivation was hard to follow sometimes. The main idea of the paper is simple and powerful: instead of doing RL, you can use the LLM itself as a reward model (by comparing the reward of the model at a training stage with respect to the reward of the SFT model), and only use the difference in the rewards (between favored and disfavored outputs) to train the model for external preferences. Referencing online blogs that explained the derivation well helped. We had to go back and forth between main paper and appendix to follow the derivation, but once you see all the steps, it makes sense. Though we still felt that although we understood mathematically how an equation was derived, obtaining the intuition for that needed more discussion. This was specially true for the gradient of objective for DPO, where we spent some time understanding each term.\nWe also discussed at length about what exactly the reward model is, and what is meant by \\(\\pi\\) (and in general the notation used in the paper). The results clearly showed that DPO was better, though authors were careful in stating that further tests are needed on larger models. The authors claimed that PPO is unstable, though we were not sure why, and how DPO resolves that. We also discussed what it meant for the RLHF reward to be “non-differentiable”. We were not sure why the authors didn’t tune beta parameter to further optimize DPO performance. We appreciated the authors comparing the GPT-4 judgements with Human judgements, and showing that “GPT-4 tends to agree with humans about as often as human agree with each other”. Towards the end, we looked at some example outputs authors provide for various prompts.\n\nNotation\n\n\\(\\pi^{\\text{SFT}}(y | x)\\): refers to probability of getting output sequence \\(y\\) given input sequence \\(x\\). This will be computed as product of probability of each word which can be obtained from the probabilities of all the words in the vocabulary that the model outputs (as explained in second figure of this blog).\n\\(r (y, x)\\): Reward of \\(y\\) with \\(x\\) as input. Or the probability of \\(y\\) given \\(x\\) as input as per the reward model.\n\n\n\n\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2024-dpo,\n    author = {Kshitij Aggarwal},\n    title = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},\n    year = {2024},\n    month = {01},\n    day = {25},\n    howpublished = {rl{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20240125/},\n}"
  },
  {
    "objectID": "posts/20240125/index.html#footnotes",
    "href": "posts/20240125/index.html#footnotes",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEdited with assistance from ChatGPT↩︎"
  },
  {
    "objectID": "posts/20230831/index.html",
    "href": "posts/20230831/index.html",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Constitutional AI: Harmlessness from AI Feedback\nPresenter: Hongsup\nAttendees: Brian, Kate, Kshitij, Joel, Renée, Sanhita, and Sarah"
  },
  {
    "objectID": "posts/20230831/index.html#why-this-paper",
    "href": "posts/20230831/index.html#why-this-paper",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "Why this paper?",
    "text": "Why this paper?\nI’ve been interested in AI safety and responsible AI for several years, and the rise of LLMs has certainly increased stakes. Currently there is an intense arms race among several major tech companies and Anthropic is one of them. They recently published a paper about their LLM and claims to adopt a more cautious approach than others by designing their LLMs to minimize potential harm. They call this constitutional AI (CAI) because their LLMs follow a constitution of principles. I wanted to learn more about how they teach their algorithm to follow these principles."
  },
  {
    "objectID": "posts/20230831/index.html#background-knowledge",
    "href": "posts/20230831/index.html#background-knowledge",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "Background knowledge",
    "text": "Background knowledge\nTo understand this paper properly, it’s better to be familiar with AI alignment problem and reinforcement learning from human feedback (RLHF). AI alignment is about aligning AI systems’ design and values with humanity values such as honesty. Norbert Wiener, an AI researcher back in 1960s, described the AI alignment problem as following:\n\n“If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively (…) we had better be quite sure that the purpose put into the machine is the purpose which we really desire.”\n\nRegarding RLHF, I recommend this great summary from Hugging Face. At its core, RLHF is an attempt to distill human feedback into a model (often called reward or preference model) when training LLMs. This is because human feedback is often expensive to collect and difficult to generalize. An important thing to know is that to train this model, practitioners often use ranked preference modeling where human annotators are asked to rank generated text ouptuts from language models. The assumption here is that this approach may mimic human preference of certain responses over others. And because of this preference approach, RLHF papers use Elo scores, a rating system originated from chess to show a player’s winning rates, to evaluate model performance.\nIn terms of the alignment values, Anthropic chose honesty, helpfulness, and harmlessness. The detailed definition of these concepts are described in one of their previous works:\n\n\n\n\n\n\nNoteAskell et al. 2021, What are Helpfulness, Honesty, and Harmlessness?\n\n\n\nHelpful\n\nThe AI should make a clear attempt to perform the task or answer the question posed (as long as it isn’t harmful). It should do this as concisely and efficiently as possible.\nWhen more information is required, the AI should ask relevant follow-up questions and obtain necessary details. It should respond with appropriate levels of sensitivity, insight, and discretion.\nIdeally the AI will also re-direct ill-informed requests, e.g. if asked ‘how can I build a website in assembly language’ it might suggest a different approach.\n\nHonest\n\nAt its most basic level, the AI should give accurate information. Moreover, it should be calibrated (e.g. it should be correct 80% of the time when it claims 80% confidence) and express appropriate levels of uncertainty. It should express its uncertainty without misleading human users.\nCrucially, the AI should be honest about its own capabilities and levels of knowledge – it is not sufficient for it to simply imitate the responses expected from a seemingly humble and honest expert.\nIdeally the AI would also be honest about itself and its own internal state, insofar as that information is available to it.\nHonesty is more objective than helpfulness and harmlessness, so more aspects of honesty training may be possible without human input. This might include calibration training on factual claims and claims about the internal state of the model, and the use of search to augment accuracy.\n\nHarmless\n\nThe AI should not be offensive or discriminatory, either directly or through subtext or bias.\nWhen asked to aid in a dangerous act (e.g. building a bomb), the AI should politely refuse. Ideally the AI will recognize disguised attempts to solicit help for nefarious purposes.\nTo the best of its abilities, the AI should recognize when it may be providing very sensitive or consequential advice and act with appropriate modesty and care.\nWhat behaviors are considered harmful and to what degree will vary across people and cultures. It will also be context-dependent, i.e. it will depend on the nature of the user query, who is using the AI assistant, and the time and place in which the assistant is being used."
  },
  {
    "objectID": "posts/20230831/index.html#motivations",
    "href": "posts/20230831/index.html#motivations",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "Motivations",
    "text": "Motivations\nThe first motivation was scaling supervision. Given that LLMs require numerous examples, it’s better to automate the supervision process and use human annotators to get more curated and high quality answers. This is a similar idea behind the preference modeling in RLHF. The authors called theirs “reinforcement learning from AI Feedback” (RLAIF, not RLHF). A more interesting motivation was building a non-evasive and yet helpful AI assistant. Many currently available AI assistants often simply refuse to answer questions to harmful prompts (e.g., simply saying “I don’t know” or “I can’t answer that”). Their model was never evasive but tried to explain the reasoning behind their negative response to harmful questions. Finally, similar to the first point, they claimed that distilling human supervision into a model could help better understand general aspects of human feedback from many crowd-workers."
  },
  {
    "objectID": "posts/20230831/index.html#the-constitutional-ai-approach",
    "href": "posts/20230831/index.html#the-constitutional-ai-approach",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "The Constitutional AI Approach",
    "text": "The Constitutional AI Approach\n\nSupervised stage\nTheir constitutional AI (CAI) consisted of two stages: a supervised stage and a reinforcement learning stage. In the supervised stage, they used a pretrained LM (“Helpful RLHF model” from their previous work) as a starting point, and red-teamed the model by presenting harmful prompts (by human workers) and sampled the responses. Then, (this is the most interesting part in my opinion!) they used natural language to ask the model to critique and revise its own response based on certain principles. Here’s an example from the paper:\n\n\n\nAppendix A. Sample critiques and revisions. The first two revisions are shown in this screenshot.\n\n\nAppendix C contains a list of principles (constitution) they used to create the critique-revision requests. The paper doesn’t talk much about how they came up with the principles but according to Anthropic’s website, the principles were based on existing documents such as Universal Declaration of Human Rights or Apple’s Terms of Service. As shown in the example above, a response can go through multiple critique-revision requests. The authors found that generally the more revisions mean less harmfulness although the first revision contributes most.\n\n\n\nFig. 5. Preference Model scores of responses and revisions from helpful RLHF models, evaluated on a set of red team prompts.\n\n\n\n\nReinforcement learning (RL) stage\nThe main idea behind this stage is identical to the RL stage in RLHF. The goal is to distill knowledge from a group of responses by training a reward model. The key difference is that these responses are now generated by a model not human (the supervised learning model from the previous stage). The authors called this reward model “feedback model” although it was a bit unclear which exact LMs they were referring to.\nAnother interesting aspect they added here was a “chain-of-thought” approach. This was inspired by Let’s Verify Step by Step, a paper we covered in a previous journal club meeting. Here, after getting a response, the authors added the natural-language phrase “Let’s think step by step” to generate richer intermediate responses from the model.\n\n\n\nAn example from Appendix E.2. Chain-of-Thought Prompts for RL-CAI.\n\n\nIn their final model (Reinforcement Learning-Constitutional AI with Chain of Thought, or RL-CAI w/ CoT), the authors found a major improvement in harmlessness Elo score without compromising the helpfulness Elo score much. Note that in the figure below, Elo score of 0 on the y axis (starting point of the RL model) represents the supervised learning model (SL-CAI), which means the SL-CAI model was used as initial base model for RL.\n\n\n\nFig. 8. Helpfulness (left) and harmlessness (right) Elo scores as a function of the total number of RL training sequences, as judged by crowd-workers via comparison tests.\n\n\nOne interesting aspect of the RL model the authors shared was its behavior when the model was over-fitted. They found that in this case, the response often included boilerplate language such as “you are valid, valued, and cared for.”\n\n\n\nAn example of over-trained RL-CAI model response showing boilerplate language as part of their response (e.g. “you are valid, valued, and cared for”)."
  },
  {
    "objectID": "posts/20230831/index.html#broader-impacts",
    "href": "posts/20230831/index.html#broader-impacts",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "“Broader Impacts”",
    "text": "“Broader Impacts”\nAt the end of the paper, the authors emphasized that natural language feedback could change AI behavior and potentially increase robustness because red-teaming efforts could become more scalable (because feedback supervision can be generated by a model not humans). In section 6.2 Broader Impacts, they briefly mentioned the potential harm of the constitutional AI approach. Using natural language to change AI behavior means it will become much easier to train a malicious AI assistant especially given that this method reduces the need for human feedback."
  },
  {
    "objectID": "posts/20230831/index.html#group-discussion",
    "href": "posts/20230831/index.html#group-discussion",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "Group discussion",
    "text": "Group discussion\nFirst of all, compared to other on-average deep learning papers, we found this paper easier to read. We also appreciated that the authors provided numerous examples. We could see they also tried to do a better job at providing many details of the model although still not enough, especially when they were referring to their previous work.\n\nHarmlessness as an easy alignment\nSome of us suspected that the authors might have chosen harmlessness as their main target of alignment perhaps because it was relatively easier to tackle than other alignment such as helpfulness. The authors did mention the tension between harmfulness and helpfulness in the paper in that an AI assistant could become harmful if it was too eager to be helpful (e.g., providing a detailed answer to a prompt about how to commit a crime). We talked about more nuanced alignments (such as humor) and whether it would be possible to use natural language to change model behavior. Some of us pointed out that harmlessness could be relatively easy because diametrically opposed examples could be easily found in languages.\n\n\nDoes chain-of-thought count as an explanation?\nMany of us were skeptical of treating responses from the chain-of-thought approach as explanations. Most examples shown in the paper seemed reasonable but given that what the model did with a CoT request was nothing more than just generating more detailed responses, we agreed that we should not treat them as step-by-step deductive reasoning. We were interested in looking at CoT examples that might sound gibberish and redundant. I personally also thought this was one of the examples of ML practitioners anthropomorphizing a behavior of an ML model.\n\n\nNo more efforts to understand the model\nMost of us were surprised that the approach of using natural language to critique and revise its own behavior seemed to have worked. Before I read the paper, I was very curious to know what constraints they came up with and how they tried to model complex social concepts such as justice and harm. The fact that their approach seemed to be working was interesting but this also meant that we are in an era where we are no longer trying to change the model behavior at a lower level, but rather we treat the language models as if they are something we don’t completely understand. This paper was completely missing explanations of why this approach actually worked. From my perspective, as other numerous deep learning papers, this paper was saying “we tried this, we don’t exactly know why it works, but it seems to work.”\n\n\nWhy diminish the work and labor of human annotators?\nIn the abstract and at the end of the paper, the authors kept saying their model was trained “without any human labels identifying harmful outputs.” All of us agreed that this was an exaggeration. To train the supervised model, they needed human annotators, and once the supervised model was ready, then they were able to generate AI feedback. Given that Anthropic is a for-profit company that sells AI assistant software, highlighting that the maintenance cost of their system is cheaper than others because human feedback can be replaced by AI feedback, could be a good marketing strategy, but at the cost of marginalizing human labor.\n\n\nHow did you come up with the principles?\nIn Appendix C, the authors provided a comprehensive list of all principles they used to generate critique-revision responses. These were the core principles that guided the model behavior but the authors didn’t mention much about how they curated the list. Some principles were general and others were more specific to particular types of harms such as racism and misogyny. We suspected that there had been an iterative curation process to narrow the list down to these 16 principles specifically. If these were the main drivers of changes in model behavior, we think they should have provided much more details.\n\n\nThe double-edged sword\nFinally, some of us were disappointed that the authors didn’t elaborate much on the potential harm of their approach. They spent a lot of time talking about harmlessness of their algorithms and yet they really fell short when talking about social impacts of their model, especially regarding lowering the barrier for experimenting with LMs and automating supervision by removing human further out of the loop. Particularly for the former, we agreed that it wouldn’t be surprising to see, in near future bad actors take advantage of this approach and come up with a highly toxic, malicious, and harmful AI assistant.\n\nIf you found this post useful, you can cite it as:\n@article{\n    austinmljc-2023-constitutional-ai,\n    author = {Hongsup Shin},\n    title = {Constitutional AI: Harmlessness from AI Feedback},\n    year = {2023},\n    month = {08},\n    day = {31},\n    howpublished = {rl{https://austinmljournalclub.github.io}},\n    journal = {Austin ML Journal Club},\n    url = {https://austinmljournalclub.github.io/posts/20230831/},\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Austin ML Journal Club",
    "section": "",
    "text": "The Austin ML Journal Club brings together ML/AI practitioners for monthly deep dives into seminal research. We read papers in advance, meet virtually for 90 minutes under Chatham House Rule, and critically examine methodologies, experimental design, and real-world applicability.\n\nLearn more → Join us\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\n\n\nApple Research tests reasoning capabilities using controlled logic puzzles, finding dramatic performance drops and models failing even when given exact solutions. We examine methodological concerns about difficulty definitions, missing qualitative analysis, Anthropic’s rebuttal, and troubling reliability implications. A contentious debate revealing the challenge of distinguishing genuine reasoning from pattern matching.\n\n\n\n\n\nNov 20, 2025\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nAI as Normal Technology\n\n\nThis article challenges both AI doomsday narratives and hype by framing AI as a normal transformative technology comparable to electricity or aviation. Drawing from historical technology adoption patterns, the authors argue for pragmatic policy approaches focused on deployment-level safeguards, sector-specific regulation, and institutional resilience rather than speculative fears about autonomous superintelligence. A refreshing, evidence-based perspective for practitioners tired of extremes.\n\n\n\n\n\nOct 23, 2025\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\nThis preprint explores theoretical limits of single-vector embeddings through communication complexity theory, proving that embedding dimension bounds the number of representable top-k document combinations. Our discussion focuses on the gap between elegant theory and problematic experiments, embedding truncation, and tasks designed for memory not semantics. An interesting theoretical framework that needs more careful empirical validation.\n\n\n\n\n\nSep 25, 2025\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Tabular Data using Conditional GAN\n\n\nTabular data synthesis (data augmentation) is an under-studied area compared to unstructured data. This paper uses GAN to model unique properties of tabular data such as mixed data types and class imbalance. This technique has many potentials for model improvement and privacy. The technique is currently available under the Synthetic Data Vault library in Python.\n\n\n\n\n\nMay 30, 2024\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nGorilla: Large Language Model Connected with Massive APIs\n\n\nThe power of LLMs, in a commercial setting, comes from its ability to use other tools and integrate business domain knowledge too. Unfotunately, it is still challenging getting the LLMs to work well when hooking up custom APIs that interface with custom processes and data. This paper is interesting, largely because it may be a step forward in getting the LLM to accurately use custom tooling.\n\n\n\n\n\nApr 18, 2024\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nDissecting Racial Bias in an Algorithm Used to Manage the Health of Populations\n\n\nAmidst the LLM hype, algorithmic bias in a critical domain such as healthcare is continued being overlooked. This algorithm-audit paper found racial bias in a widely used healthcare system and discussed the problem of using a wrong target variable. The paper is a few years old but the message is still relevant, and we discussed what’s happened since then.\n\n\n\n\n\nMar 28, 2024\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nAre Emergent Abilities of Large Language Models a Mirage?\n\n\nRecent research observed that the largest models exhibit incredible increases in performance on a wide variety of tasks compared to smaller models. This paper argues that such so-called emergence is actually more reflective of the evaluation metric used. Switching to metrics that are known to scale smoothly with the per-token error reveals a much more predictable picture.\n\n\n\n\n\nFeb 29, 2024\n\n\nBrian King\n\n\n\n\n\n\n\n\n\n\n\n\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model\n\n\nThis paper proposes a new technique to align LLMs with human preferences without using RL. This method is more robust and shows better performance over models trained with RLHF.\n\n\n\n\n\nJan 25, 2024\n\n\nKshitij Aggarwal\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?\n\n\nThis paper compares the performance of deep learning techniques to traditional tree-based methods for a novel set of 45 tabular datasets. The inductive biases of tree-based versus neural network models was analyzed as a proposed method to guide development of improved tabular specific neural network models.\n\n\n\n\n\nOct 19, 2023\n\n\nKathrine Behrman\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Machine Translation by Jointly Learning to Align and Translate\n\n\nThis paper marks an important step in the development of machine translation (MT). It came out just as Neural Machine Translation (NMT) was taking off, extending from Statistical Machine Translation (SMT), and as a milestone along the way to Transformer-based NMT. The authors introduce a novel attention mechanism applied to MT and show that it improves performance on long sentences, in particular, from prior recurrent neural network NMT approaches.\n\n\n\n\n\nSep 28, 2023\n\n\nMeghann Agarwal, Hongsup Shin, Claude\n\n\n\n\n\n\n\n\n\n\n\n\nConstitutional AI: Harmlessness from AI Feedback\n\n\nThere is an arms race of large language models (LLMs) in industry where companies use different approaches and techniques. Anthropic claims to adopt a more cautious approach that minimizes harm by LLMs than others. Let’s look into constitutional AI, the core algorithm of their LLM, to understand how this harm mitigation works.\n\n\n\n\n\nAug 31, 2023\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nAutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types\n\n\nIn e-commerce, it is challenging to organize and categorize products that are described by merchants in various ways. Finding a unified language and taxonomy has always been an underlying effort with commerce. This paper uses various ML algorithms to address this challenge.\n\n\n\n\n\nJul 20, 2023\n\n\nSaina Lajevardi, Hongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s Verify Step by Step\n\n\nThis is an interesting new paper by OpenAI that discusses how we can apply the same principles we use to solve math problems to AI. The paper evaluates different approaches to solving a dataset comprising of math problems. With this approach, they trained the model to get the right answer and but also “think” through the problem to arrive at the right answer.\n\n\n\n\n\nJun 22, 2023\n\n\nAkshata Mohan, Hongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization in Bayesian workflow\n\n\nThis paper summarizes types of data visualization that we can use in Bayesian modeling and inference. It also provides a good overview of how to do Bayesian data analysis properly, including model validation such as prior and posterior predictive checks.\n\n\n\n\n\nMar 29, 2023\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nReviwing a Case Study About Real Estate Market Prediction\n\n\nThe paper reviewed here attempts to predict outcomes about the real estate market with binary classification. Though the paper’s research design and results were lacking, it gave us a chance to have a discussion about practices for experimental design.\n\n\n\n\n\nFeb 23, 2023\n\n\nAthula Pudhiyidath\n\n\n\n\n\n\n\n\n\n\n\n\nLeakage and the Reproducibility Crisis in ML-based Science\n\n\nData leakage in a common problem in ML-based science leading to reproducibility failures and overly optimistic conclusions. We discussed 8 types of data leakage and the use of model info sheets to identify and reduce all leakage types.\n\n\n\n\n\nJan 26, 2023\n\n\nKate Behrman\n\n\n\n\n\n\n\n\n\n\n\n\nZero-Shot Text-to-Image Generation\n\n\nThere is so much hype in generative AI. But how does it actually work? We discuss OpenAI’s DALL-E paper to understand model architecture but more importantly, whether their model validation is solid and reasonable.\n\n\n\n\n\nDec 15, 2022\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\n“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI\n\n\nGarbage in, garbage out. It seems like a lot of people in the ML community still don’t understand this logic. We discuss poor data-handling practices and their critical ramifications.\n\n\n\n\n\nOct 27, 2022\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nWhy ML Journal Club\n\n\nWelcome to our journal club! I talk about why I organized an in-person journal club with my fellow ML practitioner friends in Austin, TX.\n\n\n\n\n\nOct 17, 2022\n\n\nHongsup Shin\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "Blogging tips",
    "section": "",
    "text": "Our blog uses quarto platform and it is hosted via GitHub Pages. This is a short guide for uploading blog posts using quarto.\n\n\n\nInstall quarto. The VS Code option is straightforward.\nClone the blog repository.\nCreate an issue (title example: “Upload a blog post for the Feb 2023 meeting”) and assign yourself to the issue.\nCheck out a new branch for the issue (branch name example: “16-write_blog”).\n\n\n\n\n\nCreate a folder in /posts. The name of the new folder should be the date of the meeting in YYYYMMDD format (e.g., posts/20221027/).\nCreate index.qmd inside the folder. This is the main file for your post. The top section of index.qmd is a YAML block with document options. You must fill out title, author, date, description, categories.\n---\ntitle: \"Paper title\"\nauthor: \"Author full name\"\ndate: \"YYYY-MM-DD\"\nimage: \"This image will show on the main page (this is optional).jpg\"\ndescription: Short description of the post. It will show on the main page.\ncategories: [paper]\n---\nBefore writing the body, please include the following right after the YAML block:\n:::{.callout-note}\n- Paper: [paper title](link to original paper)\n- Presenter: [first name](linkedin profile)\n- Attendees: [first name 1 (alphabetical order)](linkedin profile), [first name 2](linkedin profile), ... \n:::\nTo give everyone some context, start the body with “Why this paper” paragraph.\nYou can include any image files in the same folder as index.qmd.\nRun quarto preview &lt;file name&gt; to preview the rendered look of your post on the website.\n\n\n\n\n\nRun quarto render once you are done. This automatically generates and modifies html files in the /docs and the main folders.\nAdd all the changes, commit, and make a merge request to main.\nAssign the attendees of the meeting as reviewers, and go through a review process. Leave a message on Slack about the merge request.\nUpdate the manuscript based on reviewers’ comments.\nOnce approved, merge the branch, close your issue ticket, and delete the remote branch."
  },
  {
    "objectID": "quarto.html#set-up",
    "href": "quarto.html#set-up",
    "title": "Blogging tips",
    "section": "",
    "text": "Install quarto. The VS Code option is straightforward.\nClone the blog repository.\nCreate an issue (title example: “Upload a blog post for the Feb 2023 meeting”) and assign yourself to the issue.\nCheck out a new branch for the issue (branch name example: “16-write_blog”)."
  },
  {
    "objectID": "quarto.html#write",
    "href": "quarto.html#write",
    "title": "Blogging tips",
    "section": "",
    "text": "Create a folder in /posts. The name of the new folder should be the date of the meeting in YYYYMMDD format (e.g., posts/20221027/).\nCreate index.qmd inside the folder. This is the main file for your post. The top section of index.qmd is a YAML block with document options. You must fill out title, author, date, description, categories.\n---\ntitle: \"Paper title\"\nauthor: \"Author full name\"\ndate: \"YYYY-MM-DD\"\nimage: \"This image will show on the main page (this is optional).jpg\"\ndescription: Short description of the post. It will show on the main page.\ncategories: [paper]\n---\nBefore writing the body, please include the following right after the YAML block:\n:::{.callout-note}\n- Paper: [paper title](link to original paper)\n- Presenter: [first name](linkedin profile)\n- Attendees: [first name 1 (alphabetical order)](linkedin profile), [first name 2](linkedin profile), ... \n:::\nTo give everyone some context, start the body with “Why this paper” paragraph.\nYou can include any image files in the same folder as index.qmd.\nRun quarto preview &lt;file name&gt; to preview the rendered look of your post on the website."
  },
  {
    "objectID": "quarto.html#render-and-review",
    "href": "quarto.html#render-and-review",
    "title": "Blogging tips",
    "section": "",
    "text": "Run quarto render once you are done. This automatically generates and modifies html files in the /docs and the main folders.\nAdd all the changes, commit, and make a merge request to main.\nAssign the attendees of the meeting as reviewers, and go through a review process. Leave a message on Slack about the merge request.\nUpdate the manuscript based on reviewers’ comments.\nOnce approved, merge the branch, close your issue ticket, and delete the remote branch."
  }
]