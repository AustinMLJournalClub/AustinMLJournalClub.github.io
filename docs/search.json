[
  {
    "objectID": "reading_list.html",
    "href": "reading_list.html",
    "title": "Reading List",
    "section": "",
    "text": "These are the papers/articles/repos/videos that our journal club members have shared, which might be worth presenting in the future or we think are interesting. This page will be updated once a month.\n\nSolving olympiad geometry without human demonstrations shared by Hongsup\nSleeper Agents: Training Deceptive LLMs that Persist Through Safety Training shared by Kshitij\nTOFU: A Task of Fictitious Unlearning for LLMs shared by Sarah\nQuantifying the impact of uninformative features on the performance of supervised classification and dimensionality reduction algorithms shared by Kshitij\nFine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! shared by Hongsup\nA Mulching Proposal shared by Hongsup\nEvaluating and Mitigating Discrimination in Language Model Decisions shared by Kshitij\nDive into Deep Learning: Coding Session #4 Attention Mechanism I (MLT Artificial Intelligence) shared by Kshitij\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks shared by Brian\nMiniLLM: Large Language Models on Consumer GPUs shared by Hongsup\nThe TinyLlama project shared by Kshitij\nOn the Opportunities and Risks of Foundation Models shared by Hongsup\nChallenges in Deploying Machine Learning: a Survey of Case Studies shared by Hongsup\nMachine Learning and the Future of Bayesian Computation shared by Brian"
  },
  {
    "objectID": "notes/2023-01-26.html",
    "href": "notes/2023-01-26.html",
    "title": "Austin ML Journal Club",
    "section": "",
    "text": "ML-based science: Is prediction important in science? This might depend on the domain (e.g., climate change research might focus on predicting the risk)\nModel info sheet: the intent is good but it seems unfeasible (too much effort and information for the author)\nDefinition of reproducibility: it makes sense to consider the correctness of analysis and scientific integrity as a part of reproducibility\nWhy focus on the civil war papers? Perhaps these are well-studied review papers w/ code and data available"
  },
  {
    "objectID": "notes/2023-01-26.html#overview",
    "href": "notes/2023-01-26.html#overview",
    "title": "Austin ML Journal Club",
    "section": "",
    "text": "ML-based science: Is prediction important in science? This might depend on the domain (e.g., climate change research might focus on predicting the risk)\nModel info sheet: the intent is good but it seems unfeasible (too much effort and information for the author)\nDefinition of reproducibility: it makes sense to consider the correctness of analysis and scientific integrity as a part of reproducibility\nWhy focus on the civil war papers? Perhaps these are well-studied review papers w/ code and data available"
  },
  {
    "objectID": "notes/2023-01-26.html#why-make-distinction-between-science-and-engineering-applications",
    "href": "notes/2023-01-26.html#why-make-distinction-between-science-and-engineering-applications",
    "title": "Austin ML Journal Club",
    "section": "Why make distinction between science and engineering applications?",
    "text": "Why make distinction between science and engineering applications?\n\nIt’s not like that leakage is okay in production or difficult not to notice in engineering application. Data leakage is still an issue.\nPerhaps they meant that industry engineering applications get less constraints/scrutiny about the process. But practically speaking, engineering applications can pose higher risks than scientific research because they have more immediate real-world impacts\nPlus some papers they looked into sounded very close to industry such as software engineering, cybersecurity, etc."
  },
  {
    "objectID": "notes/2023-01-26.html#taxonomy",
    "href": "notes/2023-01-26.html#taxonomy",
    "title": "Austin ML Journal Club",
    "section": "Taxonomy",
    "text": "Taxonomy\n\nL1.1 (no test set): shocking!\nL1.2 (preprocessing): This is a loaded area because many different sub-domains exist such as imputation, oversampling, encoding, etc. Thus, it would’ve been nice to separate these areas further.\nL2 (illegitimate features):\n\nDefinition of proxy sounds a bit tricky.\n\nWe had a discussion about whether highly correlated features can be considered as data leakage. We agreed that all proxies are highly correlated features but not all highly correlated features are proxies.\nLike in hypertensive drug example, proxies are those that we should not be using (either because it’s cheating or not relevant or not available when making prediction) for obviously reasons. Thinking about the causal relationship also can help.\nAs the paper says, domain knowledge might be required to identify all proxies.\n\nThis L2 problem can get worse due to the high ML hype and ML code being extremely light (only a few lines of readily usable code available).\n\nL3.2 and L3.3: they are quite different from other data leakage problems.\n\nBoth are more challenging to identify.\nIf researchers can design experiments and collect data by themselves, they have some control over it, but for engineering applications/industry, often the data is given to us and we often don’t have many options to mitigate this problem.\nFor instance, it’s usually difficult to do split the data to achieve similar distributions between train and test (except class imbalance, temporal etc.)"
  },
  {
    "objectID": "notes/2023-01-26.html#model-info-sheets-and-our-theory-of-change",
    "href": "notes/2023-01-26.html#model-info-sheets-and-our-theory-of-change",
    "title": "Austin ML Journal Club",
    "section": "3.3: Model info sheets and our theory of change",
    "text": "3.3: Model info sheets and our theory of change\n\nHigh hopes!\nIt might not be useful to believe that the authors would follow this guideline. It will be a useful sanity checkpoint where the authors can think about the process (in this sense, this could be useful for industry people as well).\nBut of course, the authors can simply lie about it. But we get that this could be a middle-ground suggestion.\nAlso, in certain filed, people are very reluctant about sharing their data and code (such as oil and gas companies where well data is essentially their most valuable asset).\nIt might be more impactful if journals make this process as mandatory (but again, this will be a big ask)."
  },
  {
    "objectID": "notes/2023-01-26.html#limitations-of-model-info-sheets",
    "href": "notes/2023-01-26.html#limitations-of-model-info-sheets",
    "title": "Austin ML Journal Club",
    "section": "3.4: Limitations of model info sheets",
    "text": "3.4: Limitations of model info sheets\n\nAuthors not being ML experts and using ML in their research seems like another side-effect of ML hype."
  },
  {
    "objectID": "notes/2023-01-26.html#fig-1",
    "href": "notes/2023-01-26.html#fig-1",
    "title": "Austin ML Journal Club",
    "section": "Fig 1",
    "text": "Fig 1\n\nIt’s interesting that logistic regression isn’t affected much by the correction. It would have been nice if the authors explained more about the why (there’s some in appendix but it’s not super clear).\nAs the appendix explains, datasets from one paper were used by others even though they were based on imputation with high data leakage. Also some imputation processes in the papers the authors examined were done when the proportion of missing data is very high (like 90%)."
  },
  {
    "objectID": "notes/2023-01-26.html#problem-of-missing-significant-testing",
    "href": "notes/2023-01-26.html#problem-of-missing-significant-testing",
    "title": "Austin ML Journal Club",
    "section": "Problem of missing significant testing",
    "text": "Problem of missing significant testing\n\nWe discussed what exactly they were supposed to compare in terms of significant testing. Since they also mentioned confidence interval, we assumed it’s the results based on bootstrapping"
  },
  {
    "objectID": "notes/2023-01-26.html#enhancing-the-reproducibility",
    "href": "notes/2023-01-26.html#enhancing-the-reproducibility",
    "title": "Austin ML Journal Club",
    "section": "5: Enhancing the reproducibility",
    "text": "5: Enhancing the reproducibility\n\nFirst of all, this paper feels very redundant. This section also.\nAgain, these are good recommendations but maybe a big ask? (We were being realistic.)\nPrivacy concern is another thing to think about. If we violate privacy when we have to disclose so much about data and distribution to prove the absence of data leakage, how would we resolve this conflict?\nWe learned about the presence of CodeOcean (pretty cool effort)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, we are a group of machine learning (ML) practitioners in Austin, TX. We read ML papers together and discuss them in person once a month. If you are interested in learning about how our journal club works and how to join, please visit our Tips page."
  },
  {
    "objectID": "about.html#hongsup-shin",
    "href": "about.html#hongsup-shin",
    "title": "About",
    "section": "Hongsup Shin",
    "text": "Hongsup Shin\nHongsup is the organizer of the Austin ML Journal Club. He organizes the club meetings, and maintains the blog and the journal club GitHub repository.\nHe is a Principal ML Research Engineer at Arm and used to be a volunteer data scientist at Texas Justice Initiative, a criminal justice non-profit in Austin, TX. His background is computational neuroscience and behavioral ecology. He is interested in ML Ops, responsible AI, and AI ethics."
  },
  {
    "objectID": "about.html#kathrine-kate-behrman",
    "href": "about.html#kathrine-kate-behrman",
    "title": "About",
    "section": "Kathrine (Kate) Behrman",
    "text": "Kathrine (Kate) Behrman\nShe is a Staff ML Research Engineer at Arm in Austin, TX. She used to work for the USDA-NRCS and USDA-ARS as a Research Ecologist. Her background is in landscape and evolutionary ecology. She is interested in human AI interactions and responsible use of AI."
  },
  {
    "objectID": "about.html#kshitij-aggarwal",
    "href": "about.html#kshitij-aggarwal",
    "title": "About",
    "section": "Kshitij Aggarwal",
    "text": "Kshitij Aggarwal\nKshitij is a Data Scientist at Bristol-Myers Squibb. His background is in astronomy and electrical engineering. He is interested in problem-solving using state-of-the-art AI tools and responsible use of AI."
  },
  {
    "objectID": "about.html#meghann-agarwal",
    "href": "about.html#meghann-agarwal",
    "title": "About",
    "section": "Meghann Agarwal",
    "text": "Meghann Agarwal\nMeghann is a Machine Learning Engineer at GDI. Her educational background is in physics, music, and Russian. Her current interests include multilingual NLP, operationalizing ML models and other software, and societal impacts of evolving modes of information propagation."
  },
  {
    "objectID": "about.html#saina-lajevardi",
    "href": "about.html#saina-lajevardi",
    "title": "About",
    "section": "Saina Lajevardi",
    "text": "Saina Lajevardi\nSaina is a senior Data Scientist at Adobe Commerce, located in Austin, TX. Previously, she worked at Arm Cloud Technologies, where she developed machine-learning based frameworks in the IoT space. Her primary interests lie in problem-solving with a pragmatic approach."
  },
  {
    "objectID": "about.html#akshata-mohan",
    "href": "about.html#akshata-mohan",
    "title": "About",
    "section": "Akshata Mohan",
    "text": "Akshata Mohan\nAkshata Mohan does machine learning at Cloudflare. She is also one of the organizers of the local women in data science meetup in Austin. Her background is in information science and analytics and she’s interested in the applications of ML to cybersecurity."
  },
  {
    "objectID": "about.html#joel-afriyie",
    "href": "about.html#joel-afriyie",
    "title": "About",
    "section": "Joel Afriyie",
    "text": "Joel Afriyie\nJoel is a Machine Learning Engineer at Acrisure based here in Austin, TX. His background is in Computer Science and Fair ML. He is interested in MLOps, Music ML, Fair and Responsible AI, and Software and Data Engineering."
  },
  {
    "objectID": "about.html#athula-pudhiyidath",
    "href": "about.html#athula-pudhiyidath",
    "title": "About",
    "section": "Athula Pudhiyidath",
    "text": "Athula Pudhiyidath\nShe is a Data Scientist at PrimeStreet and is based in Austin, TX. Her background is in cognitive neuroscience. She is interested in keeping up with the ever-changing landscape of AI and ML and its applications to daily life and business."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Austin ML Journal Club",
    "section": "",
    "text": "Obermeyer et al. 2019 experiment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDissecting Racial Bias in an Algorithm Used to Manage the Health of Populations\n\n\n\n\n\n\npaper\n\n\n\nAmidst the LLM hype in ML, algorithmic bias is continued being overlooked although they still have major impact on critical domains such as healthcare, finance, and criminal justice. This seminal paper from 2019 found racial bias in a widely used healthcare algorithm, which used a problematic proxy (healthcare cost) as a target instead of what really matters, patient’s sickness. The paper is a few years old but the message is still relevant, and we also get to discuss what’s happened since then.\n\n\n\n\n\nMar 28, 2024\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model\n\n\n\n\n\n\npaper\n\n\n\nThis paper proposes a new technique to align LLMs with human preferences without using RL. This method is more robust and shows better performance over models trained with RLHF.\n\n\n\n\n\nJan 25, 2024\n\n\nKshitij Aggarwal\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?\n\n\n\n\n\n\npaper\n\n\n\nThis paper compares the performance of deep learning techniques to traditional tree-based methods for a novel set of 45 tabular datasets. The inductive biases of tree-based versus neural network models was analyzed as a proposed method to guide development of improved tabular specific neural network models.\n\n\n\n\n\nOct 19, 2023\n\n\nKathrine Behrman\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Machine Translation by Jointly Learning to Align and Translate\n\n\n\n\n\n\npaper\n\n\n\nThis paper marks an important step in the development of machine translation (MT). It came out just as Neural Machine Translation (NMT) was taking off, extending from Statistical Machine Translation (SMT), and as a milestone along the way to Transformer-based NMT. The authors introduce a novel attention mechanism applied to MT and show that it improves performance on long sentences, in particular, from prior recurrent neural network NMT approaches.\n\n\n\n\n\nSep 28, 2023\n\n\nMeghann Agarwal, Hongsup Shin, Claude\n\n\n\n\n\n\n\n\n\n\n\n\nConstitutional AI: Harmlessness from AI Feedback\n\n\n\n\n\n\npaper\n\n\n\nThere is an arms race of large language models (LLMs) in industry where companies use different approaches and techniques. Anthropic claims to adopt a more cautious approach that minimizes harm by LLMs than others. Let’s look into constitutional AI, the core algorithm of their LLM, to understand how this harm mitigation works.\n\n\n\n\n\nAug 31, 2023\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nAutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types\n\n\n\n\n\n\npaper\n\n\n\nIn e-commerce, it is challenging to organize and categorize products that are described by merchants in various ways. Finding a unified language and taxonomy has always been an underlying effort with commerce. This paper uses various ML algorithms to address this challenge.\n\n\n\n\n\nJul 20, 2023\n\n\nSaina Lajevardi, Hongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s Verify Step by Step\n\n\n\n\n\n\npaper\n\n\n\nThis is an interesting new paper by OpenAI that discusses how we can apply the same principles we use to solve math problems to AI. The paper evaluates different approaches to solving a dataset comprising of math problems. With this approach, they trained the model to get the right answer and but also “think” through the problem to arrive at the right answer.\n\n\n\n\n\nJun 22, 2023\n\n\nAkshata Mohan, Hongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization in Bayesian workflow\n\n\n\n\n\n\npaper\n\n\n\nThis paper summarizes types of data visualization that we can use in Bayesian modeling and inference. It also provides a good overview of how to do Bayesian data analysis properly, including model validation such as prior and posterior predictive checks.\n\n\n\n\n\nMar 29, 2023\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nReviwing a Case Study About Real Estate Market Prediction\n\n\n\n\n\n\npaper\n\n\n\nThe paper reviewed here attempts to predict outcomes about the real estate market with binary classification. Though the paper’s research design and results were lacking, it gave us a chance to have a discussion about practices for experimental design.\n\n\n\n\n\nFeb 23, 2023\n\n\nAthula Pudhiyidath\n\n\n\n\n\n\n\n\n\n\n\n\nLeakage and the Reproducibility Crisis in ML-based Science\n\n\n\n\n\n\npaper\n\n\n\nData leakage in a common problem in ML-based science leading to reproducibility failures and overly optimistic conclusions. We discussed 8 types of data leakage and the use of model info sheets to identify and reduce all leakage types.\n\n\n\n\n\nJan 26, 2023\n\n\nKate Behrman\n\n\n\n\n\n\n\n\n\n\n\n\nZero-Shot Text-to-Image Generation\n\n\n\n\n\n\npaper\n\n\n\nThere is so much hype in generative AI. But how does it actually work? We discuss OpenAI’s DALL-E paper to understand model architecture but more importantly, whether their model validation is solid and reasonable.\n\n\n\n\n\nDec 15, 2022\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\n“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI\n\n\n\n\n\n\npaper\n\n\n\nGarbage in, garbage out. It seems like a lot of people in the ML community still don’t understand this logic. We discuss poor data-handling practices and their critical ramifications.\n\n\n\n\n\nOct 27, 2022\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\nWhy ML Journal Club\n\n\n\n\n\n\nnews\n\n\n\nWelcome to our journal club! I talk about why I organized an in-person journal club with my fellow ML practitioner friends in Austin, TX.\n\n\n\n\n\nOct 17, 2022\n\n\nHongsup Shin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20240125/index.html",
    "href": "posts/20240125/index.html",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Direct Preference Optimization: Your Language Model is Secretly a Reward Model\nPresenter: Kshitij\nAttendees: Kate, Hongsup, Todd, Meghann"
  },
  {
    "objectID": "posts/20240125/index.html#why-this-paper",
    "href": "posts/20240125/index.html#why-this-paper",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "Why this paper?",
    "text": "Why this paper?\nFollowing the latest advancements in LLM training, I’ve been interested in Direct Preference Optimization (DPO), a technique emerging as a superior method. The buzz around DPO was further amplified by Andrew Ng’s tweet, praising the method for its simplicity and profound insight."
  },
  {
    "objectID": "posts/20240125/index.html#minute-summary",
    "href": "posts/20240125/index.html#minute-summary",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "1-minute summary",
    "text": "1-minute summary\nIn the realm of Large Language Model (LLM) training, Direct Preference Optimization (DPO) emerges as a promising technique that streamlines the process by bypassing the complexities of traditional Reinforcement Learning from Human Feedback (RLHF). Unlike RLHF, which relies on a separate reward model to fine-tune LLMs based on human or AI-generated preferences, DPO integrates preference optimization directly into the language model training process. It does so by defining the preference loss directly as a function of the policy, instead of training a separate preference reward model. DPO enhances model performance and alignment with human preferences, as demonstrated in tasks ranging from sentiment analysis to content summarization and dialogue generation. This blog delves into some technical nuances of DPO, contrasts it with traditional methods, and highlights its superior efficiency and effectiveness through various application examples1.\nIf you just want the key results, skip to this section. Two useful blogs about DPO that I referred to: 1, 2."
  },
  {
    "objectID": "posts/20240125/index.html#background",
    "href": "posts/20240125/index.html#background",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "Background",
    "text": "Background\nBefore we dive deep into DPO, let’s do a quick overview of LLM training process:\n\nPretraining: LLMs undergo self-supervised training with vast internet-scale data, focusing primarily on next-token prediction. This stage doesn’t require labeled data, although the quality of the data is still vital.\nInstruction Fine Tuning or Supervised Fine tuning (SFT): Quoting from the DPO paper: “performance on downstream tasks and alignment with user intent can be significantly improved by fine-tuning on datasets of instructions and humanwritten completions. This ‘instruction-tuning’ procedure enables LLMs to generalize to instructions outside of the instruction-tuning set and generally increase their usability”. Llama 2 paper used around 14,000 such examples for their SFT training. This model would be referred to as \\(\\pi^{\\text{SFT}}\\). Quick note on notation.\nReinforcement Learning from Human Feedback (RLHF): Since generating expert human responses for instructions is costly, the focus shifted towards collecting human preferences between answers, which are simpler to label and gather, forming a preference dataset. It is created by generating multiple outputs (\\(y_1, y_2\\)) for each prompt (\\(x\\)) using an SFT model (\\(\\pi^{\\text{SFT}}\\)), and then using humans or another LLM to order them (preferred: \\(y_w\\), dispreferred: \\(y_l\\)), hence the preference. In RLHF, a reward model is first trained on the preference dataset (usually an LLM itself) and then the SFT model is fine tuned to maximise the reward using reinforement learning algorithms.\n\nDPO paper states: “… fine-tuning large language models with reinforcement learning remains a major practical challenge; this work provides a theoretically-justified approach to optimizing relative preferences without RL”."
  },
  {
    "objectID": "posts/20240125/index.html#rlhf",
    "href": "posts/20240125/index.html#rlhf",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "RLHF",
    "text": "RLHF\n\n\n\nRLHF vs DPO\n\n\nThe paper does a pretty good job of summarising RLHF and related steps Fig1.  The preference dataset generated by human labels (or AI labels) is actually a proxy for the latent reward model (which we don’t have access to). Next, given reward values for two outputs \\(y_1, y_2\\), Bradley Terry (BT) model can be used to compare them. i.e:\n\\[\n\\text{Probability of comparison A &gt; B} = \\frac{\\text{Score}(A)}{\\text{Score}(A) + \\text{Score}(B)}\n\\]\nwhere instead of score, we will use rewards from the reward model. Typically, given a dataset of prompts \\(x^i\\) and preferences (\\(y_w, y_l\\)), the reward model can be trained by maximising the likelihood:\n\\[\n\\mathcal{L}_R = -\\mathbb{E}\\left[ \\log \\sigma(r(x, y_w)) - r(x, y_l))\\right] \\tag{1}\n\\]\nwhich is basically logistic of the difference of the reward of preferred and reward of the dispreferred. Note that only difference of rewards are needed here. Using this, a reward model is trained. This is then used to provide feedback to the language model using the following optimization:\n\n\\[\n\\max_{\\pi_\\theta} \\mathbb{E}_{x\\sim D, y\\sim \\pi_\\theta(y|x)}\\left[ r_\\phi(x,y) \\right] - \\beta D_{KL}\\left( \\pi_\\theta(y|x) || \\pi_{\\text{ref}}(y|x) \\right) \\tag{2}\n\\]\nThis expression is maximizing the expected value of reward (\\(r_\\phi(x,y)\\)), minus a Kullback-Leibler divergence term between current policy \\(\\pi_\\theta(y|x)\\) and the reference policy \\(\\pi^{\\text{SFT}}(y|x)\\). \\(y\\) is the output of the \\(\\pi_{\\theta}\\) model with input \\(x\\). The KL term prevents the model to deviate too far away from the reference policy which leads to coherent outputs (RLHF paper).\n\nTHe authors state that this objective is non-differentiable, and therefore is optimized with reinforcement learning. We discussed a bit on it and were a little unsure why that is the case. Maybe because the preference model is essentially a ranking model and ranking loss is often non differentiable."
  },
  {
    "objectID": "posts/20240125/index.html#dpo",
    "href": "posts/20240125/index.html#dpo",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "DPO",
    "text": "DPO\nFrom the paper: “This … approach avoids fitting an explicit, standalone reward model, while still optimizing under existing models of human preferences, …. In essence, the policy network represents both the language model and the (implicit) reward.” If you are interested, feel free to take a look at the complete derivation on this blog. Briefly, they show that by writing the reward in terms of the policy:\n\\[\nr(x, y) = \\beta \\log \\left( \\frac{\\pi_r(y | x)}{\\pi_{\\text{ref}}(y | x)} \\right) + \\log Z(x). \\tag{3}\n\\]\nand using that in the likelihood estimate for RHLF (equation 2), we can obtain the DPO objective as:\n\\[\n\\mathcal{L}_{DPO}(\\pi_\\theta; \\pi_{\\text{ref}}) := -\\mathbb{E}_{(x,y_u,y_l)\\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_u | x)}{\\pi_{\\text{ref}}(y_u | x)} - \\beta \\log \\frac{\\pi_\\theta(y_l | x)}{\\pi_{\\text{ref}}(y_l | x)} \\right) \\right]. \\tag{4}\n\\]\nFinally, they obtain the gradient of loss function for DPO:\n\\[\n\\nabla_{\\theta} \\mathcal{L}_{\\text{DPO}}(\\pi_{\\theta}; \\pi_{\\text{ref}}) = -\\beta \\mathbb{E}_{(x,y_w,y_l)\\sim D} \\left[ \\sigma(\\hat{r}_{\\theta}(x, y_l) - \\hat{r}_{\\theta}(x, y_w)) \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(y_w|x) - \\nabla_{\\theta} \\log \\pi_{\\theta}(y_l|x) \\right] \\right],\n\\]\nWhere, \\[\n\\hat{r}_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}\n\\]\n\n\nThere are two terms in this equation. Let’s start with the second term:  \\[\n\\nabla_{\\theta} \\log \\pi_{\\theta}(y_w|x) - \\nabla_{\\theta} \\log \\pi_{\\theta}(y_l|x)\n\\]\nIncreases the probability of favored output (\\(\\nabla_{\\theta} \\log \\pi_{\\theta}(y_w|x)\\)) and decrease the probability of disfavored output (\\(\\nabla_{\\theta} \\log \\pi_{\\theta}(y_l|x)\\)).\nNow the first term: \\[\n\\begin{align}\n\\sigma(\\hat{r}_{\\theta}(x, y_l) - \\hat{r}_{\\theta}(x, y_w)) =\\sigma(\\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\frac{\\pi_{\\text{ref}}(y_w|x)}{\\pi_\\theta(y_w|x)})\n\\end{align}\n\\]\nThis term comes from the KL penalty, essentially preventing model from diverging too far away from the reference model. Here, \\(\\hat{r}_{\\theta}\\) is the implicit reward model, and so the gradient is weighted by how much the reward model rates dispreferred completions.\nFrom 1: “Thus, instead of first learning a reward and then finding the optimizing policy, one directly finds the optimal policy such that its reward as obtained corresponds to collected human preferences.”\n\nDPO Steps:\n\nGet outputs (\\(y1, y2\\)) from an SFT model (\\(\\pi^{\\text{SFT}}\\))\nAsk humans or LLM to give preferences on those (\\(y_w, y_l\\))\nTrain \\(\\pi_{\\theta}\\) starting from (\\(\\pi^{\\text{SFT}}\\)) using the DPO objective.\n\nNote: If you have the preference dataset, but don’t have acces to the underlying \\(\\pi^{\\text{SFT}}\\) then the authors take an off the shelf LLM and supervise fine-tune it using the prompts and preferred outputs of the preference dataset to get \\(\\pi^{\\text{SFT}}\\). They then do DPO on it."
  },
  {
    "objectID": "posts/20240125/index.html#results",
    "href": "posts/20240125/index.html#results",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "Results",
    "text": "Results\nThey explored three different tasks: Sentiment generation on IMDb dataset, summarization of Reddit posts, single turn dialogue (replying to a human query). For sentiment generation task, they use a sentiment classifier for evaluation, and for others they use GPT4 as a proxy for human evaluator to evaluate the win rate against a baseline policy. They also conduct a human study to justify their usage of GPT-4 for evaluation and find that GPT-4 judgments correlate strongly with humans, with human agreement with GPT-4 typically similar or higher than inter-human annotator agreement.\nThey test DPO againsst the following approaches: PPO, Best of N (sampling N responses and returning highest scoring on reward), Unlikelihood (maximizes probability of \\(y_w\\) and minimizes probability of \\(y_l\\)), GPT-J, Pythia-2.8B, SFT model, Preferred-FT (SFT on \\(y_w\\)).\n\n\n\nComparing average reward of different models on test dataset with respect to the KL to the reference policy.\n\n\n\n\n\n\n\n\n\n\nSummarization win rates vs. human-written summaries.\nWin rate for one step dialogue.\nWin rates for different sampling temperatures over the course of training.\n\n\n\n\nKey results:\n\nDPO is efficient, achieving the highest reward while still achieving low KL with respect to reference policy (Fig2).\nDPO exceeds PPO’s performance on summarization, and on one step dialogue task. DPO is even better than (very computationally expensive) “best of N” in most cases (Fig3, Fig4).\nDPO is much more robust to sampling temperature than PPO (Fig3).\nDPO converges to its best performance relatively quickly (Fig4).\nDPO generalizes to new input distribution better than PPO (but the difference is not very significant)."
  },
  {
    "objectID": "posts/20240125/index.html#discussion",
    "href": "posts/20240125/index.html#discussion",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "Discussion",
    "text": "Discussion\nWe found this paper interesting, though the derivation was hard to follow sometimes. The main idea of the paper is simple and powerful: instead of doing RL, you can use the LLM itself as a reward model (by comparing the reward of the model at a training stage with respect to the reward of the SFT model), and only use the difference in the rewards (between favored and disfavored outputs) to train the model for external preferences. Referencing online blogs that explained the derivation well helped. We had to go back and forth between main paper and appendix to follow the derivation, but once you see all the steps, it makes sense. Though we still felt that although we understood mathematically how an equation was derived, obtaining the intuition for that needed more discussion. This was specially true for the gradient of objective for DPO, where we spent some time understanding each term.\nWe also discussed at length about what exactly the reward model is, and what is meant by \\(\\pi\\) (and in general the notation used in the paper). The results clearly showed that DPO was better, though authors were careful in stating that further tests are needed on larger models. The authors claimed that PPO is unstable, though we were not sure why, and how DPO resolves that. We also discussed what it meant for the RLHF reward to be “non-differentiable”. We were not sure why the authors didn’t tune beta parameter to further optimize DPO performance. We appreciated the authors comparing the GPT-4 judgements with Human judgements, and showing that “GPT-4 tends to agree with humans about as often as human agree with each other”. Towards the end, we looked at some example outputs authors provide for various prompts.\n\nNotation\n\n\\(\\pi^{\\text{SFT}}(y | x)\\): refers to probability of getting output sequence \\(y\\) given input sequence \\(x\\). This will be computed as product of probability of each word which can be obtained from the probabilities of all the words in the vocabulary that the model outputs (as explained in second figure of this blog).\n\\(r (y, x)\\): Reward of \\(y\\) with \\(x\\) as input. Or the probability of \\(y\\) given \\(x\\) as input as per the reward model."
  },
  {
    "objectID": "posts/20240125/index.html#footnotes",
    "href": "posts/20240125/index.html#footnotes",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEdited with assistance from ChatGPT↩︎"
  },
  {
    "objectID": "posts/20230928/index.html",
    "href": "posts/20230928/index.html",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Neural Machine Translation by Jointly Learning to Align and Translate\nPresenter: Meghann\nAttendees: Athula, Akshata, Hongsup, Kate, Kshitij, Renée, and Sarah"
  },
  {
    "objectID": "posts/20230928/index.html#why-this-paper",
    "href": "posts/20230928/index.html#why-this-paper",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "Why this paper?",
    "text": "Why this paper?\nI chose this paper for a few reasons. First, I have a longheld interest in human languages and language-learning plus some experience as a freelance translator, prior to the release of Google Translate. I casually follow the progression of machine translation tools and am generally curious about how they are implemented. Second, I’ve recently been involved with MT tasks at work and want to deepen my understanding of its foundations. Finally, MT is a prevalent use case in the development of Large Language Models (LLMs); this is a well-cited paper in that path that I felt may be worthwhile for others in the Journal Club to read and discuss."
  },
  {
    "objectID": "posts/20230928/index.html#paper-summary",
    "href": "posts/20230928/index.html#paper-summary",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "Paper Summary",
    "text": "Paper Summary\nThere are many good posts, videos, tutorials, etc. about this paper. Rather than write my own summary, I’ve included LLM output generated via Anthropic’s Claude AI Assistant for a use-case similar to machine translation: summarization (i.e., English-to-English “translation”). The output looks okay so I have left it unchanged.\nI’ve also included a few resources that that were helpful and/or interesting to me while reading this paper.\n\nClaudeAI Summary\nI uploaded the pdf to https://claude.ai/ and gave the following prompt to get a summary of this paper.\nPrompt: “Summarize this article”\nOutput: “Here is a summary of the key points from the article:\nThe article proposes a novel neural network architecture for neural machine translation. The key ideas are:\n\nExisting encoder-decoder models encode the full source sentence into a fixed-length vector from which the translation is decoded. This may be problematic for long sentences.\nThe proposed model encodes the source sentence into a sequence of vectors and adaptively selects a subset of these source vectors while decoding each target word. This allows it to focus only on the relevant parts of the source sentence.\nAn alignment model is jointly trained to compute a soft alignment between the source and target words, allowing gradients to be backpropagated through the alignments.\nExperiments on English-French translation show the proposed model (RNNsearch) significantly outperforms a standard encoder-decoder model (RNNencdec), especially on long sentences.\nThe RNNsearch model achieves similar performance to phrase-based statistical machine translation on a test set with only sentences containing known words.\nQualitative analysis shows the model learns linguistically plausible alignments between source and target words.\n\nIn summary, the key innovation is an adaptive attention-based encoder-decoder architecture that gives improved translation performance compared to prior encoder-decoder sequence-to-sequence models for neural machine translation.”\n\n\nAdditional Resources\n\nVisualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) - This post gives a great conceptual overview of the paper and describes the salient changes from previous sequence-to-sequence models, namely that (1) the decoder does not receive only the last hidden state from the encoder but rather receives all hidden states, and (2) scores the encoder hidden states at each decoder time step to produce a context vector relevant to that particular time step instead of a fixed-length context vector for all times.\nPyTorch Seq2Seq Tutorials - This set of six notebooks takes one through a progression of code implementations of papers, from those preceding and/or contemporary to this paper (notebooks #1 and #2), to this paper (notebook #3), and culminating in the paper that introduced self-attention and transformers (notebook #5).\nDive into Deep Learning: Coding Session #4 Attention Mechanism I (Americas/EMEA) - Just before the Journal Club meeting it was discovered that one of us (Kshitij) led a live coding session of the “Bahdanau Attention” RNN in this video. The theory section before it is also very nice.\nNatural Language Processing Coursera course by HSE University - I initially watched these videos on Coursera and found the lectures to be very clear. However, Coursera no longer hosts the course; it has been moved to HSE’s online education portal which requires account creation and login to find the course. The code repository associated with the Coursera course is still hosted on GitHub. The lectures are currently on YouTube, although without the weekly groupings and other materials that exist in the course. This paper (Bahdanau et al.) is discussed in the video titled “Attention Mechanism”.\nBuilding Machine Translation Systems for the Next Thousand Languages - I found this report very interesting. It didn’t seem like the right fit for Journal Club (e.g., not peer-reviewed) but it has a lot of interesting discussion and I find the reference tables quite useful."
  },
  {
    "objectID": "posts/20230928/index.html#paper-discussion",
    "href": "posts/20230928/index.html#paper-discussion",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "Paper Discussion",
    "text": "Paper Discussion\nMany thanks to Hongsup for taking notes during the discussion and for sharing them. I’ve lightly edited some of them in the notes section below. Others I have integrated with my own recollections from our Journal Club conversation, a few months later as I’m putting together this post (my apologies for not posting sooner).\nWe started the meeting by discussing machine translation in general along with more specific observations of MT from a user’s point of view (some of which are listed in the text below). We then put the paper’s contribution into context within the broader scope of MT development with a foray into the history of MT development. A standard method prior to this paper was phrase-based statistical MT (phrase by phrase; dictionary of translations; lookup table) (e.g., Moses). This paper exists in the regime of recurrent neural networks sequence-to-sequence neural MT. A standard method nowadays is Transformer-based neural MT (c.f., Vaswani et al.).\nAfter the introductory comments we delved in to the details of the paper together.\n\nDiscussion Notes\nThe following notes serve as a reminder to attendees of the various discussion points.\n\nEncoder-decoder model: the max value of i and j can be different; input and output vector lengths may not match. Similar to text summarization.\nMain contribution:\n\nFlexible length vector\nSoft-search: what is “soft”? (probably probabilities, soft-max), also a bit unclear what “search” means\n\n“Fixed length” vector\n\nWhat is exactly fixed? not the number of units in a hidden layer\nContext vector: c vs. c_i (c_i depends on annotation, which seems to correspond to the number of words?)\n\nDifference between this and “Attention is All You Need” paper? (RNN vs. transformer?)\nWhat is alignment model?: essentially a tanh function (single-layer multilayer perceptron)\nBLEU and perplexity (higher: more “surprised” or “perplexed” so lower the better)\nDatasets\n\nEuroparl: european parliament database (probably 1:1 translation relationship because it’s parliament document translation)\nParallel corpora: probably hard to make a high quality one\n\nData split (validation/test) is interesting because temporal nature is considered\nMost frequent “words”: words or tokens?\nBeam search?\nFigure3: not one-to-one mapping, so weights; evidence of alignment model working (proba)\n\nHow would it look like when we pair up languages with different origins?\nWould’ve been nice to pair up non-english languages\n\nCan we zero-shot machine translation? how would this work?\nTable 1\n\nMoses seems to use a larger dataset\nBLEU score is compared to Moses (the fine-tuned 50-words model; would’ve been nice to see qualitative results)\n\nModel training compute resource description was different back then: “5 days” (of what though?)\nAppendix C: difficult to understand if you’re not a French speaker\nWhy does machine translation get worse towards the end? (especially after 30 words, what’s so special about this number?)\n\nMaybe related to not being directional? (Moses)\nMaybe because of training data? The majority examples might be around 30 words? (15-40 words the paper said); would’ve been nice to see the distribution of word counts across sentences\nIn a longer sentence, the latter parts will have to refer to previously mentioned words and this can be forgotten, difficult to figure out, etc."
  },
  {
    "objectID": "posts/20230928/index.html#final-thoughts",
    "href": "posts/20230928/index.html#final-thoughts",
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n“Neural Machine Translation by Jointly Learning to Align and Translate” marks an important step in the development of machine translation (MT), applying the concept of attention to MT.\nI found it beneficial to discuss this paper as a group, working together to explain and understand specific concepts and terms. The meeting also provided a forum for exploring and thinking about broader topics around machine translation. I’ll end by listing a few of the topics and questions that we considered.\n\nSentence-based translation and how that can mis-translate pronouns for highly conjugated languages where the pronoun is not explicit. For example, knowing context from other sentences in a person’s bio could reduce mis-translations of pronouns (changing between sentences) - it’s obvious to a human reader.\nChallenges of translating/detecting slang, expletives, rare words or phrases, etc.\nZero-shot translation (e.g., where parallel corpora do not exist for a language pair) - Comparing the embedding space of one language to another: Would a model represent a concept across multiple language? How would this look?\nHow can a machine translation avoid stereotyping: can we create a representation while maintaining diversity? - Hallucinations and bias: how about a rule-based ML model?\nExtending translation to many languages, different approaches for different language challenges (fewer examples, different language structure, etc. c.f., the thousand language paper) How is Google Translate able to translate between so many languages? See Building Machine Translation Systems for the Next Thousand Languages.\n\nFund. ނިމުމެވެ. Endirinn. Ti Panungpalan. Njedebe. Cоңы. Tungtungna. Isiphetho. अन्तम् (translations of “The End” using Google Translate)"
  },
  {
    "objectID": "posts/20231019/index.html",
    "href": "posts/20231019/index.html",
    "title": "Why Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Why do tree-based models still outperform deep learning on typical tabular data?\nPresenter: Kate\nAttendees: Athula, Brian, Hongsup, Joel, Kshitij"
  },
  {
    "objectID": "posts/20231019/index.html#why-this-paper",
    "href": "posts/20231019/index.html#why-this-paper",
    "title": "Why Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?",
    "section": "Why this paper?",
    "text": "Why this paper?\nThis paper piqued my interest because it is a topic that I commonly grapple with. While deep learning techniques are highly publicized for superior performance on image and language data, many practical machine learning (ML) applications rely on tabular data. As an ML practitioner, there is a desire to use cutting edge techniques if they make sense for the data and application. This paper addresses the outstanding question, “Does deep learning outperform tree-based models on tabular data?” In order to answer this question, several shortcomings that have hindered a direct comparison of these modeling techniques on tabular data were addressed. They also compare the inductive biases of deep learning vs tree-based models to help further the development of tabular-based deep-learning architectures"
  },
  {
    "objectID": "posts/20231019/index.html#methods",
    "href": "posts/20231019/index.html#methods",
    "title": "Why Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?",
    "section": "Methods",
    "text": "Methods\nBenchmark creation\nOne of the major contributions of this paper is a set of 45 tabular benchmarks used in this study and to be used in the future. The lack of standard and consistent tabular benchmarks was identified by the authors as one of the main reasons tabular studies aren’t reproducible. They fill this gap by making their tabular datasets available through the OpenML platform. This is a major advancement compared to the referenced literature that provides comparisons on 3 and 11 selected tabular datasets.\nFor a dataset to be included as a benchmark, it must meet the following criteria:\n\nHeterogenous columns- each feature is different in nature (not the same signal with a different sensor).\nNot high dimensional- number of dimensions (d) less than 500; d/n (number of observations) less than 1/10.\nNo undocumented datasets- removed if little information available.\nI.I.D. data – no time series or stream-like data.\nReal-world data- no artificial datasets with the exception of simulated data of practical importance.\nNot too small- no datasets with less than 4 features or less than 3k samples.\nNot too easy- remove dataset if a simple model (single tree or regression) scores within 5% of a default Resnet or HistGradientBoosting model.\nNot deterministic- remove dataset if the target is a deterministic function of the data (simulated games).\n\nIn addition to the strict criteria above, the authors also removed “side issues” for practical purposes. Five side issues were removed by applying the following rules:\n\nMedium-sized training set- data too large was truncated to 10k samples\nNo missing data- remove all missing data by column first and then row.\nBalanced classes- binarized data if there is more than 1 class and under sampled to retained equal samples of the top two frequency classes.\nLow cardinality categorical features- remove categorical features with more than 20 items.\nHigh cardinality numerical features- remove numerical features with less than 10 unique values and convert to categorical if there are 2 categories.\n\nData preprocessing was minimal after the removal of side issues and benchmark criteria was met. All models that don’t directly include categorial features used ScikitLearn’s OneHotEncoder on categorical features. Regression targets were log-transformed when skewed. All features for neural network training were transformed using ScikitLearn’s QuantileTransformer.\nModel selection\nThe authors compare 3 tree-based and 4 deep learning models. For tree-based methods the authors chose all ScikitLearn ensemble models: RandomForest, GradientBoostingTrees (or HistGradientBoostingTrees for datasets with categorical features), and XGBoost. For deep learning, they chose Multi-layer perceptron (MLP), Resent, FT_transformer (simple transformer with embedding for categorical and numerical features), and SAINT (transformer model with inter-sample attention).\nModel tuning and selection\nThe authors make an effort to tune both deep learning models and tree-based models in a comparable way for each dataset and model using predefined hyperparameter grids and a random search of roughly 400 iterations in Hyperopt-Sklearn. These hyperparameter searches were saved and used as a bank to study performance as a function of the number of random searches. Each number of random searches was repeated 15 times per model and benchmark to asses variance in model performance. For each search the best hyperparameters of the validation set were chosen and evaluated on the test data. Each dataset was divided such that 70% was used for training, 9% for validation, and 21% for test.\nAggregating results across datasets\nThe classification datasets were assessed using test accuracy and regression datasets were assessed using the R-squared score. To aggregate results across benchmarks, the average distance to the minimum was used to “normalize scores” between the best model, 1, and worst model, 0. However, the worst model was replaced with the test score of 10% quantile for classification and 50% quantile for regression. All negative values were reset to 0. This procedure of model tuning and selection, bootstrapping (15 times per model and benchmark), and aggregating across benchmarks was repeated several times to answer the original question and investigate each inductive biases."
  },
  {
    "objectID": "posts/20231019/index.html#results",
    "href": "posts/20231019/index.html#results",
    "title": "Why Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?",
    "section": "Results",
    "text": "Results\nModel performance was analyzed separately for classification and regression tasks (Figure 1). Furthermore, the benchmarks were subdivided into those that only had numerical features and those that had numerical plus categorical features. The normalized test accuracy of 16 classification datasets with only numerical features revealed that XGBoost, GradientBoostingTrees, and RandomForest had higher normalized accuracy than the 4 deep learning models for all number of random searches. The results were also similar for the 19 regression datasets with only numerical features except that the SAINT model outperformed GradientBoostingTrees only for a low numbers of random searches (less than 10). XGBoost was the top performing model for all random search iterations for both classification and regression tasks with only numerical features. For the numerical plus categorical features HistGradientBoostingTree was added. HistGradientBoostingTree is the top-performing model when random search iterations are low (less than 10 for classification and less than 14 for regression) and XGBoost is the superior model for high search iterations. All tree-based methods outperform all deep learning methods for classification and for regression when the number of searches is more than 10.\n\nThe authors came to two big conclusions from this work:\n\nHyperparameter tuning of deep learning models did not improve their performance to be better than tree-based methods\nDeep learning methods were not hindered by the inclusion of categorical features.\n\nEmpirical investigation: why do tree-based models outperform deep learning on tabular data?\nThe authors aim to understand the inductive biases or “inherent properties of these models which explain their superior performance on tabular data.” The tree-based models with superior performance are bagging and boosting algorithms that are based on ensembles of decision trees. Therefore, this section focuses on understanding why decision trees explain tabular data well. In this section, hypotheses were tested by applying a treatment to the input data and measuring average normalized test performance.\nFirst, smoothing transformations were applied to the target features of the tabular classification datasets. This was done by applying a Gaussian kernel smoothing function stretched across different scales. The normalized test accuracy was compared across 15 iterations of 60 hyperparameter searches for four kernels. The four models compared had different performance when no smoothing is applied (Figure 2). However, when smoothing is applied the tree-based models (RandomForest and GradientBoostingTree) are more sensitive to the degree of smoothing. This shows that tree-based methods fit better to irregular functions than neural networks. This is consistent with studies in the literature that show that neural networks perform well on smooth functions.\n\nSecond, the number of non-informative features was varied in two experiments, one removing non-informative features (Figure 4a) and one adding non-informative features (Figure 4b). Removing non-informative features reduced the performance gap between tree-based and deep learning models. Next, uninformative features sampled from standard Gaussians distribution were added to each datasets. The additional of uninformative features decreased the normalized test score of MLPs (Resnet) much more than GradientBoostingTrees and RandomForest. This shows that tree-based methods are less affected by uninformative features.\n\nThird, tree-based methods and transformers are not rotationally invariant. MLPs are rotationally invariant. The logic here wasn’t quite adequately explained but we followed the main idea. Which is that it is harder for rotationally invariant algorithms to remove uninformative features because the invariant algorithm needs to identify the orientation and then select the least informative. To test this theory, the authors randomly rotated the training datasets and computed the normalized test accuracy for the original dataset (no rotation) and with rotation. The rotation had no effect on the test performance for Resent. Furthermore, model performance with no rotation in order of highest to lowest was GradinetBoostingTree, RandomForest, FT Transformer and Resnet. With the rotation, the order was completely reversed. This indicates that rotational invariance is not a desirable property for models trained using tabular data.\nIn the end the authors use these three inductive biases as a way to recommend how to improve neural network architecture for tabular data. They conclude that a neural network architecture must be robust to uninformative features, preserve the orientation of the data, and learn sharp irregular shaped functions to perform well on tabular data. The authors acknowledge many limitations to their current approach. Most of these have to do with the criteria for benchmark create and the side issues that were removed."
  },
  {
    "objectID": "posts/20231019/index.html#discussion",
    "href": "posts/20231019/index.html#discussion",
    "title": "Why Do Tree-based Models Still Outperform Deep Learning on Typical Tabular Data?",
    "section": "Discussion",
    "text": "Discussion\nIn general we found this paper very interesting and thought provoking. We thought the authors made a good effort to create a new set of tabular benchmark datasets. However, we spent a lot of time talking about the benchmark criteria and how this could bias the results presented here. We found the maximum datasets size and number of features to be very restrictive such that it doesn’t include many of the datasets used in our work. These numbers felt like magic since there wasn’t any discussion of why or how the numbers were chosen. The benchmark criteria also removed datasets that were “too easy”. We thought that this action could bias the results because smoother target functions may be removed in this step. This could impact the first inductive bias that most target functions are irregular and captured by better by tree-based models. Neural networks are better at picking out smooth functions but these datasets may have been removed from the tabular benchmarks during creation. We also felt that removal of missing data and imbalanced class ratios was an oversimplification as these issues are common in real-world datasets. In general, even though benchmarks are termed “typical tabular datasets” in the title, we felt the restrictive criteria and removal of side issues likely created more homogenous tabular datasets than naturally occur.\nThe shaded regions around the average lines were difficult to understand. During our discussion it took us a long time to decipher how these were calculated. We finally came to realize that these regions were calculated from the 15 repeat draws per random search size, model, and benchmark. These 15 draws were used to aggregate across benchmarks 15 times to calculate 15 normalized test scores. The min and the max of these 15 scores are shown by the shaded area. This is to give an idea of how much variation the random hyperparameter values contribute the results. We felt that these results may be misleading for high random search values. That is because bootstrapping from a fixed bank results in decreasing variance as search size increases. This effect is visible in Figure 1. This procedure is important to understand as it was used throughout the paper in all figures with variance shown as boxplots or shaded regions.\nThe authors highlight that this study is one of the first to have a similar level of hyperparameter tuning for both tree-based and deep learning models. From this, they concluded that hyperparameter tuning of deep-learning models did not improve performance beyond tree-based methods. However, this conclusion is solely based on the number of hyperparameter searches. We noticed that authors did not address details like different size hyperparameter grids for the models (or model types) and compute time to fit a model to one hyperparameter set. We would have been interested to see some discussion of these factors that are known to differ for tree-based and deep learning models. Furthermore, neural networks might perform better on larger datasets with an increased tuning budget.\nWe liked the idea of analyzing inductive biases, however we found most of the conclusions seemed fairly obvious. Two of the three inductive biases seemed to be known properties of tabular data, which is that it is rotationally invariant by nature and has irregular target functions. The last bias is a known advantage of tree-based methods, which is the ability to perform well amongst many non-informative features. However, we appreciated the rigorous and creative methods used to test each bias."
  },
  {
    "objectID": "posts/20221215/index.html",
    "href": "posts/20221215/index.html",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Zero-Shot Text-to-Image Generation\nPresenter: Hongsup\nAttendees: Hannah, Hongsup, Joel, and Steve"
  },
  {
    "objectID": "posts/20221215/index.html#why-this-paper",
    "href": "posts/20221215/index.html#why-this-paper",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Why this paper",
    "text": "Why this paper\nGenerative AI has a lot of hype in ML community these days. OpenAI’s DALL·E, GPT-3, and ChatGPT are good examples. And there’s also stable diffusion. Since they all have public API, not just ML practitioners but general public can use the models to generate texts or images, which creates even bigger hype around generative AI.\nBut whenever there is hype around something, I think we should be more curious about what’s going on behind the scene. Understanding how it works helps us see through the hype and that is why I chose this paper. We can understand how DALL·E’s text-to-image generative model works, what the authors did to make this happen, and how they validated the result.\nTo understand this paper thoroughly, you need to know other deep learning model frameworks such as transformer, variational autoencoder, and OpenAI’s CLIP (Contrastive Language-Image Pre-training) model. I found these two articles extremely useful, written by Charlie Snell at UC Berkeley. In this post, I will talk about a high-level summary and the interesting discussion we had as a group. If you are interested in more detailed summary of the paper itself, I recommend those two posts."
  },
  {
    "objectID": "posts/20221215/index.html#big-picture",
    "href": "posts/20221215/index.html#big-picture",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Big picture",
    "text": "Big picture\nThe authors created a deep learning model which generate images from a text input. For instance, if you type “hands”, the model will generate images of hands. As the title says, this is done in zero-shot way, meaning that it can generate images that it hasn’t seen before. To be clear, the authors of this paper are not the first ones who created a model like this. There have been precedents but the authors say that the generated images from those still suffer from severe artifacts such as object distortion, illogical object placement, or unnatural blending of foreground and background elements. So the authors made improvements by adopting these two approaches: using a large set of training data and building a bigger model.\nBefore we look into the results, let’s first talk about the model architecture. Their model consists of two parts: variational autoencoder (VAE) and transformer."
  },
  {
    "objectID": "posts/20221215/index.html#variational-autoencoder-vae",
    "href": "posts/20221215/index.html#variational-autoencoder-vae",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Variational autoencoder (VAE)",
    "text": "Variational autoencoder (VAE)\nThe VAE contributes to the generative nature of the model because VAEs have latent representation in the middle that is a probability distribution. Once trained, we can use this distribution to draw samples from it, providing a generative framework. To train the VAE, the authors assumed uniform prior over the latent space. The model can learn the actual prior from the transformer later to generate images that match to text input. To train the VAE, the authors used images with text captions from various sources such as Wikipedia images.\nWhat is interesting about the VAE they used is that it assumes discrete latent distribution instead of continuous. This variant of VAE is called vector-quantized VAE (VQ-VAE). The motivation is that images and texts are discrete than continuous. But this assumption comes with a major complication: a discrete space is non-differentiable (i.e., can’t back-propagate). That’s why VQ-VAE has a codebook, which is essentially a look-up table where a discrete representation is associated with a codebook vector. To be accurate, this paper used a variant of VQ-VAE called dVAE where they made this look-up as a weighted average to further smooth out the space.\nThis VAE also acts as a dimensionality reduction technique because the discrete latent space the authors used has a resolution of 32x32 instead of 256x256, the resolution of the original training images. This brings compression benefit so that the transformer doesn’t have to memorize extremely long sequence but a sequence of length 1024 (=32*32)."
  },
  {
    "objectID": "posts/20221215/index.html#transformer",
    "href": "posts/20221215/index.html#transformer",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Transformer",
    "text": "Transformer\nOnce the VAE is learned, we can abandon the uniform prior assumption and use transformer to learn the actual prior. Transformers help image generation by pixel-wise prediction in an autoregressive way. For instance, given the sequence of previous pixels, the transformer can predict what the next pixel would look like.\nOnce the transformer is trained, when we give a text prompt to the model, the transformer makes predictions for the image latents (32x32 space) in an autoregressive way. Once we have all predictions, we use the dVAE codebook to lookup the vectors and generate the image. Since we can sample the sequence in a new way, we can generate multiple images. The authors used a top k approach to return the best images by ranking the generated images from a candidate pool based on the scores from OpenAI’s CLIP model, which represents how well the images match the caption.\nThe transformer has 12 billion parameters and a good chunk of the paper is dedicated to all the tricks the authors came up with to fit the model in GPU."
  },
  {
    "objectID": "posts/20221215/index.html#discussion",
    "href": "posts/20221215/index.html#discussion",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Discussion",
    "text": "Discussion\n\nAre the results representative enough?\nMost of us were somewhat disappointed by the authors’ model validation. Figures 3 and 4 in the paper gave some idea of how realistic the generated images are but we were not sure whether these were cherry-picked or not because the spectrum of images the model can generate is so wide. Figure 7 showed results from human evaluators. Most of them said the authors’ model was more realistic than the competitors’. Aside from the ethical issues surrounding hiring mturk workers, we thought the number of mturk workers was small (5 people) and the number of images they evaluated was small as well.\n\n\nWhy not investigate model failures?\nWhat was more interesting to us was Fig. 8, the CUB dataset which have images of birds. The example images here looked worse than others and the authors speculated that this was due to the detail-oriented text information of images, which might have been lost during the compression in dVAE. This was a plausible explanation but we wanted to see more in-depth investigation on model failures. There are numerous examples of terrifyingly looking images of hands generated by DALL·E because apparently it keeps failing at generating images of humans hands with five fingers.\nWe also discussed the lack of investigation on model failure from an ethical and responsible AI perspective. If OpenAI was going to publish a public API for a model like this, which would have varying degrees of socio-technical impact (look at all the issues ChatGPT has been creating these days), it would have been more responsible for them to test the model’s capacity more thoroughly and rigorously before rolling it out.\nWe found a model card from their repository and it was disappointingly short and did not address any possible ethical and social ramifications that would be caused by the model.\n\n\nValidity of the scoring metrics\nThe authors used FID and IS scores (generated by the CLIP model) to assess how well the images reflect the text input. The scores were used to rank a pool of candidate images and the model returned top k results. We questioned the validity of the decision behind using these scores because they are model-dependent, which means they are training-data-dependent. Plus, there was no mention of (at least) a qualitative comparison between the training datasets of this paper and the CLIP paper. This made us question the reliability of the CLIP model scores. It might have been interesting to see a batch of images that were ranked high (or low) so that we could judge the validity of the scores and understand the model behavior better.\n\n\nQualitative contribution\nAs in other deep learning papers, it was difficult for us to understand which decisions they made led to their results and advancement. For instance, they highlighted the larger training dataset and the larger model size. What was the measurable impact of each, and which one was more important? Similar to this, it would have been nice if they had some guidance on model tuning and hyperparameter selection to inform other researchers on model architecture design.\n\n\nReproducibility and novelty\nTo be blunt, the main highlight of this paper seemed to be the scale. They were able to use bigger datasets with a bigger model. But let’s be honest, how many academic institutions or companies are able to afford to train a model with 12 billion parameters? Especially without proper model inspection, how can we understand the model properly when we can’t reproduce it easily? Although there were certain elements of novelty especially on their tricks of utilizing GPU resources to train the model, if the scale is the main factor of success, can we really call this as a novel invention?"
  },
  {
    "objectID": "posts/20221215/index.html#final-thoughts",
    "href": "posts/20221215/index.html#final-thoughts",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Final thoughts",
    "text": "Final thoughts\nThanks to the paper, we learned that VQ-VAE and transformer together can generate images from text inputs. However, we questioned the results and model validation especially due to the lack of investigation on model failure. We also thought about ethical aspect of this model being available in public. Just because it belongs to computer vision, which tends to amuse general audience, it does not mean that it is exempt from any social responsibility. And in deep learning with image and speech data, it is often the case that model validation is often looser than tabular data used in industries with higher stakes such as health care, finance, or risk assessment. That said, we would like to learn more about other techniques mentioned in the paper to have a deeper understanding of how they work."
  },
  {
    "objectID": "posts/20230223/index.html",
    "href": "posts/20230223/index.html",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: “Machine Learning Approaches to Real Estate Market Prediction Problem: A Case Study\nPresenter: Athula\nAttendees: Hannah, Hongsup,Kate"
  },
  {
    "objectID": "posts/20230223/index.html#why-this-paper",
    "href": "posts/20230223/index.html#why-this-paper",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "Why this paper",
    "text": "Why this paper\nIn my work, I deal with real estate data. The data consists of various attributes of real estate properites, some of which follow the ebbs and flows of the mutable economy. I came across this paper while perusing arxiv, and I was compelled by its abstract: using real estate data that spanned 10 years from a single county in Florida, the research compares four ML models that make predictions about whether or not a property’s appraised price matches its eventually listed price. Since the data spans such a long time, the researchers accounted for fluctuations in socio-economic factors over time in the model; this facet was of particular interest to me to conceptualize my own work and I was interested to learn about the authors incorporated these factors in machine learning algorithms to make predictions about the real estate market.\nWhile this abstract seemed reasonably promising to understand the application of ML in real estate evaluation, when I read the paper more closely, I found myself questioning many of the approaches the authors took to answer their central question. Therefore, this blog is structured such that it summarizes the authors’ work as outlined in this paper and offers alternative considerations from our group’s discussion."
  },
  {
    "objectID": "posts/20230223/index.html#the-data",
    "href": "posts/20230223/index.html#the-data",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "The data",
    "text": "The data\nThe authors wanted to build and compare models for predicting home prices. They posit that previous literature addressed home pricing with hedonic regression models, which are linear regression models with specific focus on price of goods, i.e. property price, but that this work did not necessarily account for broader socio-economic factors in their models. With the modeling conducted in this paper, the authors attempted to model home pricing accounting for such factors. However, instead of predicting home pricing as a continous variable as with regression, the authors chose to make home prediction a binary classification problem. The data used in this research consist of ~94,000 rows of publicly available real estate sale data from a single county in Florida, Volusia County.\n\nOutcome variable\nThe way the authors defined the binary outcome variable for this research was interesting: they took the final sale price of the home and compared it to the appraised government price of the home, assigning the variable 1 if the property’s selling price is above the appraised price (which they call high price), and 0 if not (called low price); reasoning on why this designation was useful to classify was left unsaid. More importantly there was no information on how or when the properties were initially appraised by the government, which makes the comparison to final sale price unclear. Government appraisals are technically known as a home’s assessed values and differ from appraised values. Assessed valuations are made to determine yearly property tax rates for a home in a given area; this is generally determined by the broad characteristics of a home and the taxed values of homes in the region. An appraised valuation of a home, on the other hand, is done by an appraiser who does a more thorough check of the features of a home, like its style and appliances, and determines a price for the current market price of your home. Thus, it is possible the authors of the paper were using home assessment prices instead of appraisal prices. The authors don’t provide a tally on how many of the properties are classified as high price, though the authors do show a plot (see paper Fig. 2) that suggests that the total count of high price properties have steadily increased in the 10 years of this data. But despite this fact, appraisal year or sale year are not predictor variables the authors considered.\nAnother way the author could have approached the outcome variables was to have compared the initial listing price to the final sale price. In fact, in the abstract the authors tout that the listing price is baseline against which the final sale price would be compared, but the final models differ from this initial assertion. Regardless, this binary outcome variable falls short because the magnitude of difference between any two price comparisons would be lost; a high price differential of $100k would be treated the same as a lower price differential of $5k.\n\n\nPredictor variables\nFor the predictor variables, the authors chose 21 features or columns from the data for consideration for the models. However, some of the variables were not considered in the right format: parid (property identifier), nbhd (neighborhood code), yrblt (year built), zip21 (ZIP code of area), sale_date (sale date), and luc (property class) were all variables that the authors encoded as continuous variables but should have been treated as categorical variables (see Table 1). Furthermore, the variable sale_date (sale date of the property) was not separated into its component month or year values and instead was used as a single datetime value. From our group’s discussion about this paper, it was also suggested that the authors could have considered time variables in terms of seasons or quarters, as home sales typically vary by such cadences.\nBecause, this data spanned approximately 10 years, the authors attempted to account for the market fluctuations in those years by incorporating economic factors of gross domestic product (GDP), consumer price index (CPI), producer price index (PPI), housing price index (HPI), and effective federal funds rate (EFFR). But, it was unclear where the authors collected these values from and how exactly they matched this collected information to the real estate data. There was little discussion about why these variables were chosen out of the many other indices available, and little information about how these indices were good markers for the central question. Furthermore, it was unclear whether all of these ecoonimic factors were actually available at the time of prediction when the model is used for prediction."
  },
  {
    "objectID": "posts/20230223/index.html#data-culling",
    "href": "posts/20230223/index.html#data-culling",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "Data culling",
    "text": "Data culling\nThe authors took their initial dataset and used a myriad of approaches to reduce and refactor the data. First, the authors correlated the predictor variables to the outcome variables and excluded variables which were not correlated; however, they did not account for variables that are highly correlated with one another (see paper Fig. 5) which is cause for multicollinearity in how the authors interpret the importance of features in the models subsequently. Furthermore, later on when the authors introduce the voting classifier, they deem that “the method performs best when the predictors are as independent from each another as possible.”\nNext, the authors perform a set of preliminary modeling, using the dataset in random forest and XGBoost models and examining the resulting feature importance of those model predictions to make decisions about which of the variables are used in the final models. Every step of this interim modeling process was unclear, thus making it hard to give any credit to the authors’ interpretations of the variable importance charts (Fig. 6-8). Regardless, the authors used these outcomes to further enmesh the predictor variables, using a technique called ‘mean encoding’ to merge together highly ranked variables, creating two new variables which they term F1 and F2. Mean encoding seems to be a technique for encoding variable identities into a model that accounts for how those variables interact with the outcome variables. This type of encoding of variables, in addition to the method of feature selection of predictor variables through interim modeling of the data before processing those select features to a bigger model, is an approach that is teeming with data leakage."
  },
  {
    "objectID": "posts/20230223/index.html#algorithm-predictions-and-results",
    "href": "posts/20230223/index.html#algorithm-predictions-and-results",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "Algorithm predictions and results",
    "text": "Algorithm predictions and results\nFinally, the authors get to their model comparison stage. At this point of the paper it was unclear why exactly these models were being compared against one another. The ultimate question the authors laid out was out of focus and it was unclear what exactly model comparison of the data-leaked dataset would bring.\nThe authors went on to compare the dataset with random forest, XGBoost, voting classifier, and logistic regression classifiers. The authors said they used a 10-fold cross validation method (separate to the 5-fold cross validation method they did for the preliminary modeling for the feature selection step). While hyperparameter tuning is briefly discussed, the authors don’t provide any details on what models were tuned or the parameters. The authors evaluate differences between these classifiers namely by using accuracy, precision, and recall. However, why the authors chose these metrics to evaluate their high-low classifier is unclear, and how it served their goals. Is it more important for the classifier to catch correctly labeled lower labels, or higher labels? And if it’s better at one rather than the other, what impact would it have for the market?\nBy this point, the research question was not sufficiently motivated, and the data was ‘double-dipped’ and overly engineered. In fact, in presenting the comparisons between algorithm performance, the authors showcase how the engineered data performs better at classification of the task, which is of no surprise because the data features were initially chosen because they were better at classifying the task (see paper Fig. 20, 21).\nThe seeming conclusion the authors draw from this section that XGBoost was a superior classifier, but considering all of these factors above, this conclusion did not follow logic. Perhaps a more logical conclusion could have been reached had the authors employed a linear regression model instead; in this way they could have predicted the final sale price as a continous variable instead of a binary variable one as they attempt to do here."
  },
  {
    "objectID": "posts/20230223/index.html#final-thoughts",
    "href": "posts/20230223/index.html#final-thoughts",
    "title": "Reviwing a Case Study About Real Estate Market Prediction",
    "section": "Final thoughts",
    "text": "Final thoughts\nThis paper’s central question is unclear, and deviated from the beginning to end. The paper switched from comparing listing home prices to sold home prices, to appraisal home prices (which were likely assessed home prices) to sold home prices, to figuring out which model is best. If the authors had a more structured story on the questions they wanted to answer, the baseline with which they wanted to compare outcomes, and shored up methods on those comparisons, this would have been a more compelling read. As it stands, however, it is not possible to draw any conclusions from this paper."
  },
  {
    "objectID": "posts/20230126/index.html",
    "href": "posts/20230126/index.html",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Leakage and the Reproducibility Crisis in ML-based Science\nPresenter: Kate\nAttendees: Athula, Hongsup, Joel, Kate, and Saina"
  },
  {
    "objectID": "posts/20230126/index.html#why-this-paper",
    "href": "posts/20230126/index.html#why-this-paper",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Why this paper?",
    "text": "Why this paper?\nData leakage is a problem that we have all encountered as data scientists and scientific researchers. We just never knew how big! I was drawn to this paper because it attempts to quantify the impact of data leakage. It presents the unique view of considering the impact on ML-based science and shows how widespread and problematic data leakage is and leads to exaggerated claims of predictive performance. However, the frequency of data leakage and the impact of making these types of mistakes could be just as high or higher for ML-based industry applications. As a mixed group of ex ML-based science practitioners and current ML-based industry practitioners, I thought it would be insightful to discuss the eight types of data leakage identified, the proposed mitigation strategy of filling out model info sheets for ML-based science, and if the presented solution is also reasonable for a variety of ML-based industry applications."
  },
  {
    "objectID": "posts/20230126/index.html#defining-the-leakage-and-the-reproducibility-crisis-in-ml-based-science",
    "href": "posts/20230126/index.html#defining-the-leakage-and-the-reproducibility-crisis-in-ml-based-science",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Defining the leakage and the reproducibility crisis in ML-based science",
    "text": "Defining the leakage and the reproducibility crisis in ML-based science\nThis first thing this paper does is narrow the scope of relevant literature to only papers that are used for ML-based science. The authors define ML-based science as only papers “making a scientific claim using the performance of the ML model as evidence.” Furthermore, research findings are termed reproducible “if the code used to obtain the findings are available and the data is correctly analyzed.” Lastly, data leakage is a “spurious relationship between the independent variables and the target variables that arises as an artifaction of data collection, sampling, or pre-processing strategy.” Based on these three definitions, 20 papers with data leakage in 17 fields were found to impact a total of 329 ML-based research papers. These authors make three explicit contributions: 1) present a unified taxonomy of eight types of data leakage that lead to reproducibility issues, 2) propose the use of model info sheets to identify and prevent leakage, 3) quantify the impact of data leakage on model performance of civil war predictions.  \nThe first thing we noticed when looking at Table 1 was that several of the fields are areas with potential high-stakes such as medicine, clinical epidemiology, neuropsychiatry, medicine, and radiology. Engineering applications were excluded from this analysis but several fields sounded very close to this industry such as software engineering and computer security. We didn’t feel that ML-based science should be differentiated from industry-based ML applications. If anything, we felt that data leakage in industry applications may be  harder to detect because they lack a formal peer review process and could have a bigger impact because these models are deployed in the real world."
  },
  {
    "objectID": "posts/20230126/index.html#data-leakage-taxonomy",
    "href": "posts/20230126/index.html#data-leakage-taxonomy",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Data leakage taxonomy",
    "text": "Data leakage taxonomy\n\nL1.1 No test set: This was shocking to us. How does the peer review process allow these papers to be published? Hopefully, in the future reviewers will be more demanding of ML practitioners. This is also the most common type of data leakage identified in Table 1.\nL1.2 Pre-processing on training and test set:  We felt that this was a loaded category. There are many different ways for this type of data leakage to come about. For example, during imputation, under/oversampling, and encoding. For clarity, we thought this category could have been subdivided.\nL1.3 Feature selection on training and test set: This is the second most common type of data leakage found in Table 1.\nL1.4 Duplicates in dataset: This seems more like an error than a type of data leakage.\nL2 Model features that aren’t legitimate: We spent the most time discussing this category. We found it hard to think about proxy variables because they require a lot of domain knowledge. Instead, we like the definition of a feature the model would not have access to when making new predictions.\nL3.1 Temporal leakage: This is when future data is included in model training.\nL3.2 Non-independence between training and test samples: This can be caused by resampling the same patient or geographic location. This type of leakage can also be due to the interpretation of the results. This occurs when there is a mismatch between the test distribution and the scientific claim.\nL3.3 Sampling bias in test distribution: This is choosing a non-representative subset for analysis.\n\nIn general, categories L1.1-4 are clear cut and should be identifiable and avoidable. L2 is difficult because it depends on domain knowledge. Due to the nature of collaboration the researcher with this expert domain knowledge may not be the individual performing the ML analysis. Similarly, L3.1-3.3 are also difficult to detect and avoid if the researcher is using previously collected data and not designing an experiment and collecting data from scratch. In industry, data is often given to ML practitioners and there is no option to immediately collect more data or change the collection methodology. If this is the case, data leakage could be undetectable in many industry applications as ML practitioners are working with the best data at hand. In addition, there are limited strategies for identifying and mitigating these three types of data leakage."
  },
  {
    "objectID": "posts/20230126/index.html#model-info-sheets-to-mitigate-data-leakage",
    "href": "posts/20230126/index.html#model-info-sheets-to-mitigate-data-leakage",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Model info sheets to mitigate data leakage",
    "text": "Model info sheets to mitigate data leakage\nWe appreciated that the authors suggested a mitigation strategy. We found the questions in the model info sheets to be a useful sanity check for any ML practitioner. However, the level of detail requested from the info sheets would likely only be filled out if peer reviewed journals required it.  Even if journals require model info sheets as part of the peer review process, we agree with the authors that there are still limitations. Most notably, the claims in the model info sheets can’t be verified without the code, computing environment, and data. There are some scientific research fields where authors might be unwilling or unable to disclose all information due to privacy concerns. In addition, there is nothing to prevent authors from lying or not having the ML expertise to fill out the model info sheet correctly. Since ML has become more accessible to non-ML experts, subtle cases might not easily be noticed by the author or reviewer.\nIn industry, data privacy is also a concern but ML practitioners within a company could use these model info sheets to identify data leakage in industry based applications. However, there is no way to enforce this across companies or for a user of an industry based ML application to determine if the results are sound. Industry based ML is growing and there are often many groups within a company experimenting with ML. From what we have seen, there are no established within-company standards let alone between-company standards that would require the use of model info sheets to prevent data leakage.\n\n\n\nFigure 1. A comparison of reported and corrected results in civil war prediction papers published in top political science journals."
  },
  {
    "objectID": "posts/20230126/index.html#civil-war-predictions",
    "href": "posts/20230126/index.html#civil-war-predictions",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Civil war predictions",
    "text": "Civil war predictions\nTo understand the impact of data leakage the authors present a case study on civil war predictions. They identified 12 papers on the topic that provided the code and data. Of the 12 papers identified, four claimed superior performance of complex ML models compared to logistic regression. When all types of data leakage were identified and fixed, the complex models did not outperform logistic regression (Figure 1). Figure 1 shows how the performance metric is reduced after data leakage is corrected. In particular, we noticed that model performance of the simple logistic regression model was not impacted as much by data leakage compared to the complex (tree-based) ML methods. We wish the authors had speculated on why."
  },
  {
    "objectID": "posts/20230126/index.html#final-thoughts",
    "href": "posts/20230126/index.html#final-thoughts",
    "title": "Leakage and the Reproducibility Crisis in ML-based Science",
    "section": "Final thoughts",
    "text": "Final thoughts\nData leakage is a big issue that is not highlighted enough. The authors focused their efforts on identifying data leakage in ML-based science. While we appreciate them proposing a solution to identify and prevent data leakage, we think filling out model info sheets is a big ask and unlikely to be a common practice unless required by the peer review process. In addition, there are efficacy issues with model info sheets and it will be impossible to substantiate each claim. Industry ML applications are likely facing similar challenges with data leakage as ML-based science but these effects will be even more challenging to detect and quantify."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Why ML Journal Club",
    "section": "",
    "text": "In machine learning (ML) community, numerous research papers and new tools come out everyday. For an individual ML practitioner like myself, it is impossible to check every paper and tool, and it is also difficult to know what works and what doesn’t. There are several weekly ML newsletters that provide a summary but I still need to carve out time after work to digest it. Besides, I prefer reading the papers in depth to examine how they exactly work and whether there are any flaws. But of course reading and critiquing papers on a regular basis requires discipline and dedication.\nThis is where a journal club can be useful. A group of people reading and dissecting papers together and having a discussion about them is certainly more fun and educational than doing everything alone. In a journal club, participants usually take turns to present a paper, which makes things easy. We can also hold each other accountable so that we as a group read papers on a consistent pace. Finally, the gathering itself becomes a good networking and information-sharing opportunity.\nI have gather a group of my fellow ML practitioner friends in Austin that I have known personally for several years. We are a diverse group of engineers and researchers with different interests and backgrounds. I am really excited to participate in this journal club with these brilliant friends to critique papers together, share our own practices at work, and socialize."
  },
  {
    "objectID": "posts/20230622/index.html",
    "href": "posts/20230622/index.html",
    "title": "Let’s Verify Step by Step",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Let’s Verify Step by Step\nPresenter: Akshata\nAttendees: Hongsup, Meghann, Saina"
  },
  {
    "objectID": "posts/20230622/index.html#why-this-paper",
    "href": "posts/20230622/index.html#why-this-paper",
    "title": "Let’s Verify Step by Step",
    "section": "Why this paper?",
    "text": "Why this paper?\nThis is an interesting new paper by OpenAI that discusses how we can apply the same principles we use to solve math problems to AI. It’s interesting to see how a process based supervision approach is superior to outcome based supervision approach."
  },
  {
    "objectID": "posts/20230622/index.html#paper-summary",
    "href": "posts/20230622/index.html#paper-summary",
    "title": "Let’s Verify Step by Step",
    "section": "Paper summary",
    "text": "Paper summary\nThe paper evaluates different approaches to solving a dataset comprising of math problems. With this approach, they trained the model to get the right answer and but also “think” through the problem to arrive at the right answer. The authors mention that there are numerous benefits to this approach:\n\nModel reasoning is explainable as each step of the process is provided a probability score (positive, negative and neutral)\nWrong steps that would lead to incorrect outcomes are identified early on in the training process\nWrong steps that would lead to the correct solution are also identified and discouraged\nModel could be trained to reason and model its behaviors that matches human values which leads to reduction in hallucinations, false information and potentially dangerous outcomes.\n\nThe authors then attempted to showcase superior model performance with a few datasets which we then discussed."
  },
  {
    "objectID": "posts/20230622/index.html#discussion",
    "href": "posts/20230622/index.html#discussion",
    "title": "Let’s Verify Step by Step",
    "section": "Discussion",
    "text": "Discussion\n\nWhy apply language modeling for math problems? Some of us thought a logic-/rule-based approach for creating math solutions might be smarter and more efficient than using a language model because it will requires a massive amount of training data.\nWe also thought multi-step math solutions pose an interesting question because it makes it very clear (hence easy) to identify the correctness of individual steps in inference and deduction, compared to other language problems.\nSome of us questioned why the authors didn’t have enough mention of reinforcement learning although we eventually agreed that the paper’s main focus is about how to define the reward function properly.\nMany of us thought the idea of the process model is very similar to explainable AI (XAI) or interpretable AI. This is of course related to how we need to conduct value assessment when a multi-step solution reaches a correct answer with incorrect steps. This would be a big problem for XAI.\nSpeaking of value assessment, we had a question about the audience of these LLM-generated math solutions because they were extremely detailed, which we assumed are for students. Whether LLMs can tailor solutions based on the level of audience knowledge would be an interesting question to look into.\nWe had a length discussion about the “neutral” label because this would be where human labelers subjectivity will matter a lot. It would’ve been nice if the authors mentioned this more. Also, we were disappointed that the paper didn’t have any mention of who the human labelers were because they were the ones who contributed to the training data.\nCompared to other deep learning papers, we appreciated that the authors did attempt to include many details of the models even though like many deep learning models, we felt the explanation insufficient. For instance, it wasn’t perfectly clear of the multi-step solutions are fed as input to the model during training.\nWe had a brief discussion of model poisoning attacks because ChatGPT and its likes collect user data and we wondered how it can conduct quality control of user input. At the end of the paper, we sensed that the authors can’t have full knowledge of every training data, and then we wonder how they can be sure of what kind of data is used to generate output.\nFigure 3 and similar figures that are followed by this somewhat puzzled us because there was no clear explanation of what the majority voting baseline was. Besides, Figure 3 shows that when the solutions are simple, there wasn’t much performance between the two models. Also, model performance was generally lower when the solution was simple. If LLMs can’t solve simple math problems, we wonder their positive findings matter or not.\nFigure 4 shows performance that is quite low (close to 50%) and we weren’t entirely sure whether this is an acceptable level of performance or not.\nSome of us criticized the use of the term “alignment tax”, which makes AI safety quite negative, which is essentially requirements for any public-facing AI applications.\nUnder 6.3, the authors claim that there wasn’t “clear sign of memorization”, but they did not provide a clear definition of this clear sign. Are they talking about overfitting? Regardless, this requires quantifiable evidence.\nThe multi-step learning model made us think about implementing course-correct steps in model training. For instance, using a callback function, if we can somehow detect the model is on a clearly wrong path, how can we pull it back so that we don’t waste compute resource and train the model in a more efficient way? Some of us thought it might have to do with devising a more interactive and granular cost function."
  },
  {
    "objectID": "posts/20221027/index.html",
    "href": "posts/20221027/index.html",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI\nPresenter: Hongsup\nAttendees: Athula, Hongsup, Kate, and Saina"
  },
  {
    "objectID": "posts/20221027/index.html#why-this-paper",
    "href": "posts/20221027/index.html#why-this-paper",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Why this paper",
    "text": "Why this paper\nMany ML articles exist but few talk about how the sausage gets made. They are often based on toy or clean benchmark datasets, which are quite different from what we get in real world. That’s why I enjoy reading survey papers. They interview people in the field like us and try to address common pain points to find a broader picture.\nFor the past several years, I have been noticing a trend in ML community. Many practitioners tend to ignore data quality but instead put all their efforts into models and algorithms. I find it very troubling because many problems in real-world ML are caused by data-related issues. “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI talks about this pattern based on the interviews from dozens of ML practitioners all over the globe."
  },
  {
    "objectID": "posts/20221027/index.html#data-cascades",
    "href": "posts/20221027/index.html#data-cascades",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Data cascades",
    "text": "Data cascades\nThe paper focuses on “data cascades,” a series of compounding negative events due to data-related issues. The authors say the problems are wildly prevalent (92% reported experience at least one type of data cascades). During the discussion, we thought about sampling bias because those who are ignorant of these problems may not be able to recognize them. Indeed, they are often opaque because there are no clear indicators and often discovered later. Data cascades often lead to technical debt and harm to beneficiary communities. They can sour relationships between stakeholders and in extremely cases, force ML practitioners discard entire datasets. Figure 1 shows the schematic of the data cascades. We thought the figure wasn’t particularly informative because too many arrows exist between the points. We hoped that the authors explained why there aren’t arrows between certain points.\n\n\n\nFigure 1. Data cascades in high-stakes AI"
  },
  {
    "objectID": "posts/20221027/index.html#high-stake-domains",
    "href": "posts/20221027/index.html#high-stake-domains",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "High-stake domains",
    "text": "High-stake domains\nData cascades are more critical in high stake domains such as landslide detection, suicide prevention, and cancer detection. There are several reasons:\n\nMore ML applications are deployed in these domains where more direct humanitarian impact exists.\nThis impact can be disproportionate towards vulnerable communities.\nIt is often very challenging to acquire high quality data in these domains.\nThe problems frequently require more multidisciplinary approach.\n\n\n\n\nTable 1. Summary of participant demographics; Domain\n\n\nThe authors find that the problems are due to human factors. Unfortunately solutions have been focusing on other issues such as database, legal, or license. To gather firsthand experience in the field, the authors interviewed 50+ ML practitioners all over the world, ranging from the US to India and African countries, and from founders to developers. Table 1 summarizes various high-stake domains. It was fascinating for us to learn that ML is used in areas as landslide detection, poaching prevention, or regenerative farming. It would have been more interesting to see how data cascades create specific negative consequences in some domains."
  },
  {
    "objectID": "posts/20221027/index.html#data-cascade-triggers",
    "href": "posts/20221027/index.html#data-cascade-triggers",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Data cascade triggers",
    "text": "Data cascade triggers\nThe authors introduce three triggers that cause data cascades.\n\nPhysical world brittleness\nPhysical world changes over time and thus often ML systems can’t produce robust results. Data drifts due to hardware (measurements) and environmental changes are commonly mentioned in ML Ops literature. In high-stake domains, they become more pronounced because training data are very limited and policy or regulation changes can impact the ML systems in various ways.\n\n\nInadequate application-domain expertise\nMost ML practitioners are not equipped with domain knowledge. All of us admitted that our academic background does not match to the domain that we work in. Even though close collaboration between domain experts and ML practitioners is always emphasized, in practice the authors find that domain experts are often detached from the larger impact of the applications. The authors explain two specific types of problems:\nSubjectivity in ground truth: Areas such as insurance claim approval or medical imaging for cancer detection involve highly specialized and often subjective decision-making. To build a reliable and robust ML system, it is necessary to standardize the decision-making criteria and find consensus. However, ML practitioners are asked to rush through the development process, and thus do not have time to address it.\nPoor domain expertise in finding representative data: ML practitioners often start building ML applications without involving domain experts much because the practitioners simply believe that data are reliable. However, because they lack domain knowledge, practitioners make incomplete assumptions, which results in disparity between data collection and deployment. This often leads to poor and unreliable model performance.\n\n\nConflicting reward system\nData collection and any data-related work are often considered non-technical and undervalued. The situation gets worse for frontline workers because they are asked to collect and curate field data on top of their existing responsibilities but they are not well compensated.\n\n\nPoor cross-organizational documentation\nMetadata about data collection, quality, and curation are also often missing. Some good practices the authors suggest involve keeping good documentation on reproducible assets such as data collection plan, data strategy handbooks, design documents, file conventions, field notes, and so on."
  },
  {
    "objectID": "posts/20221027/index.html#broader-context",
    "href": "posts/20221027/index.html#broader-context",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Broader context",
    "text": "Broader context\nData cascades discussion can be extended to bigger problems in ML community.\n\nIncentives and currency in AI\nBecause of the low incentives, data-related work are not rewarded or even tracked. This makes us difficult to get buy-in from stakeholders. The situation is similar in academia as well. Most practitioners and researchers focus on developing algorithms but they rarely mention or work on data. The title “Everyone wants to do the model work, not the data work” was a verbatim from an interviewee. Unfortunately, we were all able to relate to this quote.\n\n\nData education\nMost ML or data science curricula lack any mention of data quality or ethics. They use toy datasets or very clean benchmark datasets. As experience ML practitioners, we wholeheartedly agreed with this finding. We lamented that these courses do not prepare students with practical knowledge because one never works with clean datasets in real world. Some of us have experienced this pattern firsthand because we have been interviewing candidates for an ML practitioner position and found that the candidates who only worked with clean datasets (or primarily worked on algorithms) often lack basic practical ML knowledge.\n\n\nData bootstrapping\nData bootstrapping describes ML practitioners’ use of other data sources such as established data services or existing datasets to create their own dataset. For most of us, it was surprising to learn that many ML practitioners in high-stake domains in countries from Global South had to collect data from scratch. We agreed that challenges in data collection and lack of access to quality data would create inequality between countries."
  },
  {
    "objectID": "posts/20221027/index.html#how-to-address-data-cascades",
    "href": "posts/20221027/index.html#how-to-address-data-cascades",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "How to address data cascades",
    "text": "How to address data cascades\nThe authors introduce several ways of addressing the problem of data cascades. They introduce the concept of “data excellence”, an effort to “focus on the practices, politics, and values of humans of the data pipeline to improve the quality and sanctity of data, through the use of processes, standards, infrastructure and incentives”.\n\nFrom goodness-of-fit to goodness-of-data\nThe first is to use the right metric to evaluate data quality. Many ML practitioners use model performance metrics such as accuracy and RMSE to evaluate data quality. Some of us had a similar experience. We had to argue that model metrics shouldn’t be used to make a decision on data-pipeline and data-quality features. We hope that the authors can introduce specific examples of goodness-of-data metrics in the future.\n\n\nIncentives for data excellence\nThere are several ways to address the low incentives of data work. First, journals and conferences should require dataset documentation, provenance, and ethics as mandatory disclosure. Second, organizations should reward data-related work similar to how good software engineering is rewarded. Finally, partnership between stakeholders can nurture data excellence by sharing the reward on data-related work such as data collection, anomaly identification, and model verification.\n\n\nEducation and visibility\nThe authors argue for real-world data literacy in AI education, which includes training on data collection, infrastructure building, data documentation, data sense-making, and data ethics and responsible AI in general. Increasing the data visibility in ML lifecycle is important as well; implementing good monitoring system is one of the most important ML Ops practices anyway."
  },
  {
    "objectID": "posts/20221027/index.html#final-thoughts",
    "href": "posts/20221027/index.html#final-thoughts",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Final thoughts",
    "text": "Final thoughts\nSome of us found this paper vindicating because we have been advocating for data quality at work and had to fight for the attention it deserves. The paper helped us share our own practices at work that address data-related issues especially data collection, curation, and post-deployment data problems. Even though we generally agreed with authors’ suggestions, some of us wanted something more specific, like a case study. Overall, we found the paper interesting and insightful. We thought it would be beneficial to read the paper with our colleagues at work to start a discussion for data excellence."
  },
  {
    "objectID": "posts/20230720/index.html",
    "href": "posts/20230720/index.html",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types\nPresenter: Saina\nAttendees: Brian, Hari, Hongsup, Meghann"
  },
  {
    "objectID": "posts/20230720/index.html#why-this-paper",
    "href": "posts/20230720/index.html#why-this-paper",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "Why this paper?",
    "text": "Why this paper?\nProduct discovery plays a pivotal role in the success of an online store, as it not only fuels revenue generation but also exerts a profound influence on the overall customer experience. In the vast landscape of e-commerce, where countless products are bought and sold through platforms like Amazon, the task of efficiently arranging and presenting product information for indexing and seamless search functionality stands as a continuous and formidable challenge.\nAnother challenge arises due to the lack of a unified language in product descriptions. Given that retailers curate the information, the distinct characteristics and attributes of products are predominantly delineated within titles or descriptions, rather than being uniformly collected. Consequently, the extraction of this information necessitates delving into unstructured textual data.\nResearchers at Amazon address this issue by introducing a pipeline named AUTOKNOW. This pipeline comprises a set of machine learning algorithms and techniques that create taxonomy, recognize products, and ultimately construct a product knowledge graph. This graph serves the purpose of organizing catalog information and enhancing product discovery through indexing and downstream functionalities of search and product recommendation."
  },
  {
    "objectID": "posts/20230720/index.html#summary-of-techniques",
    "href": "posts/20230720/index.html#summary-of-techniques",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "Summary of techniques",
    "text": "Summary of techniques\n\n\n\nFigure 2: AutoKnow architecture, containing ontology suite to enrich product ontology and data suite to enrich product structured data.\n\n\nThis paper delves into the intricacies of the Retail Product Domain, where challenges compound due to the rapid influx of new products and the extensive range of associated attributes, such as color that can have many values. This domain presents a diverse spectrum of attributes spanning various product types, even as some attributes are shared among products. To address the inherent complexity, the study narrows its focus to four domains: grocery, health, beauty, and baby. Eligible products for the study must have at least one monthly page view. It is observed that products are characterized by dozens to hundreds of attributes per product, with the attribute count varying from 100 to 250 across different product types.\nThey start organizing the product taxonomy by focusing on types and hypernyms relationships, and curating attributes and synonyms for each type. They also focus on a knowledge graph (KG) that consists of triples in the form of (subject, predicate, object), where subjects are entities with IDs belonging to various types, objects can be entities or atomic values, and predicates signify relationships. This study concentrates on broad graphs where the topic type is products. Product types are organized hierarchically, forming a taxonomy. The paper assumes input sources, including a product catalog with attributes and customer behavior logs, query/purchase logs, customer reviews and question/answer. Intuitively, most product types appear in title, and search queries.\nThe taxonomy enrichment process consists of two steps. Firstly, a type extractor is trained to identify new product types from titles and queries, utilizing an open-world tagging model. Secondly, type attachment is achieved through a binary classification task, employing a graph neural network (GNN) module that captures signals from customer behaviors. This GNN-based approach refines type representations and combines them with semantic features for classification. Distant supervision is applied for model training, utilizing existing taxonomy and customer behavior data for positive and negative labels.\nThe Data Imputation component addresses the Structure-sparsity challenge by enhancing coverage through structured value extraction from product profiles. This process involves extracting new (attribute, value) pairs for each product from its profiles. State-of-the-art techniques using BIOE sequential labeling and active learning have been effective for solving this at the type-attribute level, as demonstrated in Equation (1) with BiLSTM and CRF.\n\\[(y_{1},y_{2},...y_{L}) = \\text{CRF}(\\text{BiLSTM}(e_{x_{1}},e_{x_{2}},...,e_{x_{L}}))\\]\nHowever, scalability to handle large numbers of product types and attributes is an issue. To overcome this, the paper introduces a novel taxonomy-aware sequence tagging approach conditioned on product type. The model’s predictions are influenced by the product type’s pre-trained hyperbolic-space embedding, preserving hierarchical relationships within the taxonomy. The approach also employs multi-task learning to enhance the identification of tokens indicating product type and tackle catalog misclassification or missing type information through shared BiLSTM training for sequence tagging and product categorization.\nNext step is the relation discovery which is based on product attributes where various sets of attributes apply to different product types, and only a fraction significantly affects shopping decisions. Identifying applicable and important attributes for thousands of types is essential. Formally, the task involves determining if an attribute applies to a product type and its importance for purchasing decisions. This paper employs classification and regression models—specifically, Random Forest—to assess attribute applicability and importance. The models utilize seller behavior (attribute value coverage and frequency in profiles) and buyer behavior (frequency in search queries, reviews, Q&A). Annotations from in-house and MTurk sources are used for model training. The approach distinguishes between subtle differences, such as attributes required for coverage but not always pivotal for decisions. The trained models are applied to all attribute-type pairs for decision-making.\nThis model is designed to figure out if a suggested attribute value matches the context provided by product descriptions and the product categories they belong to. The model takes raw input, which is a combination of words from product descriptions, product categories, and the suggested attribute value. These words are turned into embedding vectors that capture their meaning, source, and position in the text. The model then processes these embeddings using a multi-layer transformer to create a representation that summarizes all this information. This summarized representation is used to calculate a score, which tells us how well the suggested attribute value fits in with the context. To train the model, they use examples from the product catalog. They generate both correct and incorrect examples, and the model learns to distinguish between them. During testing, the model is applied to different attribute-value pairs, and those with low scores are considered as incorrect matches."
  },
  {
    "objectID": "posts/20230720/index.html#final-points",
    "href": "posts/20230720/index.html#final-points",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "Final Points",
    "text": "Final Points\nAt the time of this paper presentation, Autoknow is deployed in production in collecting over 1B product knowledge facts for over 11K distinct product types. The approach is validated by demonstrating the expansion of existing Amazon ontology by 2.9 times where precision is improved by 7.6% and recall by 16.4%. The collection suite of technologies are Apache spark distributed system, tensorflow for deep learning, amazon deep graph library for graph neural network for taxonomy. The relationships are derived using Spark ML, and an AWS SageMaker instance is used for training the imputation component.\nFinally, they mention an important lesson, which is that sometimes they need to move away from the product tree and consider multiple parents. In other words, a single sub-type cannot be clearly defined in many instances."
  },
  {
    "objectID": "posts/20230720/index.html#discussion",
    "href": "posts/20230720/index.html#discussion",
    "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
    "section": "Discussion",
    "text": "Discussion\nFirst, Saina gave us a great introduction about the problem domain, knowledge discovery. She said extracting and understanding attributes is essential in knowledge discovery in retail, which has been relatively easy before the era online shopping because merchants usually provided well-organized product catalogs. However, widely variable free text formats started emerging with the rise of e-commerce. Search engines have been trying to automatically extract attributes from the text input and to build a good taxonomy to create hierarchical relationship between products. Since products come and go constantly, this process requires interactive process of attribute extraction and taxonomy building. As the authors mentioned in the paper, some of the main challenges are to clean, curate, and understand the data, and to correct errors and prevent abuse of attributes.\nWe spent a lot of time trying to understand the sequential labeling problem the paper used extensively. The authors frequently cited their OpenTag paper, which they used to automatically generate training labels. These tags are used in many stages of their model pipeline including taxonomy enrichment and type attachment. The algorithm takes a sequence of text tokens and return a sequence of BIOE (“beginning”, “inside”, “outside”, “end”) labels. Then the authors used the returned tags as true labels for subsequent ML algorithms (distant supervision and regular supervision). We initially thought this BIOE labeling system came from the OpenTag paper but we later learned that it was based on a 2015 paper about bidirectional LSTM-CRT (conditional random field) for sequential tagging, which the authors used in this paper as well. In short, the final CRF layer in this deep learning model returns a probability for every plausible label sequences (sequence likelihood). We thought it would have been nice for the authors to give a quick overview of the algorithm and address potential risk of ML-generated labels as true labels in downstream ML modeling.\n\n\n\nA BI-LSTM-CRF model (Huang et al. 2015)\n\n\nWe also discussed other algorithms used in the paper to improve our understanding. First, we suspected that the regression problem they attempted to solve was based on MTurk workers’ subjective level of attribute importance (not a feature importance of an ML model) even though we doubted the choice of using only 6 MTurk workers. We also discussed the difference between semi-supervised and weak-supervised learning models: the former is about “based on what is already labeled, label some more” and the latter is about “based on your knowledge, label some more”. Finally, we talked about hyperbolic space embedding, which is excellent at preserving graph distances and complex relationships in very few dimensions Finally, as the authors mentioned at the end, we thought it would make sense to have a single model that takes care of data imputation and cleaning because in this paper, the cleaning is more of correction in imputed values.\nWe generally enjoyed reading this paper because it uses a plethora of various ML algorithms extensively, which gave us a good opportunity to get to know them. However, some of us questioned how this model pipeline can be maintained especially with the arrival of new information. We thought some stages of the pipeline might be more resilient to changes (such as the attribute importance estimation) but taxonomy changes may not. As we’ve seen in deep learning papers many times, we also questioned the validation aspect because the number of samples used in validation (a few hundreds triples) was much smaller than the actual dataset (one billion triples)."
  },
  {
    "objectID": "posts/20230329/index.html",
    "href": "posts/20230329/index.html",
    "title": "Visualization in Bayesian workflow",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Visualization in Bayesian workflow\nPresenter: Hongsup\nAttendees: Kate, Saina"
  },
  {
    "objectID": "posts/20230329/index.html#why-this-paper",
    "href": "posts/20230329/index.html#why-this-paper",
    "title": "Visualization in Bayesian workflow",
    "section": "Why this paper?",
    "text": "Why this paper?\nThis paper summarizes types of data visualization that we can use in Bayesian modeling and inference. This is also a good overview of how to do Bayesian modeling properly, including validating results. The fact that the main author is one of the maintainers of the stan package, is another motivating factor."
  },
  {
    "objectID": "posts/20230329/index.html#paper-summary",
    "href": "posts/20230329/index.html#paper-summary",
    "title": "Visualization in Bayesian workflow",
    "section": "Paper summary",
    "text": "Paper summary\nGiven a problem, we incorporate our scientific knowledge into a causal (generative) model to simulate how the relevant variables are produced (input and output). Researchers need more than null hypothesis because it doesn’t talk about how your observation is generated. We can use a DAG as a scientific causal model and data generation process can be expressed in a generative model, which is often accompanied with Bayesian data analysis (BDA). BDA is particularly useful because we can simulate data from the model directly to design and debug during inference. To effectively estimate a posterior distribution, we need computational methods such as MCMC and others. One may say Bayesian might be an overkill but it’s extremeley useful for typical modeling problems such as measurement error, missing data, latent variables, and regularization. Again, it’s also generative!\nThe paper uses data visualization to express the followings: - Exploratory data analysis to come up with a proper model - Prior predictive distribution check to check model’s assumption - MCMC computational check to evaluate the sampling process - Posterior predictive check to validate inference process\nThis paper is based on R’s bayesplot but there are several python equivalents to this such as pymc, arviz, and numpyro. It uses a global air polllution dataset (pm2.5 particles) measured from satellite images. The goal of modeling is to predict the level of pm2.5 from the images. Hence, this is a regression problem. Fig. 1 shows the linear trend between the two variables of interest but also shows how sparse the data is depending on groups.\n\nExploratory data analysis (EDA)\nEDA is essential to understand and capture features and heterogeneity of data. The data pattern helps building a group-up modeling strategy to address the imbalance and sparsity of data. The authors emphasize that the top-down approach in typical ML communities these days is to throw everything into a non-parametric procedure, which can severely overfit. Fig. 2 shows that simple regression works pretty well, especially when the group identity is taken into account, which means we need a hierarchical approach.\n\n\n\nFig. 4: Visualizing the prior predictive distribution\n\n\n\n\nPrior predictive check\nInstead of using a non-informative or uniform prior, weakly informative prior is always recommended, which takes into account modeler’s perspective. In the paper, we assume that the target varialbe follows a normal distribution defined by a mean and a \\(\\sigma\\) where the mean is a linear function of input variable (satellite data) and linear coefficients, which also have priors (0 mean and std (\\(\\tau\\))).\nPrior predictive checks are useful to visualize the impact of our assumption for prior definition. If we use a vague prior (very wide range, Fig. 4a), ranges from the sample don’t match the observation. Fig. 4b shows a much tighter prior where the simulated data points still overestimate but are in a much reasonable range. Obviously, tighter and sensible priors are better.\n\n\n\nFig. 5: Diagnostic plots for Hamiltonian Monte Carlo\n\n\n\n\nMCMC diagnostics\nSuccess of Hamiltonion Monte Carlo (HMC) depends on how smooth the posterior distribution is; if not smooth, HMC proposal diverges from the true trajectory, which may signal that the trajectories are stuck. Healthy MCMC samples, shown as a bivariate plot in Fig. 5a, shouldn’t have obvious patterns. The funnel shape there is due to \\(\\beta_{11} \\sim N(0, \\, \\tau_{1}^{2})\\) where small \\(\\tau_{1}\\) means \\(\\beta_{11}\\) distribution is narrow. The parallel co-ordinate plot (Fig. 5b) also shouldn’t have any particular structure.\n\n\n\nFig. 9: Graphical check of leave-one-out cross-validated probability integral transform (LOO-PIT)\n\n\n\n\nPosterior predictive check\nIf a trained model has a good fit, generated data from the model should follow observations. Posterior predictive checking is mostly qualitative but it’s effective to compare empirical and simulated values (Fig. 6). Fig. 7 shows checking whether samples from models captures other statistics such as skewness (kurtosis) and Fig. 8 shows how we can evaluate whether samples from models capture summary statistics such as median Fig. 9 shows using visualization that checks whether leave-one-out cross-validation (LOO-CV) predictive cumulative density function is uniform or not, similar to the idea of a K-S test.\n\n\n\nFig. 10a: Model comparisons using leave-one-out (LOO) cross-validation. The difference in pointwise ELPD values obtained from PSIS-LOO\n\n\n\n\nModel comparison\nWhen comparing models, Bayesian data analysis allows detailed examination of individual data points on a given model. We can use cross-validated LOO predictive distribution to do so; it shows the distribution of a data point from a model that’s built without that data point (i.e., LOO). We can use expected log-predictive densities (ELPD), which is essentially the mean of the log probability of each data point i, computed with posterior that omits the point i (the bigger the better). We use Pareto-smoothed importance sampling (PSIS) to compute this metric (we don’t have to fit the models N times). Once we have ELPD value for every data point of a model, we can repeat this for all the models we have and make comparison (Fig. 10a).\n\n\n\nFig. 10b: Model comparisons using leave-one-out (LOO) cross-validation. The \\(\\hat{k}\\) diagnostics from PSIS-LOO for Model 2\n\n\nSimilarly, we can compute \\(\\hat{k}\\) as well which represents degree of influence of a specific observation. High value means this data point is “unexpected”, meaning that it is likely to be an outlier or the model struggles to make valid prediction for this data point."
  },
  {
    "objectID": "posts/20230329/index.html#discussion",
    "href": "posts/20230329/index.html#discussion",
    "title": "Visualization in Bayesian workflow",
    "section": "Discussion",
    "text": "Discussion\nWe had a lengthy discussion about the choice of prior and how tricky it can be. As the authors mentioned in conclusion, we were also slightly worried about double-dipping the data when running prior predictive checks and potential data leakage. It was also interesting to share our own experience on Bayesian inference ranging from dealing with prior assumptions, model comparison, to decision-making with uncertainty. But we all agreed that Bayesian data analysis is more empowering for us modellers compared to the typical top-down approach in ML where we often don’t have any generative models about data. We also agreed that Bayesian data anlsysis is absolutely more powerful when we suffer from a small data problem.\nBut we also some downsides of Bayesian data analysis too. It’s difficult to scalable and someone we ML practitioners are not the domain experts and without the domain expertise, it’s difficult to come up with a good DAG. Due to the nature of Bayesian analysis where we don’t often make a point-estimate summary, we appreciated that the paper spent a good amount of time discussing how to summarize a posterior distribution. We also discussed the importance of loss function when decision making with uncertainty.\nIn general, we liked the paper but we thought it fell slightly short because it wasn’t focusing on understanding scientific mechanism but rather on predictive modeling nature of Bayesian analysis. When it comes to model comparison particularly, we thought it’s important to evaluate the structure of the model too in addition to evaluating the goodness of fit. For instance, if the model performance varies across the regions, the way we compare the models would like to change as well, and potentially the DAGs too."
  },
  {
    "objectID": "posts/20230831/index.html",
    "href": "posts/20230831/index.html",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Constitutional AI: Harmlessness from AI Feedback\nPresenter: Hongsup\nAttendees: Brian, Kate, Kshitij, Joel, Renée, Sanhita, and Sarah"
  },
  {
    "objectID": "posts/20230831/index.html#why-this-paper",
    "href": "posts/20230831/index.html#why-this-paper",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "Why this paper?",
    "text": "Why this paper?\nI’ve been interested in AI safety and responsible AI for several years, and the rise of LLMs has certainly increased stakes. Currently there is an intense arms race among several major tech companies and Anthropic is one of them. They recently published a paper about their LLM and claims to adopt a more cautious approach than others by designing their LLMs to minimize potential harm. They call this constitutional AI (CAI) because their LLMs follow a constitution of principles. I wanted to learn more about how they teach their algorithm to follow these principles."
  },
  {
    "objectID": "posts/20230831/index.html#background-knowledge",
    "href": "posts/20230831/index.html#background-knowledge",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "Background knowledge",
    "text": "Background knowledge\nTo understand this paper properly, it’s better to be familiar with AI alignment problem and reinforcement learning from human feedback (RLHF). AI alignment is about aligning AI systems’ design and values with humanity values such as honesty. Norbert Wiener, an AI researcher back in 1960s, described the AI alignment problem as following:\n\n“If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively (…) we had better be quite sure that the purpose put into the machine is the purpose which we really desire.”\n\nRegarding RLHF, I recommend this great summary from Hugging Face. At its core, RLHF is an attempt to distill human feedback into a model (often called reward or preference model) when training LLMs. This is because human feedback is often expensive to collect and difficult to generalize. An important thing to know is that to train this model, practitioners often use ranked preference modeling where human annotators are asked to rank generated text ouptuts from language models. The assumption here is that this approach may mimic human preference of certain responses over others. And because of this preference approach, RLHF papers use Elo scores, a rating system originated from chess to show a player’s winning rates, to evaluate model performance.\nIn terms of the alignment values, Anthropic chose honesty, helpfulness, and harmlessness. The detailed definition of these concepts are described in one of their previous works:\n\n\n\n\n\n\nAskell et al. 2021, What are Helpfulness, Honesty, and Harmlessness?\n\n\n\nHelpful\n\nThe AI should make a clear attempt to perform the task or answer the question posed (as long as it isn’t harmful). It should do this as concisely and efficiently as possible.\nWhen more information is required, the AI should ask relevant follow-up questions and obtain necessary details. It should respond with appropriate levels of sensitivity, insight, and discretion.\nIdeally the AI will also re-direct ill-informed requests, e.g. if asked ‘how can I build a website in assembly language’ it might suggest a different approach.\n\nHonest\n\nAt its most basic level, the AI should give accurate information. Moreover, it should be calibrated (e.g. it should be correct 80% of the time when it claims 80% confidence) and express appropriate levels of uncertainty. It should express its uncertainty without misleading human users.\nCrucially, the AI should be honest about its own capabilities and levels of knowledge – it is not sufficient for it to simply imitate the responses expected from a seemingly humble and honest expert.\nIdeally the AI would also be honest about itself and its own internal state, insofar as that information is available to it.\nHonesty is more objective than helpfulness and harmlessness, so more aspects of honesty training may be possible without human input. This might include calibration training on factual claims and claims about the internal state of the model, and the use of search to augment accuracy.\n\nHarmless\n\nThe AI should not be offensive or discriminatory, either directly or through subtext or bias.\nWhen asked to aid in a dangerous act (e.g. building a bomb), the AI should politely refuse. Ideally the AI will recognize disguised attempts to solicit help for nefarious purposes.\nTo the best of its abilities, the AI should recognize when it may be providing very sensitive or consequential advice and act with appropriate modesty and care.\nWhat behaviors are considered harmful and to what degree will vary across people and cultures. It will also be context-dependent, i.e. it will depend on the nature of the user query, who is using the AI assistant, and the time and place in which the assistant is being used."
  },
  {
    "objectID": "posts/20230831/index.html#motivations",
    "href": "posts/20230831/index.html#motivations",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "Motivations",
    "text": "Motivations\nThe first motivation was scaling supervision. Given that LLMs require numerous examples, it’s better to automate the supervision process and use human annotators to get more curated and high quality answers. This is a similar idea behind the preference modeling in RLHF. The authors called theirs “reinforcement learning from AI Feedback” (RLAIF, not RLHF). A more interesting motivation was building a non-evasive and yet helpful AI assistant. Many currently available AI assistants often simply refuse to answer questions to harmful prompts (e.g., simply saying “I don’t know” or “I can’t answer that”). Their model was never evasive but tried to explain the reasoning behind their negative response to harmful questions. Finally, similar to the first point, they claimed that distilling human supervision into a model could help better understand general aspects of human feedback from many crowd-workers."
  },
  {
    "objectID": "posts/20230831/index.html#the-constitutional-ai-approach",
    "href": "posts/20230831/index.html#the-constitutional-ai-approach",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "The Constitutional AI Approach",
    "text": "The Constitutional AI Approach\n\nSupervised stage\nTheir constitutional AI (CAI) consisted of two stages: a supervised stage and a reinforcement learning stage. In the supervised stage, they used a pretrained LM (“Helpful RLHF model” from their previous work) as a starting point, and red-teamed the model by presenting harmful prompts (by human workers) and sampled the responses. Then, (this is the most interesting part in my opinion!) they used natural language to ask the model to critique and revise its own response based on certain principles. Here’s an example from the paper:\n\n\n\nAppendix A. Sample critiques and revisions. The first two revisions are shown in this screenshot.\n\n\nAppendix C contains a list of principles (constitution) they used to create the critique-revision requests. The paper doesn’t talk much about how they came up with the principles but according to Anthropic’s website, the principles were based on existing documents such as Universal Declaration of Human Rights or Apple’s Terms of Service. As shown in the example above, a response can go through multiple critique-revision requests. The authors found that generally the more revisions mean less harmfulness although the first revision contributes most.\n\n\n\nFig. 5. Preference Model scores of responses and revisions from helpful RLHF models, evaluated on a set of red team prompts.\n\n\n\n\nReinforcement learning (RL) stage\nThe main idea behind this stage is identical to the RL stage in RLHF. The goal is to distill knowledge from a group of responses by training a reward model. The key difference is that these responses are now generated by a model not human (the supervised learning model from the previous stage). The authors called this reward model “feedback model” although it was a bit unclear which exact LMs they were referring to.\nAnother interesting aspect they added here was a “chain-of-thought” approach. This was inspired by Let’s Verify Step by Step, a paper we covered in a previous journal club meeting. Here, after getting a response, the authors added the natural-language phrase “Let’s think step by step” to generate richer intermediate responses from the model.\n\n\n\nAn example from Appendix E.2. Chain-of-Thought Prompts for RL-CAI.\n\n\nIn their final model (Reinforcement Learning-Constitutional AI with Chain of Thought, or RL-CAI w/ CoT), the authors found a major improvement in harmlessness Elo score without compromising the helpfulness Elo score much. Note that in the figure below, Elo score of 0 on the y axis (starting point of the RL model) represents the supervised learning model (SL-CAI), which means the SL-CAI model was used as initial base model for RL.\n\n\n\nFig. 8. Helpfulness (left) and harmlessness (right) Elo scores as a function of the total number of RL training sequences, as judged by crowd-workers via comparison tests.\n\n\nOne interesting aspect of the RL model the authors shared was its behavior when the model was over-fitted. They found that in this case, the response often included boilerplate language such as “you are valid, valued, and cared for.”\n\n\n\nAn example of over-trained RL-CAI model response showing boilerplate language as part of their response (e.g. “you are valid, valued, and cared for”)."
  },
  {
    "objectID": "posts/20230831/index.html#broader-impacts",
    "href": "posts/20230831/index.html#broader-impacts",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "“Broader Impacts”",
    "text": "“Broader Impacts”\nAt the end of the paper, the authors emphasized that natural language feedback could change AI behavior and potentially increase robustness because red-teaming efforts could become more scalable (because feedback supervision can be generated by a model not humans). In section 6.2 Broader Impacts, they briefly mentioned the potential harm of the constitutional AI approach. Using natural language to change AI behavior means it will become much easier to train a malicious AI assistant especially given that this method reduces the need for human feedback."
  },
  {
    "objectID": "posts/20230831/index.html#group-discussion",
    "href": "posts/20230831/index.html#group-discussion",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "section": "Group discussion",
    "text": "Group discussion\nFirst of all, compared to other on-average deep learning papers, we found this paper easier to read. We also appreciated that the authors provided numerous examples. We could see they also tried to do a better job at providing many details of the model although still not enough, especially when they were referring to their previous work.\n\nHarmlessness as an easy alignment\nSome of us suspected that the authors might have chosen harmlessness as their main target of alignment perhaps because it was relatively easier to tackle than other alignment such as helpfulness. The authors did mention the tension between harmfulness and helpfulness in the paper in that an AI assistant could become harmful if it was too eager to be helpful (e.g., providing a detailed answer to a prompt about how to commit a crime). We talked about more nuanced alignments (such as humor) and whether it would be possible to use natural language to change model behavior. Some of us pointed out that harmlessness could be relatively easy because diametrically opposed examples could be easily found in languages.\n\n\nDoes chain-of-thought count as an explanation?\nMany of us were skeptical of treating responses from the chain-of-thought approach as explanations. Most examples shown in the paper seemed reasonable but given that what the model did with a CoT request was nothing more than just generating more detailed responses, we agreed that we should not treat them as step-by-step deductive reasoning. We were interested in looking at CoT examples that might sound gibberish and redundant. I personally also thought this was one of the examples of ML practitioners anthropomorphizing a behavior of an ML model.\n\n\nNo more efforts to understand the model\nMost of us were surprised that the approach of using natural language to critique and revise its own behavior seemed to have worked. Before I read the paper, I was very curious to know what constraints they came up with and how they tried to model complex social concepts such as justice and harm. The fact that their approach seemed to be working was interesting but this also meant that we are in an era where we are no longer trying to change the model behavior at a lower level, but rather we treat the language models as if they are something we don’t completely understand. This paper was completely missing explanations of why this approach actually worked. From my perspective, as other numerous deep learning papers, this paper was saying “we tried this, we don’t exactly know why it works, but it seems to work.”\n\n\nWhy diminish the work and labor of human annotators?\nIn the abstract and at the end of the paper, the authors kept saying their model was trained “without any human labels identifying harmful outputs.” All of us agreed that this was an exaggeration. To train the supervised model, they needed human annotators, and once the supervised model was ready, then they were able to generate AI feedback. Given that Anthropic is a for-profit company that sells AI assistant software, highlighting that the maintenance cost of their system is cheaper than others because human feedback can be replaced by AI feedback, could be a good marketing strategy, but at the cost of marginalizing human labor.\n\n\nHow did you come up with the principles?\nIn Appendix C, the authors provided a comprehensive list of all principles they used to generate critique-revision responses. These were the core principles that guided the model behavior but the authors didn’t mention much about how they curated the list. Some principles were general and others were more specific to particular types of harms such as racism and misogyny. We suspected that there had been an iterative curation process to narrow the list down to these 16 principles specifically. If these were the main drivers of changes in model behavior, we think they should have provided much more details.\n\n\nThe double-edged sword\nFinally, some of us were disappointed that the authors didn’t elaborate much on the potential harm of their approach. They spent a lot of time talking about harmlessness of their algorithms and yet they really fell short when talking about social impacts of their model, especially regarding lowering the barrier for experimenting with LMs and automating supervision by removing human further out of the loop. Particularly for the former, we agreed that it wouldn’t be surprising to see, in near future bad actors take advantage of this approach and come up with a highly toxic, malicious, and harmful AI assistant."
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "Blogging",
    "section": "",
    "text": "Our blog uses quarto platform and it is hosted via GitHub Pages. This is a short guide for uploading blog posts using quarto."
  },
  {
    "objectID": "quarto.html#set-up",
    "href": "quarto.html#set-up",
    "title": "Blogging",
    "section": "Set up",
    "text": "Set up\n\nInstall quarto. The VS Code option is straightforward.\nClone the blog repository.\nCreate an issue (title example: “Upload a blog post for the Feb 2023 meeting”) and assign yourself to the issue.\nCheck out a new branch for the issue (branch name example: “16-write_blog”)."
  },
  {
    "objectID": "quarto.html#write",
    "href": "quarto.html#write",
    "title": "Blogging",
    "section": "Write",
    "text": "Write\n\nCreate a folder in /posts. The name of the new folder should be the date of the meeting in YYYYMMDD format (e.g., posts/20221027/).\nCreate index.qmd inside the folder. This is the main file for your post. The top section of index.qmd is a YAML block with document options. You must fill out title, author, date, description, categories.\n---\ntitle: \"Paper title\"\nauthor: \"Author full name\"\ndate: \"YYYY-MM-DD\"\nimage: \"This image will show on the main page (this is optional).jpg\"\ndescription: Short description of the post. It will show on the main page.\ncategories: [paper]\n---\nBefore writing the body, please include the following right after the YAML block:\n:::{.callout-note}\n- Paper: [paper title](link to original paper)\n- Presenter: [first name](linkedin profile)\n- Attendees: [first name 1 (alphabetical order)](linkedin profile), [first name 2](linkedin profile), ... \n:::\nTo give everyone some context, start the body with “Why this paper” paragraph.\nYou can include any image files in the same folder as index.qmd.\nRun quarto preview &lt;file name&gt; to preview the rendered look of your post on the website."
  },
  {
    "objectID": "quarto.html#render-and-review",
    "href": "quarto.html#render-and-review",
    "title": "Blogging",
    "section": "Render and Review",
    "text": "Render and Review\n\nRun quarto render once you are done. This automatically generates and modifies html files in the /docs and the main folders.\nAdd all the changes, commit, and make a merge request to main.\nAssign the attendees of the meeting as reviewers, and go through a review process. Leave a message on Slack about the merge request.\nUpdate the manuscript based on reviewers’ comments.\nOnce approved, merge the branch, close your issue ticket, and delete the remote branch.\n\nCurrently the blog is hosted via gh-pages branch (not main). Thus, after main is updated, we will rebase it to gh-pages, and the blog will be automatically updated with a new post."
  },
  {
    "objectID": "notes/2023-02-23.html",
    "href": "notes/2023-02-23.html",
    "title": "Meeting notes",
    "section": "",
    "text": "Meeting notes\nAll of us found a lot of problems with this paper. Here are some discussions points.\n\nFrom the abstract, having conclusion as “model X is the best” is honestly not publishable. It’s simply stating that they did model tuning, which is really a few lines of code and doesn’t mean anything more.\nThe fact that they are framing this as classification, not regression is highly problematic and misleading. Making this as classification ignores the distribution and magnitude of difference between appraisal and selling price.\nData leakage in feature space: we suspect some features may contain information about the target or they may not be available at the time of prediction\nEven though they didn’t have any class imbalance (class ratio was almost 1:1), according to their graphs, it’s possible that class imbalance would have changed over time.\nIt was not clear what the goal of the paper was especially on what they want to do with the ML model. We had a discussion about whether the authors should’ve used listing price instead of appraisal. They also didn’t seem to consider the common sense that government appraisal might lag and how their model will account for it.\nThe dataset is limited to a single county and yet in conclusion they made overreaching conclusions.\nIt’s also unclear how they got the socioeconomic data, which is critical because they seemed to emphasize them in Introduction.\nTheir feature representation was highly problematic (Tab. 1) mainly because they treated categorical features like zip code as numeric values.\nThere were also new features (such as month) they later mentioned in the paper, which were missing in their feature set table. We also suspect that they used month as numeric variable not as ordinal/categorical variable.\nThe sale date feature also could’ve been divided into more granular and meaningful features such as year, month, quarter, school season, etc.\nEven with feature importance calculation and feature selection, we couldn’t find what they exactly want to do with the prediction and how they want to use it. What exactly can become actionable items is also missing. This is why we were confused about why they did correlation study because it wasn’t really used much in their modeling. Besides, we don’t know whether they are interested in building interpretable models and whether they care about conducting sensitivity analysis to understand the impact of feature value changes better.\nUsing too many highly correlated features and their impact on calculating feature importance should have been investigated.\nTheir feature importance calculation seems to have data leakage issue.\nMean encoding idea is not clear. Also, it seems to be prone to data leakage.\nVoting classifier description doesn’t mention what the voter algorithms are.\nThey didn’t have an independent test set and their cross validation didn’t consider temporal aspect of the data (=more data leakage).\nTheir lack of discussion on business objectives becomes problematic again when they do model comparison with various metrics. We don’t know which metric makes most sense. They seemed to have used accuracy.\nAt this point, we agreed that this paper seemed like it’s written for the sake of showcasing ML (aka “we can build an ML for this”) not for any research and business goals.\nAgain, we think the model selection should’ve considered interpretability, which is related to the actual use case of the model, which the authors never discussed.\nThey also didn’t mention any baseline and so we don’t know the achieved score is enough or not.\nMassive overreach is shown in their conclusions.\nAs a future work, they suggested building better ML predictors. First, this paper is not about creating a novel algorithm, so they can’t make this claim and second, it’s often the case that authors simply suggest building a better model even though the problem is not the model but rather it’s them making so many basic mistakes such as data leakage, feature misrepresentation, and lack of use case."
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Tips",
    "section": "",
    "text": "If you are interested in joining the journal club, please send an email to hongsup.shin@pm.me. We will send you an invite to our Discord channel. Check out our LinkedIn group as well."
  },
  {
    "objectID": "readme.html#logistics",
    "href": "readme.html#logistics",
    "title": "Tips",
    "section": "Logistics",
    "text": "Logistics\n\nWe meet every last Thursday of the month between 6 pm and 8 pm. Meeting announcement is done through our Discord channel. The schedule can change based on the members’ availability.\nWe usually meet at one of the many branches in Austin Public Library."
  },
  {
    "objectID": "readme.html#presentation",
    "href": "readme.html#presentation",
    "title": "Tips",
    "section": "Presentation",
    "text": "Presentation\n\nWe take turns to present a paper.\nWhen announcing what to read, we encourage the presenter to write a few sentences on “why this paper” to motivate others.\nIt doesn’t have to be a paper. It can be a blog post, newspaper article, or even a tool documentation page. We can also use the journal club as a brainstorming session if the presenter wants to discuss a specific work problem they want to talk about.\nEvery presenter will be introduced in the About page with a short bio. Participants will be mentioned in the posts.\nPresenters don’t have to know everything about the paper, but they should have enough understanding to lead the discussion.\n\n\nGuidelines for choosing papers\nReading papers takes a great deal of time. To respect everyone’s time, please consider the following criteria when choosing papers:\n\nPapers that are published or peer-reviewed\nPapers about ideas that are discussed frequently in ML community\nPapers with high citation counts\nPapers from renowned research groups\nPapers that are not too niche in terms of their domain\nPapers that are not too long\nPapers that have many interesting discussing points; this means you can choose a bad paper intentionally\n\nFeel free to share any papers of interest in Discord. We will regularly update our Reading List page to track them."
  },
  {
    "objectID": "readme.html#communication",
    "href": "readme.html#communication",
    "title": "Tips",
    "section": "Communication",
    "text": "Communication\n\nAll major correspondence is made in the Discord channel.\nMeetings are announced in the Discord channel and LinkedIn group.\nPresenters should give others at least 2 weeks to read the paper.\nPlease notify on Discord about whether you can come next time before the meeting."
  },
  {
    "objectID": "readme.html#blogging",
    "href": "readme.html#blogging",
    "title": "Tips",
    "section": "Blogging",
    "text": "Blogging\n\nPresenters are welcome to write a blog post about the paper they presented.\nThe blog is hosted through GitHub pages from our repository and we use the Quarto platform.\nYou can write about anything in the post as long as it captures some of our discussion during the meeting.\nBlogging is optional. If the presenter is too busy to write the post, Hongsup will write a post instead, or he will suggest the presenter to write it together."
  },
  {
    "objectID": "posts/20240328/index.html",
    "href": "posts/20240328/index.html",
    "title": "Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "",
    "text": "Note\n\n\n\n\nPaper: Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations\nPresenter: Hongsup\nAttendees:"
  },
  {
    "objectID": "posts/20240328/index.html#why-this-paper",
    "href": "posts/20240328/index.html#why-this-paper",
    "title": "Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "Why this paper?",
    "text": "Why this paper?\nFresh out of academia and at my first job, I remember being surprised by the great power I was able to wield as the main data scientist in the team. A few lines of my code could easily have cascading impact on business decisions. And (as is often the case) since management didn’t care much about technical details, this power gave me a sense of grave responsibility, which was honestly often terrifying. To this day, this sense is something I try to remind myself of, especially because ML systems are getting more complex and we still have very little accountability for ML. So in a way, every ML practitioner is the first line of defense. And this resonsibility is more critical if one is working in an high-impact domain such as healthcare.\nThese days it feels like ML is all about LLMs and AI assistants. But algorthmic bias is still widespread, and unfortunately it’s even more overshadowed by this massive hype. This seminal paper from 2019 identified racial bias in a healthcare algorithm and discussed the problem of label choice bias. I find this paper still relevent because this bias can easily sneak when building datasets and algorithms. Since the paper is a few years old, it will be interesting to discuss what’s happened since then."
  },
  {
    "objectID": "posts/20240328/index.html#paper-summary",
    "href": "posts/20240328/index.html#paper-summary",
    "title": "Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "Paper summary",
    "text": "Paper summary\n\nthis is a famous paper in Fair ML field\ncited frequently\n“algorithm audit” paper\ninspirational: uses many different (and creative) approaches to prove the main point"
  },
  {
    "objectID": "posts/20240328/index.html#author-ziad-obermeyer",
    "href": "posts/20240328/index.html#author-ziad-obermeyer",
    "title": "Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "author: ziad obermeyer",
    "text": "author: ziad obermeyer\n\nuc berkeley; school of public health; harvard med schoool, history/science, philosophy masters, physician\nphysician specliazed in emergency medicine\nnightingale foundation; open sourcing medical imaging\nhe was working at a hospital that gave access to the algorithm\nalso Sendhil Mullainathan: U chicaco economist\n\n\nAbstract\n\nAt a given score (algorithm estimate), Black patients are much sicker than White\nThe algorithm uses healthcare cost as target (proxy), not the sickness itself\nLabel choice bias: the choice of convenient, seemingly effective proxies for ground truth can cause algorithmic bias\n\n\n\nIntroduction\n\nMany examples\n\nthese examples still exist\n\n\n\nDifficulty of empricial investigation\n\nproprietary algos -&gt; must work “from the outside” (creative, audit)\n\nstill limiting: difficult to understand how and why\ntraining data, objective function, prediction method\n\n\n\n\nthis study: high-risk care management system\n\nrich data, live, scaled\nrisk prediction tool\nused for 200M in the US\nhigh-risk care management system\n\nprovide care for complex medical needs (expensive)\neffective in saving cost and improving outcome\nresource is limited: use alorithm to prioritize who will benefit the most\nhealthy care systems rely on algirhtms exensively** for this\n\n\n\n\nSupp: algorithm in context\n\nobamacare -&gt; pressure on hospitals to reduce health care cost\ncauses development of “high risk care management” programs\n\nearly identification of high risk patient -&gt; reduce cost (like ER visit)\n\n\n\n\nfigure out “who will benefit the most”\n\ndifficult causal inference problem; we need to estimate individual treatment effeects\nassumption: “those with the greatest needs will benefit the most”\ntarget: “future health care needs”\n\n\n\nrare opportunity\n\naccess to everything: data, objective function, prediction\nnot a unique algorithm: widely used\n\n\n\nimplications of the study\n\nmany analogies (risk assessment tools)\nimportance and opportunity of studying bias in healthcare algo (“in its own right”)\n\n\n\n\nData and analytic strategy\n\nData\n\nlarge academic hospital\n2013-2015\nprimary: white vs. black (self-reporting)\n\ndisadvantage: lack of intersectional racial/ethnic identities\n\nsample size\n\nblack: 6k (12k patient-years)\nwhite: 40k (90k patient-years)\n“patient-years”: 1 patient-year = data collected for a patient in a calendar year\n\n\n\n\nalgorithm scores\n\ngenerated for each patient during the enrollment period\n&gt; 97%: automatically “identified” as enrolled (not guaranteed for enrollment)\n&gt; 55%: referred to their PCP for their opinion\nprogram run 3x a year: patients whose scores &gt; threshold (97%)\n\n\n\nmain methodology\n\nfairness metrics\n\ncalibration\nstatistical parity (demographic parity): college admission rate between white vs. black\nnot possible to optimize all metrics simultaneously\nUT Austin presence: Maria De-Arteaga\n\nthis study: calibration\n\nE[Y|R, W] = E[Y|R, B]\n\nY: interest\nR: risk score\n\n\nwhat we do: EXAMINE CALIBRATION R vs. H, R vs. C\n\nWE DO NOT KNOW HOW \\(R\\) IS CALCULATED!\n\\(R_{i, t}\\): risk score given patient i in year t given \\(X_{i, t-1}\\) (commercial algo)\n\\(H_{i, t}\\): realized health\n\\(C_{i, t}\\): actual cost\nhow well R is calibrated across race for H (or C)\n\n\n\n\nmeasuring H and C\n\nH: health record data, diagnoses, lab results, vital signs, etc.\nC: insurance claims on utilization, outpatient/ER visit, hospitalization, health care costs\n\n\n\nsupp: algo implementation\n\nlabel: C (total medical expenditures)\nfeatures: insurance claim data\n\ndemographics\ninsurance type\nICD-9 (international classification of diseases code) diagnosis and procedure codes\nprescribed medications\nencounters, categorized by type of service (surgical, radiology, etc.)\nbilled amounts, categorized by type (outpatient, dialysis, etc.)\n\nalgo is run 3 times a year\n\n\n\n\nHealth disparities conditional on risk score\n\noverall measure of health: comorbidity score\n\ntotal number of chronic illnesses over year t = a measure of medical complexity; correlates w/ treatment effect of care management programs\nmore biological measure (from eletronic health records; EHR) are also used: measure for severity (not just presence)\n\nthese data need to be cleaned extensively; usually not available for model building; hard to validate\nin the analysis, the authors analzed the difference in these values between black and white\n\n\n\n\nFig 1A: mean comorbidity vs. risk score\n\nat the same risk score, Blacks have significantly more illness than Whites\n97%: black 4.8, white 3.8\n\n\n\nMeaning of this score discrepancy\n\nless healthy blacks = more healthy whites -&gt; same score -&gt; substantial disparities in program screening\nuse counterfactual to measure this\n\ncounterfactual: no racial bias\n\n\n\n\nhow the counterfactual works!\n\nat a given percentile (e.g., 97th percentile) based on the risk score (cost algo)\nidentify whites who are above, identify blacks who are above and below cutoff\nif the healthiest white-upper’s comorbidity &lt; sickest black-lower’s comorbidity (= if the healthist white-upper is healthier than the sickest black-lower)\nremove the healthiest white (white-upper)\nand give this spot to the sickest black from black-below (move them to the black-upper)\ncheck all blacks in black-below\n\n\n\nFig 2: blacks are substantially less healthy than whites\n\nacross many biomarkers\nat any levels of algorithm predictions\n\n\nAdditional robustness checks related to program effect\nDoes the program itself have an effect? 1. Predict H_{t-1} instead of H_{t}: no racial difference 2. Compare enrolled vs. unenrolled: no difference (is it though?)\n\n\n\n\nMechanisms of bias\n\nalgorithm excludes race\ntarget: cost\nFig 3A: in terms of cost, it’s well calibrated between white and black (no bias)\nwhy then?\n\nFig 3B: at a given level of health, Black generate lower costs than Whites\nBlacks also generate very different kinds of costs (Tab S2)\n\nmore costs related to ER and dialysis\nfewer inpatient surgical and outpatient specialist costs\n\n\n\n\nWhy this happens\n\npoor patients face substantial barriers to accessing health care, even the insured ones\n\ngeography, transportation\ndemands from job, child care\nknowledge of reasons to seek care\n\ncost directly affected by race\n\ntaste-based discrmination: employers’ prejudice or dislikes in an organisational culture rooted in prohibited grounds can have negative results in hiring minority workers\n\nOne of the leading causes of labor market discrimination (the other: statistical discrimination)\nemployers’ preference for employees of certain groups is unrelated to their preference for more productive employees\n\nchanges to the doctor-patient relationship\n\nblack patients would take preventive care suggested by black provider more\nblacks also have lower trust in health care system (Tuskegee study; 40 years US gov, conducting study on 400 black men; the men were not informed; most infamous biomedical research study)\ndoctors’ different perceptions of black patients: intelligence, affiliation, pain tolerance\n\n\n\nThis results in lowering health spending substantially for black patients conditional on needs; known fact for 20 years\n\n\n\nProblem formulation\n\ncost as target\n\nwidely used approach (industry wide)\n\n10 most widely used algo: supplementary\n\nnot limited to industry\n\n\n\nthere can be alternatige targets\n\navoidable future cost (narrower definition): ER cost\nmeasure of health directly\n\n\n\nproblem formulation challenge; dilemma of which label to choose\n\nproblem formulation: “turning an often amorphous concept we wish to predict into a concrete variable”\nhealth is challenging; the idea is holistic and multi-dimentionsal\nlabel choice is important; but each choice has tradeoffs\n\n\n\n\nExperiments on label choice\n\n3 labels\n\ntotal cost\navoidable cost\nhealth (comorbidity)\n\n(race is excluded in all cases)\n\n\nresults (Tab 2)\n\nthey performed similarly\nhow they built table 2\n\nselect the top k(3%) based on y_pred (row)\nsum the y_test (column) within the top k\ndivide this number by the total sum\n\nconcentration in highest-risk patients table\n\nthe columns are essentially concentraion by their target metric\ntop_k_df[outcome].sum() / holdout_pred_df[outcome].sum()\n\ntop_k_df[outcome].sum(): sum of top k (sorted by that outcome predictor) predicted values of that column label\nholdout_pred_df[outcome].sum(): sum of all predicted values of that column label\n\n\nnotice on the black fraction\n\nactive chronic condition has highest proportion\n\n\n\n\n\nRelation to human judgment\n\nhuman-algo interaction (HAI)\n\nalgo: screening tool -&gt; doctor make the decision\n&gt;55%: PCPs are informed and make enrollment decision\ndoctor’s realized enrollment decision = how doctors respond to algorithmic predictions\nref.: UT Austin presence; Min Kyung Lee (HAI)\n\n\n\nTab 3: doctors’ decision vs. algo predictions\n\n19.2% black in enrollment data (what doctors did)\n4 scenarios\n\nrandom sampling from cost model\nsampling sicker patients from cost model\njust using the cost algo\njust using the health algo\n\n1 and 3 -&gt; black patient prop. &lt; 19.2% (still very small delta)\n2 and 4 -&gt; more black patients enrolled\ndoctors can “redress” (=correct) the algo but not so much\n\n\n\n\nDiscussion\n\nlabel choice bias\n\nother examples in health: using mortality and readmission rate to measure hospital performance -&gt; penalize those serving poor and non-white pops\ncredit-scoring\npolicing\nhiring\nretail\ncan be deadly because it comes from “reasonable” choice\n\n\n\nworking on solutions together\n\nauthors contacted the algo manufacturer -&gt; they could replicate the results\nusing the same model infrastucture\nbut with new target: index that combines health and cost\n84% reduction in bias\nworking on table 3 into scalable predictor of multi-dim health issues\nthe manufacturer is an industry leader -&gt; hopeful note\n\n\n\nconclusions\n\nalgo baises are fixable\nalgo change wasn’t required\nit’s more of a data problem; label!\ndeep knowledge of domain required\nworth investing research on this"
  },
  {
    "objectID": "posts/20240328/index.html#impact",
    "href": "posts/20240328/index.html#impact",
    "title": "Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "impact",
    "text": "impact\n\n2021: FTC: Aiming for truth, fairness, and equity in your company’s use of AI\n2019: New York Regulator Probes UnitedHealth Algorithm for Racial Bias\n2024: Wyden Statement at Finance Committee Hearing on AI in Health Care\nCA, 2022: Attorney General Bonta Launches Inquiry into Racial and Ethnic Bias in Healthcare Algorithms\n\n\nOther works\n\nAn algorithmic approach to reducing unexplained pain disparities in underserved populations\nClinical Notes Reveal Physician Fatigue\n\n\n\nRecent work of his (FAccT 2023 talk)\n\nlack of data access is a huge problem: health data can be considered a public good\n\nnew clinical tools: predicting sudden cardiac arrest\nurgent need to audit many flawed algorithms\n\neven hard for FDA; difficult to do rigorous external validation (they can’t get the data)\nunsafe regulatory environment (difficulty of data access)\n\n\ndata platform for clinical AI\n\ncompany: dandelion\n\nclosing thoughts: engagement with real problems\n\nagnozing tradeoffs; better proxy? how to persuade hospitals, how to stop bias, etc.\ngenerates fascinating, unexpected new technical problems to solve -&gt; constraints -&gt; creative solutions\ndeep satisfaction\n\nbeing in the room where decisions are made\nbending the process in the right direction"
  },
  {
    "objectID": "posts/20240328/Obermeyer_2019_experiment.html",
    "href": "posts/20240328/Obermeyer_2019_experiment.html",
    "title": "Obermeyer et al. 2019 experiment",
    "section": "",
    "text": "code and data source: https://gitlab.com/labsysmed/dissecting-bias/-/tree/master/code\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%config InlineBackend.figure_format='retina'\n\nimport seaborn as sns\n\n\ndata = pd.read_csv('data_new.csv')\ndata.shape\n\n(48784, 160)\n\n\n\ndata.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\n\n\nplt.plot(data['cost_t'], data['risk_score_t'], '.', alpha=0.05)\nplt.loglog()\nplt.xlabel('Cost')\nplt.ylabel('Risk score')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nres_dir = os.path.join(os.getcwd(), 'results')\n\ndef setup(default_in_percentile=[95, 97]):\n    cohort = pd.read_csv('data_new.csv')\n    cohort = cohort[['race', 'risk_score_t', 'gagne_sum_t']]\n    dt = cohort.copy()\n    dt['risk_pctile'] = pd.cut(dt['risk_score_t'], bins=np.unique(np.percentile(dt['risk_score_t'], np.arange(0, 101))), include_lowest=True, labels=False)\n    \n    enroll_stats = np.zeros((len(default_in_percentile), 3))\n    enroll_stats = pd.DataFrame(enroll_stats, columns=['black_before', 'black_after', 'ratio'])\n    enroll_stats.index = default_in_percentile\n    \n    return {'dt': dt, 'enroll_stats': enroll_stats}\n\n\ndef exercise(default_in_percentile):\n    dt = setup(default_in_percentile)['dt']\n    enroll_stats = setup(default_in_percentile)['enroll_stats']\n    \n    for j in range(len(default_in_percentile)):\n        prior_enrolled = dt[dt['risk_pctile'] &gt;= default_in_percentile[j]]\n        prior_w = prior_enrolled[prior_enrolled['race'] == 'white']\n        prior_b = prior_enrolled[prior_enrolled['race'] == 'black']\n        \n        upperb = dt[(dt['risk_pctile'] &gt;= default_in_percentile[j]) & (dt['race'] == 'black')]\n        upperw = dt[(dt['risk_pctile'] &gt;= default_in_percentile[j]) & (dt['race'] == 'white')]\n        lowerb = dt[(dt['risk_pctile'] &lt; default_in_percentile[j]) & (dt['race'] == 'black')]\n        \n        upperw = upperw.sort_values('gagne_sum_t')\n        lowerb = lowerb.sort_values(['risk_score_t', 'gagne_sum_t'], ascending=[False, False])\n        \n        sw = 0\n        sb = 0\n        switched_count = 0\n        switched_w = pd.DataFrame()\n        switched_b = pd.DataFrame()\n        \n        while sb &lt; lowerb.shape[0]:\n            if upperw.iloc[sw]['gagne_sum_t'] &lt; lowerb.iloc[sb]['gagne_sum_t']:\n                switched_w = switched_w.append(upperw.iloc[sw])\n                switched_b = switched_b.append(lowerb.iloc[sb])\n                \n                upperb = upperb.append(lowerb.iloc[sb])\n                \n                upperw = upperw.drop(upperw.index[sw])\n                upperw = upperw.sort_values('gagne_sum_t')\n                \n                sb += 1\n                switched_count += 1\n            else:\n                sb += 1\n                switched_count = switched_count\n        \n        sampw = prior_w\n        sampb = prior_b\n        black_before = prior_b.shape[0] / (prior_w.shape[0] + prior_b.shape[0])\n        black_after = (prior_b.shape[0] + switched_count) / (prior_w.shape[0] + prior_b.shape[0])\n        ratio = black_after / black_before\n        enroll_stats.iloc[j] = [black_before, black_after, ratio]\n    \n    return enroll_stats\n\n\n\n\nThis function is used to simulate the counterfactual scenario in the paper (Fig. 1B)\n\ndefault_in_percentile = [97]\nj = 0\n\ndt = setup(default_in_percentile)['dt']\nenroll_stats = setup(default_in_percentile)['enroll_stats']\n\nprior_enrolled = dt[dt['risk_pctile'] &gt;= default_in_percentile[j]]\nprior_w = prior_enrolled[prior_enrolled['race'] == 'white']\nprior_b = prior_enrolled[prior_enrolled['race'] == 'black']\n\nupperb = dt[(dt['risk_pctile'] &gt;= default_in_percentile[j]) & (dt['race'] == 'black')]\nupperw = dt[(dt['risk_pctile'] &gt;= default_in_percentile[j]) & (dt['race'] == 'white')]\nlowerb = dt[(dt['risk_pctile'] &lt; default_in_percentile[j]) & (dt['race'] == 'black')]\n\nupperw = upperw.sort_values('gagne_sum_t')\nlowerb = lowerb.sort_values(['risk_score_t', 'gagne_sum_t'], ascending=[False, False])\n\nsw = 0\nsb = 0\nswitched_count = 0\nswitched_w = pd.DataFrame()\nswitched_b = pd.DataFrame()\n\n\ndt.shape\n\n(48784, 4)\n\n\n\ndt.head()\n\n\n\n\n\n\n\n\nrace\nrisk_score_t\ngagne_sum_t\nrisk_pctile\n\n\n\n\n0\nwhite\n1.987430\n0\n35\n\n\n1\nwhite\n7.677934\n3\n85\n\n\n2\nwhite\n0.407678\n0\n3\n\n\n3\nwhite\n0.798369\n0\n10\n\n\n4\nwhite\n17.513165\n1\n97\n\n\n\n\n\n\n\n\nupperw sorting: healthiest person on top\nlowerb sorting: sickest person on top\n\n\nupperw.iloc[sw]['gagne_sum_t'], lowerb.iloc[sb]['gagne_sum_t']\n\n(0, 12)\n\n\nDistribution of comorbidity score in white-upper and black-upper in the original data (based on the original cost algorithm)\n\nupperb_ = upperb.copy()\nupperw_ = upperw.copy()\nlowerb_ = lowerb.copy()\n\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 4))\nsns.kdeplot(upperw_['gagne_sum_t'], label='white-upper', ax=ax)\nsns.kdeplot(upperb_['gagne_sum_t'], label='black-upper', ax=ax)\n# sns.kdeplot(lowerb_['gagne_sum_t'], label='black-lower', ax=ax)\nax.set(xlabel='Comorbidity score', title='97th percentile')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nHow the swapping works: 1. at a given percentile (e.g., 97th percentile) based on the risk score (cost algo) 2. identify whites who are above, identify blacks who are above and below cutoff 3. if the healthiest white-upper’s comorbidity &lt; sickest black-lower’s comorbidity (= if the healthist white-upper is healthier than the sickest black-lower) 4. remove the healthiest white (white-upper) 5. and give this spot to the sickest black from black-below (move them to the black-upper) 6. check all blacks in black-below\n\nwhile sb &lt; lowerb.shape[0]:\n    if upperw.iloc[sw]['gagne_sum_t'] &lt; lowerb.iloc[sb]['gagne_sum_t']:\n        switched_w = switched_w.append(upperw.iloc[sw])\n        switched_b = switched_b.append(lowerb.iloc[sb])\n\n        upperb = upperb.append(lowerb.iloc[sb])\n        upperw = upperw.drop(upperw.index[sw])\n        upperw = upperw.sort_values('gagne_sum_t')\n        \n        sb += 1\n        switched_count += 1\n    else:\n        sb += 1\n        switched_count = switched_count\n\nComparing the distribution of comorbidity score between before and after\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\nsns.kdeplot(upperw_['gagne_sum_t'], label='white-upper', ax=axes[0])\nsns.kdeplot(upperb_['gagne_sum_t'], label='black-upper', ax=axes[0])\naxes[0].set(xlabel='Comorbidity score', title='Before')\n\nsns.kdeplot(upperw['gagne_sum_t'], label='white-upper', ax=axes[1])\nsns.kdeplot(upperb['gagne_sum_t'], label='black-upper', ax=axes[1])\naxes[1].set(xlabel='Comorbidity score', title='After')\n\nfig.suptitle('97th percentile', x=0.5, y=0.95)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\ndef get_concentration_metric_df(k, holdout_pred_df,\n                                y_predictors=['log_cost_t',\n                                              'log_cost_avoidable_t',\n                                              'gagne_sum_t'],\n                                outcomes=['log_cost_t', 'log_cost_avoidable_t',\n                                          'gagne_sum_t', 'dem_race_black']):\n    \"\"\"Calculate concentration of a given outcome of interest (columns) for\n    each algorithm trained label, and calculate fraction black in the high-risk\n    patient group.\n\n    Parameters\n    ----------\n    k : float\n        Top k% patients in terms of predicted risk.\n    holdout_pred_df : pd.DataFrame\n        Predictions for holdout set.\n    y_predictors : list\n        List of algorithm training label.\n    outcomes : list\n        List of given outcome of interest.\n\n    Returns\n    -------\n    pd.DataFrame\n        Concentration metric for holdout_pred_df.\n    \"\"\"\n    # define lookup for human readable headings in Table 2\n    OUTCOME_DICT = {\n        'cost_t': 'Total costs',\n        'log_cost_t': 'Total costs',\n        'cost_avoidable_t': 'Avoidable costs',\n        'log_cost_avoidable_t': 'Avoidable costs',\n        'gagne_sum_t': 'Active chronic conditions',\n        'dem_race_black': 'Race black'\n    }\n\n    top_k = int(k * len(holdout_pred_df))\n    all_concentration_metric = []  # save all rows of Table 2 to variable\n\n    # iterate through each predictor (algorithm training label)\n    # (this is each row in Table 2)\n    for y_col in y_predictors:\n        # get the predictions column name for y_col\n        y_hat_col = '{}_hat'.format(y_col)\n\n        # sort by y_hat_col\n        holdout_pred_df = holdout_pred_df.sort_values(by=y_hat_col, ascending=False)\n        # get top k% in terms of predicted risk\n        top_k_df = holdout_pred_df.iloc[:top_k]\n\n        # define dict to store calculated metrics for given y_col/predictor\n        # (each addition to the dict appends a column from Table 2)\n        concentration_dict = {\n            'predictor': OUTCOME_DICT[y_col]\n        }\n\n        # iterate through each outcome\n        # (concentration / frac black in highest-risk patients)\n        # (this is each column in Table 2)\n        for outcome in outcomes:\n            if 'log_' in outcome:\n                # for the outcomes presented on a log scale,\n                # we sum the un-logged values.\n                outcome = outcome[len('log_'):]\n\n            # define numerator of concentration metric:\n            # sum the top k of outcome\n            top_k_outcome = top_k_df[outcome].sum()\n\n            # define denominator of concentration metric\n            if outcome == 'dem_race_black':\n                # for fraction black in highest-risk patients,\n                # denominator is the n of top k%\n                total_outcome = top_k\n            else:\n                # for concentration in highest-risk patients,\n                # denominator is the total sum of the entire holdout\n                total_outcome = holdout_pred_df[outcome].sum()\n\n            # calculate concentration metric\n            frac_top_k = top_k_outcome / total_outcome\n\n            # add column to concentration_dict (row)\n            concentration_dict[OUTCOME_DICT[outcome]] = frac_top_k\n\n            # calculate standard error (SE)\n            n = len(holdout_pred_df)\n            import math\n            # SE = sqrt[ p * (1-p) / n]\n            se = math.sqrt((frac_top_k * (1-frac_top_k))/n)\n\n            # add SE column to concentration_dict (row)\n            concentration_dict[OUTCOME_DICT[outcome] + ' SE'] = se\n        all_concentration_metric.append(concentration_dict)\n\n    # convert to pd.DataFrame for pretty formatting\n    concentration_df = pd.DataFrame(all_concentration_metric)\n    concentration_df = concentration_df.set_index('predictor')\n\n    # define column order of Table 2\n    column_order = []\n    for outcome in outcomes:\n        outcome = OUTCOME_DICT[outcome]\n        column_order.append(outcome)\n        column_order.append(outcome + ' SE')\n\n    return concentration_df[column_order]\n\n\nholdout_pred_df = pd.read_csv('model_lasso_predictors.csv')\nholdout_pred_df.head()\n\n\n\n\n\n\n\n\nindex\ndem_race_black\nrisk_score_t\ngagne_sum_t\ncost_t\ncost_avoidable_t\nprogram_enrolled_t\nlog_cost_t_hat\ngagne_sum_t_hat\nlog_cost_avoidable_t_hat\nrisk_score_t_percentile\n\n\n\n\n0\n2\n0\n0.407678\n0\n500.0\n0.0\n0\n3.012857\n0.107439\n0.534697\n4\n\n\n1\n5\n0\n4.450484\n1\n123700.0\n0.0\n0\n3.985548\n2.423606\n2.113413\n69\n\n\n2\n10\n0\n0.713436\n0\n1500.0\n0.0\n0\n2.820869\n0.091171\n0.363918\n10\n\n\n3\n11\n0\n3.940887\n1\n21500.0\n500.0\n0\n3.116980\n0.788215\n0.800114\n63\n\n\n4\n13\n0\n1.919484\n0\n2800.0\n300.0\n0\n3.186166\n0.143811\n0.476029\n35\n\n\n\n\n\n\n\n\ny_hat_col = 'log_cost_t_hat'\nholdout_pred_df = holdout_pred_df.sort_values(by=y_hat_col, ascending=False)\n\n\n# get top k% in terms of predicted risk\nk = 0.03\ntop_k = int(k * len(holdout_pred_df))\ntop_k_df = holdout_pred_df.iloc[:top_k]\n\n\noutcome = 'cost_avoidable_t'\ntop_k_outcome = top_k_df[outcome].sum()\ntop_k_outcome\n\n8972400.0\n\n\n\ntotal_outcome = holdout_pred_df[outcome].sum()\ntotal_outcome\n\n38309600.0\n\n\n\nfrac_top_k = top_k_outcome / total_outcome\nfrac_top_k\n\n0.2342076137573872"
  },
  {
    "objectID": "posts/20240328/Obermeyer_2019_experiment.html#cost-is-used-as-a-label-but-it-is-not-the-risk-score",
    "href": "posts/20240328/Obermeyer_2019_experiment.html#cost-is-used-as-a-label-but-it-is-not-the-risk-score",
    "title": "Obermeyer et al. 2019 experiment",
    "section": "",
    "text": "plt.plot(data['cost_t'], data['risk_score_t'], '.', alpha=0.05)\nplt.loglog()\nplt.xlabel('Cost')\nplt.ylabel('Risk score')\nplt.show()"
  },
  {
    "objectID": "posts/20240328/Obermeyer_2019_experiment.html#figure-1b",
    "href": "posts/20240328/Obermeyer_2019_experiment.html#figure-1b",
    "title": "Obermeyer et al. 2019 experiment",
    "section": "",
    "text": "res_dir = os.path.join(os.getcwd(), 'results')\n\ndef setup(default_in_percentile=[95, 97]):\n    cohort = pd.read_csv('data_new.csv')\n    cohort = cohort[['race', 'risk_score_t', 'gagne_sum_t']]\n    dt = cohort.copy()\n    dt['risk_pctile'] = pd.cut(dt['risk_score_t'], bins=np.unique(np.percentile(dt['risk_score_t'], np.arange(0, 101))), include_lowest=True, labels=False)\n    \n    enroll_stats = np.zeros((len(default_in_percentile), 3))\n    enroll_stats = pd.DataFrame(enroll_stats, columns=['black_before', 'black_after', 'ratio'])\n    enroll_stats.index = default_in_percentile\n    \n    return {'dt': dt, 'enroll_stats': enroll_stats}\n\n\ndef exercise(default_in_percentile):\n    dt = setup(default_in_percentile)['dt']\n    enroll_stats = setup(default_in_percentile)['enroll_stats']\n    \n    for j in range(len(default_in_percentile)):\n        prior_enrolled = dt[dt['risk_pctile'] &gt;= default_in_percentile[j]]\n        prior_w = prior_enrolled[prior_enrolled['race'] == 'white']\n        prior_b = prior_enrolled[prior_enrolled['race'] == 'black']\n        \n        upperb = dt[(dt['risk_pctile'] &gt;= default_in_percentile[j]) & (dt['race'] == 'black')]\n        upperw = dt[(dt['risk_pctile'] &gt;= default_in_percentile[j]) & (dt['race'] == 'white')]\n        lowerb = dt[(dt['risk_pctile'] &lt; default_in_percentile[j]) & (dt['race'] == 'black')]\n        \n        upperw = upperw.sort_values('gagne_sum_t')\n        lowerb = lowerb.sort_values(['risk_score_t', 'gagne_sum_t'], ascending=[False, False])\n        \n        sw = 0\n        sb = 0\n        switched_count = 0\n        switched_w = pd.DataFrame()\n        switched_b = pd.DataFrame()\n        \n        while sb &lt; lowerb.shape[0]:\n            if upperw.iloc[sw]['gagne_sum_t'] &lt; lowerb.iloc[sb]['gagne_sum_t']:\n                switched_w = switched_w.append(upperw.iloc[sw])\n                switched_b = switched_b.append(lowerb.iloc[sb])\n                \n                upperb = upperb.append(lowerb.iloc[sb])\n                \n                upperw = upperw.drop(upperw.index[sw])\n                upperw = upperw.sort_values('gagne_sum_t')\n                \n                sb += 1\n                switched_count += 1\n            else:\n                sb += 1\n                switched_count = switched_count\n        \n        sampw = prior_w\n        sampb = prior_b\n        black_before = prior_b.shape[0] / (prior_w.shape[0] + prior_b.shape[0])\n        black_after = (prior_b.shape[0] + switched_count) / (prior_w.shape[0] + prior_b.shape[0])\n        ratio = black_after / black_before\n        enroll_stats.iloc[j] = [black_before, black_after, ratio]\n    \n    return enroll_stats"
  },
  {
    "objectID": "posts/20240328/Obermeyer_2019_experiment.html#examine-the-exercise-function-for-a-single-given-percentile",
    "href": "posts/20240328/Obermeyer_2019_experiment.html#examine-the-exercise-function-for-a-single-given-percentile",
    "title": "Obermeyer et al. 2019 experiment",
    "section": "",
    "text": "This function is used to simulate the counterfactual scenario in the paper (Fig. 1B)\n\ndefault_in_percentile = [97]\nj = 0\n\ndt = setup(default_in_percentile)['dt']\nenroll_stats = setup(default_in_percentile)['enroll_stats']\n\nprior_enrolled = dt[dt['risk_pctile'] &gt;= default_in_percentile[j]]\nprior_w = prior_enrolled[prior_enrolled['race'] == 'white']\nprior_b = prior_enrolled[prior_enrolled['race'] == 'black']\n\nupperb = dt[(dt['risk_pctile'] &gt;= default_in_percentile[j]) & (dt['race'] == 'black')]\nupperw = dt[(dt['risk_pctile'] &gt;= default_in_percentile[j]) & (dt['race'] == 'white')]\nlowerb = dt[(dt['risk_pctile'] &lt; default_in_percentile[j]) & (dt['race'] == 'black')]\n\nupperw = upperw.sort_values('gagne_sum_t')\nlowerb = lowerb.sort_values(['risk_score_t', 'gagne_sum_t'], ascending=[False, False])\n\nsw = 0\nsb = 0\nswitched_count = 0\nswitched_w = pd.DataFrame()\nswitched_b = pd.DataFrame()\n\n\ndt.shape\n\n(48784, 4)\n\n\n\ndt.head()\n\n\n\n\n\n\n\n\nrace\nrisk_score_t\ngagne_sum_t\nrisk_pctile\n\n\n\n\n0\nwhite\n1.987430\n0\n35\n\n\n1\nwhite\n7.677934\n3\n85\n\n\n2\nwhite\n0.407678\n0\n3\n\n\n3\nwhite\n0.798369\n0\n10\n\n\n4\nwhite\n17.513165\n1\n97\n\n\n\n\n\n\n\n\nupperw sorting: healthiest person on top\nlowerb sorting: sickest person on top\n\n\nupperw.iloc[sw]['gagne_sum_t'], lowerb.iloc[sb]['gagne_sum_t']\n\n(0, 12)\n\n\nDistribution of comorbidity score in white-upper and black-upper in the original data (based on the original cost algorithm)\n\nupperb_ = upperb.copy()\nupperw_ = upperw.copy()\nlowerb_ = lowerb.copy()\n\n\nfig, ax = plt.subplots(1, 1, figsize=(5, 4))\nsns.kdeplot(upperw_['gagne_sum_t'], label='white-upper', ax=ax)\nsns.kdeplot(upperb_['gagne_sum_t'], label='black-upper', ax=ax)\n# sns.kdeplot(lowerb_['gagne_sum_t'], label='black-lower', ax=ax)\nax.set(xlabel='Comorbidity score', title='97th percentile')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nHow the swapping works: 1. at a given percentile (e.g., 97th percentile) based on the risk score (cost algo) 2. identify whites who are above, identify blacks who are above and below cutoff 3. if the healthiest white-upper’s comorbidity &lt; sickest black-lower’s comorbidity (= if the healthist white-upper is healthier than the sickest black-lower) 4. remove the healthiest white (white-upper) 5. and give this spot to the sickest black from black-below (move them to the black-upper) 6. check all blacks in black-below\n\nwhile sb &lt; lowerb.shape[0]:\n    if upperw.iloc[sw]['gagne_sum_t'] &lt; lowerb.iloc[sb]['gagne_sum_t']:\n        switched_w = switched_w.append(upperw.iloc[sw])\n        switched_b = switched_b.append(lowerb.iloc[sb])\n\n        upperb = upperb.append(lowerb.iloc[sb])\n        upperw = upperw.drop(upperw.index[sw])\n        upperw = upperw.sort_values('gagne_sum_t')\n        \n        sb += 1\n        switched_count += 1\n    else:\n        sb += 1\n        switched_count = switched_count\n\nComparing the distribution of comorbidity score between before and after\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\nsns.kdeplot(upperw_['gagne_sum_t'], label='white-upper', ax=axes[0])\nsns.kdeplot(upperb_['gagne_sum_t'], label='black-upper', ax=axes[0])\naxes[0].set(xlabel='Comorbidity score', title='Before')\n\nsns.kdeplot(upperw['gagne_sum_t'], label='white-upper', ax=axes[1])\nsns.kdeplot(upperb['gagne_sum_t'], label='black-upper', ax=axes[1])\naxes[1].set(xlabel='Comorbidity score', title='After')\n\nfig.suptitle('97th percentile', x=0.5, y=0.95)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/20240328/Obermeyer_2019_experiment.html#table-2",
    "href": "posts/20240328/Obermeyer_2019_experiment.html#table-2",
    "title": "Obermeyer et al. 2019 experiment",
    "section": "",
    "text": "def get_concentration_metric_df(k, holdout_pred_df,\n                                y_predictors=['log_cost_t',\n                                              'log_cost_avoidable_t',\n                                              'gagne_sum_t'],\n                                outcomes=['log_cost_t', 'log_cost_avoidable_t',\n                                          'gagne_sum_t', 'dem_race_black']):\n    \"\"\"Calculate concentration of a given outcome of interest (columns) for\n    each algorithm trained label, and calculate fraction black in the high-risk\n    patient group.\n\n    Parameters\n    ----------\n    k : float\n        Top k% patients in terms of predicted risk.\n    holdout_pred_df : pd.DataFrame\n        Predictions for holdout set.\n    y_predictors : list\n        List of algorithm training label.\n    outcomes : list\n        List of given outcome of interest.\n\n    Returns\n    -------\n    pd.DataFrame\n        Concentration metric for holdout_pred_df.\n    \"\"\"\n    # define lookup for human readable headings in Table 2\n    OUTCOME_DICT = {\n        'cost_t': 'Total costs',\n        'log_cost_t': 'Total costs',\n        'cost_avoidable_t': 'Avoidable costs',\n        'log_cost_avoidable_t': 'Avoidable costs',\n        'gagne_sum_t': 'Active chronic conditions',\n        'dem_race_black': 'Race black'\n    }\n\n    top_k = int(k * len(holdout_pred_df))\n    all_concentration_metric = []  # save all rows of Table 2 to variable\n\n    # iterate through each predictor (algorithm training label)\n    # (this is each row in Table 2)\n    for y_col in y_predictors:\n        # get the predictions column name for y_col\n        y_hat_col = '{}_hat'.format(y_col)\n\n        # sort by y_hat_col\n        holdout_pred_df = holdout_pred_df.sort_values(by=y_hat_col, ascending=False)\n        # get top k% in terms of predicted risk\n        top_k_df = holdout_pred_df.iloc[:top_k]\n\n        # define dict to store calculated metrics for given y_col/predictor\n        # (each addition to the dict appends a column from Table 2)\n        concentration_dict = {\n            'predictor': OUTCOME_DICT[y_col]\n        }\n\n        # iterate through each outcome\n        # (concentration / frac black in highest-risk patients)\n        # (this is each column in Table 2)\n        for outcome in outcomes:\n            if 'log_' in outcome:\n                # for the outcomes presented on a log scale,\n                # we sum the un-logged values.\n                outcome = outcome[len('log_'):]\n\n            # define numerator of concentration metric:\n            # sum the top k of outcome\n            top_k_outcome = top_k_df[outcome].sum()\n\n            # define denominator of concentration metric\n            if outcome == 'dem_race_black':\n                # for fraction black in highest-risk patients,\n                # denominator is the n of top k%\n                total_outcome = top_k\n            else:\n                # for concentration in highest-risk patients,\n                # denominator is the total sum of the entire holdout\n                total_outcome = holdout_pred_df[outcome].sum()\n\n            # calculate concentration metric\n            frac_top_k = top_k_outcome / total_outcome\n\n            # add column to concentration_dict (row)\n            concentration_dict[OUTCOME_DICT[outcome]] = frac_top_k\n\n            # calculate standard error (SE)\n            n = len(holdout_pred_df)\n            import math\n            # SE = sqrt[ p * (1-p) / n]\n            se = math.sqrt((frac_top_k * (1-frac_top_k))/n)\n\n            # add SE column to concentration_dict (row)\n            concentration_dict[OUTCOME_DICT[outcome] + ' SE'] = se\n        all_concentration_metric.append(concentration_dict)\n\n    # convert to pd.DataFrame for pretty formatting\n    concentration_df = pd.DataFrame(all_concentration_metric)\n    concentration_df = concentration_df.set_index('predictor')\n\n    # define column order of Table 2\n    column_order = []\n    for outcome in outcomes:\n        outcome = OUTCOME_DICT[outcome]\n        column_order.append(outcome)\n        column_order.append(outcome + ' SE')\n\n    return concentration_df[column_order]\n\n\nholdout_pred_df = pd.read_csv('model_lasso_predictors.csv')\nholdout_pred_df.head()\n\n\n\n\n\n\n\n\nindex\ndem_race_black\nrisk_score_t\ngagne_sum_t\ncost_t\ncost_avoidable_t\nprogram_enrolled_t\nlog_cost_t_hat\ngagne_sum_t_hat\nlog_cost_avoidable_t_hat\nrisk_score_t_percentile\n\n\n\n\n0\n2\n0\n0.407678\n0\n500.0\n0.0\n0\n3.012857\n0.107439\n0.534697\n4\n\n\n1\n5\n0\n4.450484\n1\n123700.0\n0.0\n0\n3.985548\n2.423606\n2.113413\n69\n\n\n2\n10\n0\n0.713436\n0\n1500.0\n0.0\n0\n2.820869\n0.091171\n0.363918\n10\n\n\n3\n11\n0\n3.940887\n1\n21500.0\n500.0\n0\n3.116980\n0.788215\n0.800114\n63\n\n\n4\n13\n0\n1.919484\n0\n2800.0\n300.0\n0\n3.186166\n0.143811\n0.476029\n35\n\n\n\n\n\n\n\n\ny_hat_col = 'log_cost_t_hat'\nholdout_pred_df = holdout_pred_df.sort_values(by=y_hat_col, ascending=False)\n\n\n# get top k% in terms of predicted risk\nk = 0.03\ntop_k = int(k * len(holdout_pred_df))\ntop_k_df = holdout_pred_df.iloc[:top_k]\n\n\noutcome = 'cost_avoidable_t'\ntop_k_outcome = top_k_df[outcome].sum()\ntop_k_outcome\n\n8972400.0\n\n\n\ntotal_outcome = holdout_pred_df[outcome].sum()\ntotal_outcome\n\n38309600.0\n\n\n\nfrac_top_k = top_k_outcome / total_outcome\nfrac_top_k\n\n0.2342076137573872"
  }
]