[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Why ML Journal Club",
    "section": "",
    "text": "In machine learning (ML) community, numerous research papers and new tools come out everyday. For an individual ML practitioner like myself, it is impossible to check every paper and tool, and it is also difficult to know what works and what doesn’t. There are several weekly ML newsletters that provide a summary but I still need to carve out time after work to digest it. Besides, I prefer reading the papers in depth to examine how they exactly work and whether there are any flaws. But of course reading and critiquing papers on a regular basis requires discipline and dedication.\nThis is where a journal club can be useful. A group of people reading and dissecting papers together and having a discussion about them is certainly more fun and educational than doing everything alone. In a journal club, participants usually take turns to present a paper, which makes things easy. We can also hold each other accountable so that we as a group read papers on a consistent pace. Finally, the gathering itself becomes a good networking and information-sharing opportunity.\nI have gather a group of my fellow ML practitioner friends in Austin that I have known personally for several years. We are a diverse group of engineers and researchers with different interests and backgrounds. I am really excited to participate in this journal club with these brilliant friends to critique papers together, share our own practices at work, and socialize."
  },
  {
    "objectID": "posts/20221215/index.html",
    "href": "posts/20221215/index.html",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "",
    "text": "For our second meeting, Hannah, Hongsup, Joel, and Steve participated. Hongsup presented the paper Zero-Shot Text-to-Image Generation. For our discussion, see the Discussion section."
  },
  {
    "objectID": "posts/20221215/index.html#why-this-paper",
    "href": "posts/20221215/index.html#why-this-paper",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Why this paper",
    "text": "Why this paper\nGenerative AI has a lot of hype in ML community these days. OpenAI’s DALL·E, GPT-3, and ChatGPT are good examples. And there’s also stable diffusion. Since they all have public API, not just ML practitioners but general public can use the models to generate texts or images, which creates even bigger hype around generative AI.\nBut whenever there is hype around something, I think we should be more curious about what’s going on behind the scene. Understanding how it works helps us see through the hype and that is why I chose this paper. We can understand how DALL·E’s text-to-image generative model works, what the authors did to make this happen, and how they validated the result.\nTo understand this paper thoroughly, you need to know other deep learning model frameworks such as transformer, variational autoencoder, and OpenAI’s CLIP (Contrastive Language-Image Pre-training) model. I found these two articles extremely useful, written by Charlie Snell at UC Berkeley. In this post, I will talk about a high-level summary and the interesting discussion we had as a group. If you are interested in more detailed summary of the paper itself, I recommend those two posts."
  },
  {
    "objectID": "posts/20221215/index.html#big-picture",
    "href": "posts/20221215/index.html#big-picture",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Big picture",
    "text": "Big picture\nThe authors created a deep learning model which generate images from a text input. For instance, if you type “hands”, the model will generate images of hands. As the title says, this is done in zero-shot way, meaning that it can generate images that it hasn’t seen before. To be clear, the authors of this paper are not the first ones who created a model like this. There have been precedents but the authors say that the generated images from those still suffer from severe artifacts such as object distortion, illogical object placement, or unnatural blending of foreground and background elements. So the authors made improvements by adopting these two approaches: using a large set of training data and building a bigger model.\nBefore we look into the results, let’s first talk about the model architecture. Their model consists of two parts: variational autoencoder (VAE) and transformer."
  },
  {
    "objectID": "posts/20221215/index.html#variational-autoencoder-vae",
    "href": "posts/20221215/index.html#variational-autoencoder-vae",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Variational autoencoder (VAE)",
    "text": "Variational autoencoder (VAE)\nThe VAE contributes to the generative nature of the model because VAEs have latent representation in the middle that is a probability distribution. Once trained, we can use this distribution to draw samples from it, providing a generative framework. To train the VAE, the authors assumed uniform prior over the latent space. The model can learn the actual prior from the transformer later to generate images that match to text input. To train the VAE, the authors used images with text captions from various sources such as Wikipedia images.\nWhat is interesting about the VAE they used is that it assumes discrete latent distribution instead of continuous. This variant of VAE is called vector-quantized VAE (VQ-VAE). The motivation is that images and texts are discrete than continuous. But this assumption comes with a major complication: a discrete space is non-differentiable (i.e., can’t back-propagate). That’s why VQ-VAE has a codebook, which is essentially a look-up table where a discrete representation is associated with a codebook vector. To be accurate, this paper used a variant of VQ-VAE called dVAE where they made this look-up as a weighted average to further smooth out the space.\nThis VAE also acts as a dimensionality reduction technique because the discrete latent space the authors used has a resolution of 32x32 instead of 256x256, the resolution of the original training images. This brings compression benefit so that the transformer doesn’t have to memorize extremely long sequence but a sequence of length 1024 (=32*32)."
  },
  {
    "objectID": "posts/20221215/index.html#transformer",
    "href": "posts/20221215/index.html#transformer",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Transformer",
    "text": "Transformer\nOnce the VAE is learned, we can abandon the uniform prior assumption and use transformer to learn the actual prior. Transformers help image generation by pixel-wise prediction in an autoregressive way. For instance, given the sequence of previous pixels, the transformer can predict what the next pixel would look like.\nOnce the transformer is trained, when we give a text prompt to the model, the transformer makes predictions for the image latents (32x32 space) in an autoregressive way. Once we have all predictions, we use the dVAE codebook to lookup the vectors and generate the image. Since we can sample the sequence in a new way, we can generate multiple images. The authors used a top k approach to return the best images by ranking the generated images from a candidate pool based on the scores from OpenAI’s CLIP model, which represents how well the images match the caption.\nThe transformer has 12 billion parameters and a good chunk of the paper is dedicated to all the tricks the authors came up with to fit the model in GPU."
  },
  {
    "objectID": "posts/20221215/index.html#discussion",
    "href": "posts/20221215/index.html#discussion",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Discussion",
    "text": "Discussion\n\nAre the results representative enough?\nMost of us were somewhat disappointed by the authors’ model validation. Figures 3 and 4 in the paper gave some idea of how realistic the generated images are but we were not sure whether these were cherry-picked or not because the spectrum of images the model can generate is so wide. Figure 7 showed results from human evaluators. Most of them said the authors’ model was more realistic than the competitors’. Aside from the ethical issues surrounding hiring mturk workers, we thought the number of mturk workers was small (5 people) and the number of images they evaluated was small as well.\n\n\nWhy not investigate model failures?\nWhat was more interesting to us was Fig. 8, the CUB dataset which have images of birds. The example images here looked worse than others and the authors speculated that this was due to the detail-oriented text information of images, which might have been lost during the compression in dVAE. This was a plausible explanation but we wanted to see more in-depth investigation on model failures. There are numerous examples of terrifyingly looking images of hands generated by DALL·E because apparently it keeps failing at generating images of humans hands with five fingers.\nWe also discussed the lack of investigation on model failure from an ethical and responsible AI perspective. If OpenAI was going to publish a public API for a model like this, which would have varying degrees of socio-technical impact (look at all the issues ChatGPT has been creating these days), it would have been more responsible for them to test the model’s capacity more thoroughly and rigorously before rolling it out.\nWe found a model card from their repository and it was disappointingly short and did not address any possible ethical and social ramifications that would be caused by the model.\n\n\nValidity of the scoring metrics\nThe authors used FID and IS scores (generated by the CLIP model) to assess how well the images reflect the text input. The scores were used to rank a pool of candidate images and the model returned top k results. We questioned the validity of the decision behind using these scores because they are model-dependent, which means they are training-data-dependent. Plus, there was no mention of (at least) a qualitative comparison between the training datasets of this paper and the CLIP paper. This made us question the reliability of the CLIP model scores. It might have been interesting to see a batch of images that were ranked high (or low) so that we could judge the validity of the scores and understand the model behavior better.\n\n\nQualitative contribution\nAs in other deep learning papers, it was difficult for us to understand which decisions they made led to their results and advancement. For instance, they highlighted the larger training dataset and the larger model size. What was the measurable impact of each, and which one was more important? Similar to this, it would have been nice if they had some guidance on model tuning and hyperparameter selection to inform other researchers on model architecture design.\n\n\nReproducibility and novelty\nTo be blunt, the main highlight of this paper seemed to be the scale. They were able to use bigger datasets with a bigger model. But let’s be honest, how many academic institutions or companies are able to afford to train a model with 12 billion parameters? Especially without proper model inspection, how can we understand the model properly when we can’t reproduce it easily? Although there were certain elements of novelty especially on their tricks of utilizing GPU resources to train the model, if the scale is the main factor of success, can we really call this as a novel invention?"
  },
  {
    "objectID": "posts/20221215/index.html#final-thoughts",
    "href": "posts/20221215/index.html#final-thoughts",
    "title": "Zero-Shot Text-to-Image Generation",
    "section": "Final thoughts",
    "text": "Final thoughts\nThanks to the paper, we learned that VQ-VAE and transformer together can generate images from text inputs. However, we questioned the results and model validation especially due to the lack of investigation on model failure. We also thought about ethical aspect of this model being available in public. Just because it belongs to computer vision, which tends to amuse general audience, it does not mean that it is exempt from any social responsibility. And in deep learning with image and speech data, it is often the case that model validation is often looser than tabular data used in industries with higher stakes such as health care, finance, or risk assessment. That said, we would like to learn more about other techniques mentioned in the paper to have a deeper understanding of how they work."
  },
  {
    "objectID": "posts/20221027/index.html",
    "href": "posts/20221027/index.html",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "",
    "text": "We had our first meeting on Oct 27, 2022. Athula, Hongsup, Kate, and Saina participated. Hongsup presented the paper “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI."
  },
  {
    "objectID": "posts/20221027/index.html#why-this-paper",
    "href": "posts/20221027/index.html#why-this-paper",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Why this paper",
    "text": "Why this paper\nMany ML articles exist but few talk about how the sausage gets made. They are often based on toy or clean benchmark datasets, which are quite different from what we get in real world. That’s why I enjoy reading survey papers. They interview people in the field like us and try to address common pain points to find a broader picture.\nFor the past several years, I have been noticing a trend in ML community. Many practitioners tend to ignore data quality but instead put all their efforts into models and algorithms. I find it very troubling because many problems in real-world ML are caused by data-related issues. “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI talks about this pattern based on the interviews from dozens of ML practitioners all over the globe."
  },
  {
    "objectID": "posts/20221027/index.html#data-cascades",
    "href": "posts/20221027/index.html#data-cascades",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Data cascades",
    "text": "Data cascades\nThe paper focuses on “data cascades,” a series of compounding negative events due to data-related issues. The authors say the problems are wildly prevalent (92% reported experience at least one type of data cascades). During the discussion, we thought about sampling bias because those who are ignorant of these problems may not be able to recognize them. Indeed, they are often opaque because there are no clear indicators and often discovered later. Data cascades often lead to technical debt and harm to beneficiary communities. They can sour relationships between stakeholders and in extremely cases, force ML practitioners discard entire datasets. Figure 1 shows the schematic of the data cascades. We thought the figure wasn’t particularly informative because too many arrows exist between the points. We hoped that the authors explained why there aren’t arrows between certain points.\n\n\n\nFigure 1. Data cascades in high-stakes AI"
  },
  {
    "objectID": "posts/20221027/index.html#high-stake-domains",
    "href": "posts/20221027/index.html#high-stake-domains",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "High-stake domains",
    "text": "High-stake domains\nData cascades are more critical in high stake domains such as landslide detection, suicide prevention, and cancer detection. There are several reasons:\n\nMore ML applications are deployed in these domains where more direct humanitarian impact exists.\nThis impact can be disproportionate towards vulnerable communities.\nIt is often very challenging to acquire high quality data in these domains.\nThe problems frequently require more multidisciplinary approach.\n\n\n\n\nTable 1. Summary of participant demographics; Domain\n\n\nThe authors find that the problems are due to human factors. Unfortunately solutions have been focusing on other issues such as database, legal, or license. To gather firsthand experience in the field, the authors interviewed 50+ ML practitioners all over the world, ranging from the US to India and African countries, and from founders to developers. Table 1 summarizes various high-stake domains. It was fascinating for us to learn that ML is used in areas as landslide detection, poaching prevention, or regenerative farming. It would have been more interesting to see how data cascades create specific negative consequences in some domains."
  },
  {
    "objectID": "posts/20221027/index.html#data-cascade-triggers",
    "href": "posts/20221027/index.html#data-cascade-triggers",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Data cascade triggers",
    "text": "Data cascade triggers\nThe authors introduce three triggers that cause data cascades.\n\nPhysical world brittleness\nPhysical world changes over time and thus often ML systems can’t produce robust results. Data drifts due to hardware (measurements) and environmental changes are commonly mentioned in ML Ops literature. In high-stake domains, they become more pronounced because training data are very limited and policy or regulation changes can impact the ML systems in various ways.\n\n\nInadequate application-domain expertise\nMost ML practitioners are not equipped with domain knowledge. All of us admitted that our academic background does not match to the domain that we work in. Even though close collaboration between domain experts and ML practitioners is always emphasized, in practice the authors find that domain experts are often detached from the larger impact of the applications. The authors explain two specific types of problems:\nSubjectivity in ground truth: Areas such as insurance claim approval or medical imaging for cancer detection involve highly specialized and often subjective decision-making. To build a reliable and robust ML system, it is necessary to standardize the decision-making criteria and find consensus. However, ML practitioners are asked to rush through the development process, and thus do not have time to address it.\nPoor domain expertise in finding representative data: ML practitioners often start building ML applications without involving domain experts much because the practitioners simply believe that data are reliable. However, because they lack domain knowledge, practitioners make incomplete assumptions, which results in disparity between data collection and deployment. This often leads to poor and unreliable model performance.\n\n\nConflicting reward system\nData collection and any data-related work are often considered non-technical and undervalued. The situation gets worse for frontline workers because they are asked to collect and curate field data on top of their existing responsibilities but they are not well compensated.\n\n\nPoor cross-organizational documentation\nMetadata about data collection, quality, and curation are also often missing. Some good practices the authors suggest involve keeping good documentation on reproducible assets such as data collection plan, data strategy handbooks, design documents, file conventions, field notes, and so on."
  },
  {
    "objectID": "posts/20221027/index.html#broader-context",
    "href": "posts/20221027/index.html#broader-context",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Broader context",
    "text": "Broader context\nData cascades discussion can be extended to bigger problems in ML community.\n\nIncentives and currency in AI\nBecause of the low incentives, data-related work are not rewarded or even tracked. This makes us difficult to get buy-in from stakeholders. The situation is similar in academia as well. Most practitioners and researchers focus on developing algorithms but they rarely mention or work on data. The title “Everyone wants to do the model work, not the data work” was a verbatim from an interviewee. Unfortunately, we were all able to relate to this quote.\n\n\nData education\nMost ML or data science curricula lack any mention of data quality or ethics. They use toy datasets or very clean benchmark datasets. As experience ML practitioners, we wholeheartedly agreed with this finding. We lamented that these courses do not prepare students with practical knowledge because one never works with clean datasets in real world. Some of us have experienced this pattern firsthand because we have been interviewing candidates for an ML practitioner position and found that the candidates who only worked with clean datasets (or primarily worked on algorithms) often lack basic practical ML knowledge.\n\n\nData bootstrapping\nData bootstrapping describes ML practitioners’ use of other data sources such as established data services or existing datasets to create their own dataset. For most of us, it was surprising to learn that many ML practitioners in high-stake domains in countries from Global South had to collect data from scratch. We agreed that challenges in data collection and lack of access to quality data would create inequality between countries."
  },
  {
    "objectID": "posts/20221027/index.html#how-to-address-data-cascades",
    "href": "posts/20221027/index.html#how-to-address-data-cascades",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "How to address data cascades",
    "text": "How to address data cascades\nThe authors introduce several ways of addressing the problem of data cascades. They introduce the concept of “data excellence”, an effort to “focus on the practices, politics, and values of humans of the data pipeline to improve the quality and sanctity of data, through the use of processes, standards, infrastructure and incentives”.\n\nFrom goodness-of-fit to goodness-of-data\nThe first is to use the right metric to evaluate data quality. Many ML practitioners use model performance metrics such as accuracy and RMSE to evaluate data quality. Some of us had a similar experience. We had to argue that model metrics shouldn’t be used to make a decision on data-pipeline and data-quality features. We hope that the authors can introduce specific examples of goodness-of-data metrics in the future.\n\n\nIncentives for data excellence\nThere are several ways to address the low incentives of data work. First, journals and conferences should require dataset documentation, provenance, and ethics as mandatory disclosure. Second, organizations should reward data-related work similar to how good software engineering is rewarded. Finally, partnership between stakeholders can nurture data excellence by sharing the reward on data-related work such as data collection, anomaly identification, and model verification.\n\n\nEducation and visibility\nThe authors argue for real-world data literacy in AI education, which includes training on data collection, infrastructure building, data documentation, data sense-making, and data ethics and responsible AI in general. Increasing the data visibility in ML lifecycle is important as well; implementing good monitoring system is one of the most important ML Ops practices anyway."
  },
  {
    "objectID": "posts/20221027/index.html#final-thoughts",
    "href": "posts/20221027/index.html#final-thoughts",
    "title": "“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI",
    "section": "Final thoughts",
    "text": "Final thoughts\nSome of us found this paper vindicating because we have been advocating for data quality at work and had to fight for the attention it deserves. The paper helped us share our own practices at work that address data-related issues especially data collection, curation, and post-deployment data problems. Even though we generally agreed with authors’ suggestions, some of us wanted something more specific, like a case study. Overall, we found the paper interesting and insightful. We thought it would be beneficial to read the paper with our colleagues at work to start a discussion for data excellence."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Austin ML Journal Club",
    "section": "",
    "text": "Zero-Shot Text-to-Image Generation\n\n\n\n\n\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\nHongsup Shin\n\n\n\n\n\n\n  \n\n\n\n\n“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI\n\n\n\n\n\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\nHongsup Shin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy ML Journal Club\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2022\n\n\nHongsup Shin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Member introduction"
  }
]