## Overview
- ML-based science: Is prediction important in science? This might depend on the domain (e.g., climate change research might focus on predicting the risk) 
- Model info sheet: the intent is good but it seems unfeasible (too much effort and information for the author)
- Definition of reproducibility: it makes sense to consider the correctness of analysis and scientific integrity as a part of reproducibility
- Why focus on the civil war papers? Perhaps these are well-studied review papers w/ code and data available

## Why make distinction between science and engineering applications?
- It's not like that leakage is okay in production or difficult not to notice in engineering application. Data leakage is still an issue.
- Perhaps they meant that industry engineering applications get less constraints/scrutiny about the process. But practically speaking, engineering applications can pose higher risks than scientific research because they have more immediate real-world impacts
- Plus some papers they looked into sounded very close to industry such as software engineering, cybersecurity, etc.

## Taxonomy
- L1.1 (no test set): shocking!
- L1.2 (preprocessing): This is a loaded area because many different sub-domains exist such as imputation, oversampling, encoding, etc. Thus, it would've been nice to separate these areas further.
- L2 (illegitimate features):
	- Definition of proxy sounds a bit tricky.
		- We had a discussion about whether highly correlated features can be considered as data leakage. We agreed that all proxies are highly correlated features but not all highly correlated features are proxies.
		- Like in hypertensive drug example, proxies are those that we should not be using (either because it's cheating or not relevant or not available when making prediction) for obviously reasons. Thinking about the causal relationship also can help.
		- As the paper says, domain knowledge might be required to identify all proxies.
	- This L2 problem can get worse due to the high ML hype and ML code being extremely light (only a few lines of readily usable code available).
- L3.2 and L3.3: they are quite different from other data leakage problems.
	- Both are more challenging to identify.
	- If researchers can design experiments and collect data by themselves, they have some control over it, but for engineering applications/industry, often the data is given to us and we often don't have many options to mitigate this problem.
	- For instance, it's usually difficult to do split the data to achieve similar distributions between train and test (except class imbalance, temporal etc.)

## 3.3: Model info sheets and our theory of change
- High hopes!
- It might not be useful to believe that the authors would follow this guideline. It will be a useful sanity checkpoint where the authors can think about the process (in this sense, this could be useful for industry people as well).
- But of course, the authors can simply lie about it. But we get that this could be a middle-ground suggestion.
- Also, in certain filed, people are very reluctant about sharing their data and code (such as oil and gas companies where well data is essentially their most valuable asset).
- It might be more impactful if journals make this process as mandatory (but again, this will be a big ask).

## 3.4: Limitations of model info sheets
- Authors not being ML experts and using ML in their research seems like another side-effect of ML hype.

## Fig 1

- It's interesting that logistic regression isn't affected much by the correction. It would have been nice if the authors explained more about the why (there's some in appendix but it's not super clear).
- As the appendix explains, datasets from one paper were used by others even though they were based on imputation with high data leakage. Also some imputation processes in the papers the authors examined were done when the proportion of missing data is very high (like 90%).

## Problem of missing significant testing

- We discussed what exactly they were supposed to compare in terms of significant testing. Since they also mentioned confidence interval, we assumed it's the results based on bootstrapping

## 5: Enhancing the reproducibility 

- First of all, this paper feels very redundant. This section also.
- Again, these are good recommendations but maybe a big ask? (We were being realistic.)
- Privacy concern is another thing to think about. If we violate privacy when we have to disclose so much about data and distribution to prove the absence of data leakage, how would we resolve this conflict?
- We learned about the presence of CodeOcean (pretty cool effort).