# Meeting notes

All of us found a lot of problems with this paper. Here are some discussions points.

- From the abstract, having conclusion as "model X is the best" is honestly not publishable. It's simply stating that they did model tuning, which is really a few lines of code and doesn't mean anything more.
- The fact that they are framing this as classification, not regression is highly problematic and misleading. Making this as classification ignores the distribution and magnitude of difference between appraisal and selling price.
- Data leakage in feature space: we suspect some features may contain information about the target or they may not be available at the time of prediction
- Even though they didn't have any class imbalance (class ratio was almost 1:1), according to their graphs, it's possible that class imbalance would have changed over time.
- It was not clear what the goal of the paper was especially on what they want to do with the ML model. We had a discussion about whether the authors should've used listing price instead of appraisal. They also didn't seem to consider the common sense that government appraisal might lag and how their model will account for it.
- The dataset is limited to a single county and yet in conclusion they made overreaching conclusions.
- It's also unclear how they got the socioeconomic data, which is critical because they seemed to emphasize them in Introduction.
- Their feature representation was highly problematic (Tab. 1) mainly because they treated categorical features like zip code as numeric values.
- There were also new features (such as month) they later mentioned in the paper, which were missing in their feature set table. We also suspect that they used month as numeric variable not as ordinal/categorical variable.
- The sale date feature also could've been divided into more granular and meaningful features such as year, month, quarter, school season, etc.
- Even with feature importance calculation and feature selection, we couldn't find what they exactly want to do with the prediction and how they want to use it. What exactly can become actionable items is also missing. This is why we were confused about why they did correlation study because it wasn't really used much in their modeling. Besides, we don't know whether they are interested in building interpretable models and whether they care about conducting sensitivity analysis to understand the impact of feature value changes better.
- Using too many highly correlated features and their impact on calculating feature importance should have been investigated.
- Their feature importance calculation seems to have data leakage issue.
- Mean encoding idea is not clear. Also, it seems to be prone to data leakage.
- Voting classifier description doesn't mention what the voter algorithms are.
- They didn't have an independent test set and their cross validation didn't consider temporal aspect of the data (=more data leakage).
- Their lack of discussion on business objectives becomes problematic again when they do model comparison with various metrics. We don't know which metric makes most sense. They seemed to have used accuracy.
- At this point, we agreed that this paper seemed like it's written for the sake of showcasing ML (aka "we can build an ML for this") not for any research and business goals.
- Again, we think the model selection should've considered interpretability, which is related to the actual use case of the model, which the authors never discussed.
- They also didn't mention any baseline and so we don't know the achieved score is enough or not.
- Massive overreach is shown in their conclusions. 
- As a future work, they suggested building better ML predictors. First, this paper is not about creating a novel algorithm, so they can't make this claim and second, it's often the case that authors simply suggest building a better model even though the problem is not the model but rather it's them making so many basic mistakes such as data leakage, feature misrepresentation, and lack of use case.